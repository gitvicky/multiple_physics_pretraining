Loading rhel8/default-amp
  Loading requirement: dot rhel8/slurm singularity/current rhel8/global
    cuda/11.4 libpciaccess/0.16/gcc-9.4.0-6fonbj6
    libiconv/1.16/gcc-9.4.0-ahebbov libxml2/2.9.12/gcc-9.4.0-gnknt5e
    ncurses/6.2/gcc-9.4.0-aiirok7 hwloc/2.5.0/gcc-9.4.0-7sqomga
    libevent/2.1.12/gcc-9.4.0-hgny7cm numactl/2.0.14/gcc-9.4.0-52dwc6n
    cuda/11.4.0/gcc-9.4.0-3hnxhjt gdrcopy/2.2/gcc-9.4.0-e4igtfp
    knem/1.1.4/gcc-9.4.0-bpbxgva libnl/3.3.0/gcc-9.4.0-whwhrwb
    rdma-core/34.0/gcc-9.4.0-5eo5n2u ucx/1.11.1/gcc-9.4.0-lktqyl4
    openmpi/4.1.1/gcc-9.4.0-epagguv
ERROR: Unable to locate a modulefile for 'cudnn/cuda12-8.9.0'
ERROR: Unable to locate a modulefile for 'nccl/cuda12.1-2.18.1'
Changed directory to /home/ir-gopa2/rds/rds-ukaea-ap001/ir-gopa2/Code/multiple_physics_pretraining.

JobID: 50752145
======
Time: Mon Apr 15 20:01:11 BST 2024
Running on master node: gpu-q-15
Current directory: /home/ir-gopa2/rds/rds-ukaea-ap001/ir-gopa2/Code/multiple_physics_pretraining

Nodes allocated:
================
cat: machine.file.50752145: No such file or directory


numtasks=4, numnodes=1, mpi_tasks_per_node=4 (OMP_NUM_THREADS=1)

Executing command:
==================
python /home/ir-gopa2/rds/rds-ukaea-ap001/ir-gopa2/Code/multiple_physics_pretraining/train_basic.py --yaml_config config/mpp_avit_ti_config.yaml --config basic_config 

Initializing data on rank 0
Initializing model on rank 0
Model parameter count: 7285884
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
AViT                                               --
├─SubsampledLinear: 1-1                            624
├─hMLP_stem: 1-2                                   --
│    └─Sequential: 2-1                             --
│    │    └─Conv2d: 3-1                            36,864
│    │    └─RMSInstanceNorm2d: 3-2                 96
│    │    └─GELU: 3-3                              --
│    │    └─Conv2d: 3-4                            9,216
│    │    └─RMSInstanceNorm2d: 3-5                 96
│    │    └─GELU: 3-6                              --
│    │    └─Conv2d: 3-7                            36,864
│    │    └─RMSInstanceNorm2d: 3-8                 384
├─ModuleList: 1-3                                  --
│    └─SpaceTimeBlock: 2-2                         --
│    │    └─AxialAttentionBlock: 3-9               445,984
│    │    └─AttentionBlock: 3-10                   149,536
│    └─SpaceTimeBlock: 2-3                         --
│    │    └─AxialAttentionBlock: 3-11              445,984
│    │    └─AttentionBlock: 3-12                   149,536
│    └─SpaceTimeBlock: 2-4                         --
│    │    └─AxialAttentionBlock: 3-13              445,984
│    │    └─AttentionBlock: 3-14                   149,536
│    └─SpaceTimeBlock: 2-5                         --
│    │    └─AxialAttentionBlock: 3-15              445,984
│    │    └─AttentionBlock: 3-16                   149,536
│    └─SpaceTimeBlock: 2-6                         --
│    │    └─AxialAttentionBlock: 3-17              445,984
│    │    └─AttentionBlock: 3-18                   149,536
│    └─SpaceTimeBlock: 2-7                         --
│    │    └─AxialAttentionBlock: 3-19              445,984
│    │    └─AttentionBlock: 3-20                   149,536
│    └─SpaceTimeBlock: 2-8                         --
│    │    └─AxialAttentionBlock: 3-21              445,984
│    │    └─AttentionBlock: 3-22                   149,536
│    └─SpaceTimeBlock: 2-9                         --
│    │    └─AxialAttentionBlock: 3-23              445,984
│    │    └─AttentionBlock: 3-24                   149,536
│    └─SpaceTimeBlock: 2-10                        --
│    │    └─AxialAttentionBlock: 3-25              445,984
│    │    └─AttentionBlock: 3-26                   149,536
│    └─SpaceTimeBlock: 2-11                        --
│    │    └─AxialAttentionBlock: 3-27              445,984
│    │    └─AttentionBlock: 3-28                   149,536
│    └─SpaceTimeBlock: 2-12                        --
│    │    └─AxialAttentionBlock: 3-29              445,984
│    │    └─AttentionBlock: 3-30                   149,536
│    └─SpaceTimeBlock: 2-13                        --
│    │    └─AxialAttentionBlock: 3-31              445,984
│    │    └─AttentionBlock: 3-32                   149,536
├─hMLP_output: 1-4                                 9,228
│    └─Sequential: 2-14                            --
│    │    └─ConvTranspose2d: 3-33                  36,864
│    │    └─RMSInstanceNorm2d: 3-34                96
│    │    └─GELU: 3-35                             --
│    │    └─ConvTranspose2d: 3-36                  9,216
│    │    └─RMSInstanceNorm2d: 3-37                96
│    │    └─GELU: 3-38                             --
===========================================================================
Total params: 7,285,884
Trainable params: 7,285,884
Non-trainable params: 0
===========================================================================
Starting Training Loop...
train_loader_size 3400 68000
Epoch 1 Batch 0 Train Loss 0.09672169387340546
Total Times. Batch: 0, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 1.9566779136657715, Forward: 2.0105724334716797, Backward: 0.8045122623443604, Optimizer: 0
Epoch 1 Batch 1 Train Loss 0.09580846130847931
Total Times. Batch: 1, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0015153884887695312, Forward: 0.08311915397644043, Backward: 0.10270977020263672, Optimizer: 0
Epoch 1 Batch 2 Train Loss 0.09619239717721939
Total Times. Batch: 2, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001312255859375, Forward: 0.08171844482421875, Backward: 0.1031646728515625, Optimizer: 0
Epoch 1 Batch 3 Train Loss 0.09669273346662521
Total Times. Batch: 3, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001241445541381836, Forward: 0.08162736892700195, Backward: 0.10359501838684082, Optimizer: 0
ng: 2 lr: 1.0 dlr: 1e-06 d_hat: -4.352828802031126e-13, d: 1e-06. sksq_weighted=1.7e-15 sk_l1=1.7e-08 gsq_weighted=1.7e-13
Epoch 1 Batch 4 Train Loss 0.09778475016355515
Total Times. Batch: 4, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012488365173339844, Forward: 0.08132076263427734, Backward: 0.10316824913024902, Optimizer: 0.32048535346984863
Epoch 1 Batch 5 Train Loss 0.09812457114458084
Total Times. Batch: 5, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012547969818115234, Forward: 0.08179783821105957, Backward: 0.09362435340881348, Optimizer: 0
Epoch 1 Batch 6 Train Loss 0.09615939110517502
Total Times. Batch: 6, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.00141143798828125, Forward: 0.08185958862304688, Backward: 0.10251069068908691, Optimizer: 0
Epoch 1 Batch 7 Train Loss 0.10203968733549118
Total Times. Batch: 7, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001295328140258789, Forward: 0.0827796459197998, Backward: 0.10332703590393066, Optimizer: 0
Epoch 1 Batch 8 Train Loss 0.10031072050333023
Total Times. Batch: 8, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001379251480102539, Forward: 0.08152246475219727, Backward: 0.10354375839233398, Optimizer: 0
Epoch 1 Batch 9 Train Loss 0.09819512814283371
Total Times. Batch: 9, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001363992691040039, Forward: 0.08190131187438965, Backward: 0.10339164733886719, Optimizer: 0.16797089576721191
Epoch 1 Batch 10 Train Loss 0.0888918936252594
Total Times. Batch: 10, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011515617370605469, Forward: 0.08236193656921387, Backward: 0.09299588203430176, Optimizer: 0
Epoch 1 Batch 11 Train Loss 0.09713587909936905
Total Times. Batch: 11, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012824535369873047, Forward: 0.08154821395874023, Backward: 0.10352444648742676, Optimizer: 0
Epoch 1 Batch 12 Train Loss 0.09996572136878967
Total Times. Batch: 12, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012276172637939453, Forward: 0.08205223083496094, Backward: 0.10342264175415039, Optimizer: 0
Epoch 1 Batch 13 Train Loss 0.10281921923160553
Total Times. Batch: 13, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012514591217041016, Forward: 0.08138275146484375, Backward: 0.1034848690032959, Optimizer: 0
Epoch 1 Batch 14 Train Loss 0.09956976026296616
Total Times. Batch: 14, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001215219497680664, Forward: 0.0814826488494873, Backward: 0.10337209701538086, Optimizer: 0.1681821346282959
Epoch 1 Batch 15 Train Loss 0.09647607803344727
Total Times. Batch: 15, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001245260238647461, Forward: 0.08178067207336426, Backward: 0.0940704345703125, Optimizer: 0
Epoch 1 Batch 16 Train Loss 0.09070605039596558
Total Times. Batch: 16, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011756420135498047, Forward: 0.08149909973144531, Backward: 0.10320186614990234, Optimizer: 0
Epoch 1 Batch 17 Train Loss 0.09757420420646667
Total Times. Batch: 17, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001237630844116211, Forward: 0.08196830749511719, Backward: 0.10322165489196777, Optimizer: 0
Epoch 1 Batch 18 Train Loss 0.10516476631164551
Total Times. Batch: 18, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012574195861816406, Forward: 0.08122897148132324, Backward: 0.10350203514099121, Optimizer: 0
Epoch 1 Batch 19 Train Loss 0.10437088459730148
Total Times. Batch: 19, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012826919555664062, Forward: 0.0817110538482666, Backward: 0.10326933860778809, Optimizer: 0.1695871353149414
Epoch 1 Batch 20 Train Loss 0.09396395832300186
Total Times. Batch: 20, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011715888977050781, Forward: 0.08121752738952637, Backward: 0.09411001205444336, Optimizer: 0
Epoch 1 Batch 21 Train Loss 0.09436360746622086
Total Times. Batch: 21, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0014867782592773438, Forward: 0.08131909370422363, Backward: 0.10319685935974121, Optimizer: 0
Epoch 1 Batch 22 Train Loss 0.1064014807343483
Total Times. Batch: 22, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012028217315673828, Forward: 0.08133673667907715, Backward: 0.10330462455749512, Optimizer: 0
Epoch 1 Batch 23 Train Loss 0.09603945910930634
Total Times. Batch: 23, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012977123260498047, Forward: 0.08178544044494629, Backward: 0.10351037979125977, Optimizer: 0
Epoch 1 Batch 24 Train Loss 0.09887372702360153
Total Times. Batch: 24, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012526512145996094, Forward: 0.08147788047790527, Backward: 0.10304117202758789, Optimizer: 0.16863036155700684
Epoch 1 Batch 25 Train Loss 0.09755221754312515
Total Times. Batch: 25, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001222372055053711, Forward: 0.08136343955993652, Backward: 0.09412217140197754, Optimizer: 0
Epoch 1 Batch 26 Train Loss 0.09002824127674103
Total Times. Batch: 26, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012085437774658203, Forward: 0.08151578903198242, Backward: 0.10347414016723633, Optimizer: 0
Epoch 1 Batch 27 Train Loss 0.0887376144528389
Total Times. Batch: 27, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013396739959716797, Forward: 0.0813438892364502, Backward: 0.10352039337158203, Optimizer: 0
Epoch 1 Batch 28 Train Loss 0.10156366974115372
Total Times. Batch: 28, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011677742004394531, Forward: 0.08119559288024902, Backward: 0.10387539863586426, Optimizer: 0
Epoch 1 Batch 29 Train Loss 0.09524782001972198
Total Times. Batch: 29, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012052059173583984, Forward: 0.08146882057189941, Backward: 0.10342788696289062, Optimizer: 0.16884088516235352
Epoch 1 Batch 30 Train Loss 0.10657042264938354
Total Times. Batch: 30, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011703968048095703, Forward: 0.08151102066040039, Backward: 0.09381675720214844, Optimizer: 0
Epoch 1 Batch 31 Train Loss 0.09551653265953064
Total Times. Batch: 31, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012478828430175781, Forward: 0.08196115493774414, Backward: 0.10343503952026367, Optimizer: 0
Epoch 1 Batch 32 Train Loss 0.10028137266635895
Total Times. Batch: 32, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012378692626953125, Forward: 0.0816500186920166, Backward: 0.10347318649291992, Optimizer: 0
Epoch 1 Batch 33 Train Loss 0.09619391709566116
Total Times. Batch: 33, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001256704330444336, Forward: 0.08157896995544434, Backward: 0.10350847244262695, Optimizer: 0
Epoch 1 Batch 34 Train Loss 0.10287340730428696
Total Times. Batch: 34, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011463165283203125, Forward: 0.0824739933013916, Backward: 0.10306000709533691, Optimizer: 0.1685628890991211
Epoch 1 Batch 35 Train Loss 0.09541336447000504
Total Times. Batch: 35, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012352466583251953, Forward: 0.08136391639709473, Backward: 0.0939030647277832, Optimizer: 0
Epoch 1 Batch 36 Train Loss 0.0967852845788002
Total Times. Batch: 36, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012521743774414062, Forward: 0.08167004585266113, Backward: 0.10312199592590332, Optimizer: 0
Epoch 1 Batch 37 Train Loss 0.08903056383132935
Total Times. Batch: 37, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013058185577392578, Forward: 0.08165764808654785, Backward: 0.10353684425354004, Optimizer: 0
Epoch 1 Batch 38 Train Loss 0.10295803844928741
Total Times. Batch: 38, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001157522201538086, Forward: 0.08156895637512207, Backward: 0.10370779037475586, Optimizer: 0
Epoch 1 Batch 39 Train Loss 0.10077934712171555
Total Times. Batch: 39, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013225078582763672, Forward: 0.08188819885253906, Backward: 0.10343694686889648, Optimizer: 0.16860127449035645
Epoch 1 Batch 40 Train Loss 0.09667058289051056
Total Times. Batch: 40, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0014362335205078125, Forward: 0.08162260055541992, Backward: 0.09402656555175781, Optimizer: 0
Epoch 1 Batch 41 Train Loss 0.0993194505572319
Total Times. Batch: 41, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0015194416046142578, Forward: 0.08169269561767578, Backward: 0.10315680503845215, Optimizer: 0
Epoch 1 Batch 42 Train Loss 0.10359889268875122
Total Times. Batch: 42, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012390613555908203, Forward: 0.08134293556213379, Backward: 0.10338592529296875, Optimizer: 0
Epoch 1 Batch 43 Train Loss 0.10095157474279404
Total Times. Batch: 43, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011780261993408203, Forward: 0.08146142959594727, Backward: 0.10327410697937012, Optimizer: 0
Epoch 1 Batch 44 Train Loss 0.0989559218287468
Total Times. Batch: 44, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013070106506347656, Forward: 0.08139824867248535, Backward: 0.10325002670288086, Optimizer: 0.16876816749572754
Epoch 1 Batch 45 Train Loss 0.09748475253582001
Total Times. Batch: 45, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001138925552368164, Forward: 0.08156585693359375, Backward: 0.09401130676269531, Optimizer: 0
Epoch 1 Batch 46 Train Loss 0.09889669716358185
Total Times. Batch: 46, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012583732604980469, Forward: 0.08150815963745117, Backward: 0.10352396965026855, Optimizer: 0
Epoch 1 Batch 47 Train Loss 0.09318365901708603
Total Times. Batch: 47, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013778209686279297, Forward: 0.08167147636413574, Backward: 0.10337996482849121, Optimizer: 0
Epoch 1 Batch 48 Train Loss 0.10032079368829727
Total Times. Batch: 48, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012350082397460938, Forward: 0.08163666725158691, Backward: 0.10315656661987305, Optimizer: 0
Epoch 1 Batch 49 Train Loss 0.09682343155145645
Total Times. Batch: 49, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012652873992919922, Forward: 0.0816643238067627, Backward: 0.1036064624786377, Optimizer: 0.16963934898376465
Epoch 1 Batch 50 Train Loss 0.09305591881275177
Total Times. Batch: 50, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012309551239013672, Forward: 0.0812835693359375, Backward: 0.09393000602722168, Optimizer: 0
Epoch 1 Batch 51 Train Loss 0.09582462161779404
Total Times. Batch: 51, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013074874877929688, Forward: 0.08152556419372559, Backward: 0.10329985618591309, Optimizer: 0
Epoch 1 Batch 52 Train Loss 0.09447766840457916
Total Times. Batch: 52, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001161336898803711, Forward: 0.08161258697509766, Backward: 0.10334944725036621, Optimizer: 0
Epoch 1 Batch 53 Train Loss 0.10420157760381699
Total Times. Batch: 53, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011091232299804688, Forward: 0.0816810131072998, Backward: 0.10325050354003906, Optimizer: 0
Epoch 1 Batch 54 Train Loss 0.09478239715099335
Total Times. Batch: 54, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001201629638671875, Forward: 0.08191943168640137, Backward: 0.1031026840209961, Optimizer: 0.16965866088867188
Epoch 1 Batch 55 Train Loss 0.09537544846534729
Total Times. Batch: 55, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011806488037109375, Forward: 0.0814054012298584, Backward: 0.0941779613494873, Optimizer: 0
Epoch 1 Batch 56 Train Loss 0.09957615286111832
Total Times. Batch: 56, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011718273162841797, Forward: 0.08115410804748535, Backward: 0.10384917259216309, Optimizer: 0
Epoch 1 Batch 57 Train Loss 0.09905420988798141
Total Times. Batch: 57, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012545585632324219, Forward: 0.0816349983215332, Backward: 0.10319352149963379, Optimizer: 0
Epoch 1 Batch 58 Train Loss 0.0955679640173912
Total Times. Batch: 58, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011603832244873047, Forward: 0.08173346519470215, Backward: 0.10359883308410645, Optimizer: 0
Epoch 1 Batch 59 Train Loss 0.09783661365509033
Total Times. Batch: 59, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011641979217529297, Forward: 0.08155322074890137, Backward: 0.1033623218536377, Optimizer: 0.16794872283935547
Epoch 1 Batch 60 Train Loss 0.09422971308231354
Total Times. Batch: 60, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.00121307373046875, Forward: 0.08140420913696289, Backward: 0.09390521049499512, Optimizer: 0
Epoch 1 Batch 61 Train Loss 0.10412091016769409
Total Times. Batch: 61, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011701583862304688, Forward: 0.08118224143981934, Backward: 0.10351347923278809, Optimizer: 0
Epoch 1 Batch 62 Train Loss 0.09984388202428818
Total Times. Batch: 62, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001251220703125, Forward: 0.08147239685058594, Backward: 0.10316824913024902, Optimizer: 0
Epoch 1 Batch 63 Train Loss 0.09590502828359604
Total Times. Batch: 63, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012750625610351562, Forward: 0.08172965049743652, Backward: 0.10305404663085938, Optimizer: 0
Epoch 1 Batch 64 Train Loss 0.09605057537555695
Total Times. Batch: 64, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011894702911376953, Forward: 0.08121132850646973, Backward: 0.10321235656738281, Optimizer: 0.16779088973999023
Epoch 1 Batch 65 Train Loss 0.09805484861135483
Total Times. Batch: 65, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011703968048095703, Forward: 0.08143997192382812, Backward: 0.09387016296386719, Optimizer: 0
Epoch 1 Batch 66 Train Loss 0.09556056559085846
Total Times. Batch: 66, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011990070343017578, Forward: 0.0814049243927002, Backward: 0.10316991806030273, Optimizer: 0
Epoch 1 Batch 67 Train Loss 0.0925520583987236
Total Times. Batch: 67, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011734962463378906, Forward: 0.08138132095336914, Backward: 0.10348725318908691, Optimizer: 0
Epoch 1 Batch 68 Train Loss 0.10012872517108917
Total Times. Batch: 68, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012958049774169922, Forward: 0.08179235458374023, Backward: 0.10341572761535645, Optimizer: 0
Epoch 1 Batch 69 Train Loss 0.0933912843465805
Total Times. Batch: 69, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012042522430419922, Forward: 0.08160924911499023, Backward: 0.10363984107971191, Optimizer: 0.16961026191711426
Epoch 1 Batch 70 Train Loss 0.10209858417510986
Total Times. Batch: 70, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011625289916992188, Forward: 0.0813760757446289, Backward: 0.09402632713317871, Optimizer: 0
Epoch 1 Batch 71 Train Loss 0.09511962532997131
Total Times. Batch: 71, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012972354888916016, Forward: 0.08167600631713867, Backward: 0.10363030433654785, Optimizer: 0
Epoch 1 Batch 72 Train Loss 0.09566711634397507
Total Times. Batch: 72, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012364387512207031, Forward: 0.08129000663757324, Backward: 0.10340046882629395, Optimizer: 0
Epoch 1 Batch 73 Train Loss 0.10505767166614532
Total Times. Batch: 73, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010941028594970703, Forward: 0.08171892166137695, Backward: 0.10383033752441406, Optimizer: 0
Epoch 1 Batch 74 Train Loss 0.10483825206756592
Total Times. Batch: 74, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012416839599609375, Forward: 0.08133506774902344, Backward: 0.10354113578796387, Optimizer: 0.16922903060913086
Epoch 1 Batch 75 Train Loss 0.10240738838911057
Total Times. Batch: 75, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001192331314086914, Forward: 0.08115315437316895, Backward: 0.09393668174743652, Optimizer: 0
Epoch 1 Batch 76 Train Loss 0.09092173725366592
Total Times. Batch: 76, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011434555053710938, Forward: 0.08211374282836914, Backward: 0.1032874584197998, Optimizer: 0
Epoch 1 Batch 77 Train Loss 0.08969884365797043
Total Times. Batch: 77, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0014338493347167969, Forward: 0.0812692642211914, Backward: 0.10316634178161621, Optimizer: 0
Epoch 1 Batch 78 Train Loss 0.09508803486824036
Total Times. Batch: 78, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011744499206542969, Forward: 0.08161139488220215, Backward: 0.10309696197509766, Optimizer: 0
Epoch 1 Batch 79 Train Loss 0.10429475456476212
Total Times. Batch: 79, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001348733901977539, Forward: 0.08140826225280762, Backward: 0.10344886779785156, Optimizer: 0.17106270790100098
Epoch 1 Batch 80 Train Loss 0.10432834923267365
Total Times. Batch: 80, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001249074935913086, Forward: 0.08150148391723633, Backward: 0.09406065940856934, Optimizer: 0
Epoch 1 Batch 81 Train Loss 0.10347183048725128
Total Times. Batch: 81, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012617111206054688, Forward: 0.08134722709655762, Backward: 0.10351395606994629, Optimizer: 0
Epoch 1 Batch 82 Train Loss 0.10273130983114243
Total Times. Batch: 82, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011563301086425781, Forward: 0.0811161994934082, Backward: 0.10337686538696289, Optimizer: 0
Epoch 1 Batch 83 Train Loss 0.09949374198913574
Total Times. Batch: 83, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011854171752929688, Forward: 0.08134651184082031, Backward: 0.10296130180358887, Optimizer: 0
Epoch 1 Batch 84 Train Loss 0.10192852467298508
Total Times. Batch: 84, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012044906616210938, Forward: 0.08145833015441895, Backward: 0.10304927825927734, Optimizer: 0.16925907135009766
Epoch 1 Batch 85 Train Loss 0.09925419092178345
Total Times. Batch: 85, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012180805206298828, Forward: 0.08137655258178711, Backward: 0.09408926963806152, Optimizer: 0
Epoch 1 Batch 86 Train Loss 0.09567995369434357
Total Times. Batch: 86, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013697147369384766, Forward: 0.08166718482971191, Backward: 0.10311174392700195, Optimizer: 0
Epoch 1 Batch 87 Train Loss 0.09665010124444962
Total Times. Batch: 87, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012555122375488281, Forward: 0.08139467239379883, Backward: 0.10346698760986328, Optimizer: 0
Epoch 1 Batch 88 Train Loss 0.09349983930587769
Total Times. Batch: 88, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011620521545410156, Forward: 0.08155441284179688, Backward: 0.10308957099914551, Optimizer: 0
Epoch 1 Batch 89 Train Loss 0.10603504627943039
Total Times. Batch: 89, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011081695556640625, Forward: 0.08112263679504395, Backward: 0.10319924354553223, Optimizer: 0.16801142692565918
Epoch 1 Batch 90 Train Loss 0.10054522007703781
Total Times. Batch: 90, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011248588562011719, Forward: 0.08119010925292969, Backward: 0.09392070770263672, Optimizer: 0
Epoch 1 Batch 91 Train Loss 0.09591908007860184
Total Times. Batch: 91, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013005733489990234, Forward: 0.08168745040893555, Backward: 0.10330867767333984, Optimizer: 0
Epoch 1 Batch 92 Train Loss 0.09597696363925934
Total Times. Batch: 92, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012323856353759766, Forward: 0.08152914047241211, Backward: 0.10342597961425781, Optimizer: 0
Epoch 1 Batch 93 Train Loss 0.09662029892206192
Total Times. Batch: 93, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012044906616210938, Forward: 0.08116793632507324, Backward: 0.10370516777038574, Optimizer: 0
Epoch 1 Batch 94 Train Loss 0.08396923542022705
Total Times. Batch: 94, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011715888977050781, Forward: 0.08144235610961914, Backward: 0.10316610336303711, Optimizer: 0.1675245761871338
Epoch 1 Batch 95 Train Loss 0.08968127518892288
Total Times. Batch: 95, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011641979217529297, Forward: 0.08138036727905273, Backward: 0.09427356719970703, Optimizer: 0
Epoch 1 Batch 96 Train Loss 0.09588693827390671
Total Times. Batch: 96, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011372566223144531, Forward: 0.08164715766906738, Backward: 0.10311603546142578, Optimizer: 0
Epoch 1 Batch 97 Train Loss 0.1019177958369255
Total Times. Batch: 97, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012874603271484375, Forward: 0.08199381828308105, Backward: 0.10320782661437988, Optimizer: 0
Epoch 1 Batch 98 Train Loss 0.10107536613941193
Total Times. Batch: 98, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001190185546875, Forward: 0.0813143253326416, Backward: 0.1032109260559082, Optimizer: 0
Epoch 1 Batch 99 Train Loss 0.0951615571975708
Total Times. Batch: 99, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011839866638183594, Forward: 0.08147549629211426, Backward: 0.1033930778503418, Optimizer: 0.1668236255645752
Epoch 1 Batch 100 Train Loss 0.09692632406949997
Total Times. Batch: 100, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011322498321533203, Forward: 0.08132457733154297, Backward: 0.09391903877258301, Optimizer: 0
Epoch 1 Batch 101 Train Loss 0.09527536481618881
Total Times. Batch: 101, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0015027523040771484, Forward: 0.08129000663757324, Backward: 0.1035003662109375, Optimizer: 0
Epoch 1 Batch 102 Train Loss 0.1011153981089592
Total Times. Batch: 102, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011911392211914062, Forward: 0.08234715461730957, Backward: 0.10334587097167969, Optimizer: 0
Epoch 1 Batch 103 Train Loss 0.10433895885944366
Total Times. Batch: 103, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011806488037109375, Forward: 0.08148980140686035, Backward: 0.10317206382751465, Optimizer: 0
Epoch 1 Batch 104 Train Loss 0.1012779101729393
Total Times. Batch: 104, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011856555938720703, Forward: 0.08146214485168457, Backward: 0.10329914093017578, Optimizer: 0.16795015335083008
Epoch 1 Batch 105 Train Loss 0.1027236208319664
Total Times. Batch: 105, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.00119781494140625, Forward: 0.08175301551818848, Backward: 0.09423446655273438, Optimizer: 0
Epoch 1 Batch 106 Train Loss 0.09384305775165558
Total Times. Batch: 106, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012738704681396484, Forward: 0.08148860931396484, Backward: 0.10321235656738281, Optimizer: 0
Epoch 1 Batch 107 Train Loss 0.09974080324172974
Total Times. Batch: 107, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001184701919555664, Forward: 0.08144426345825195, Backward: 0.10347318649291992, Optimizer: 0
Epoch 1 Batch 108 Train Loss 0.09887848049402237
Total Times. Batch: 108, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012021064758300781, Forward: 0.08132505416870117, Backward: 0.10317492485046387, Optimizer: 0
Epoch 1 Batch 109 Train Loss 0.09856981784105301
Total Times. Batch: 109, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011010169982910156, Forward: 0.08130836486816406, Backward: 0.1032416820526123, Optimizer: 0.1666269302368164
Epoch 1 Batch 110 Train Loss 0.09638386964797974
Total Times. Batch: 110, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011146068572998047, Forward: 0.08131074905395508, Backward: 0.09376406669616699, Optimizer: 0
Epoch 1 Batch 111 Train Loss 0.08390682190656662
Total Times. Batch: 111, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012826919555664062, Forward: 0.08111310005187988, Backward: 0.1034400463104248, Optimizer: 0
Epoch 1 Batch 112 Train Loss 0.09397714585065842
Total Times. Batch: 112, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011370182037353516, Forward: 0.08234333992004395, Backward: 0.1032564640045166, Optimizer: 0
Epoch 1 Batch 113 Train Loss 0.09797333925962448
Total Times. Batch: 113, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011892318725585938, Forward: 0.0815274715423584, Backward: 0.10316753387451172, Optimizer: 0
Epoch 1 Batch 114 Train Loss 0.09454281628131866
Total Times. Batch: 114, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011773109436035156, Forward: 0.08139204978942871, Backward: 0.1036381721496582, Optimizer: 0.16663098335266113
Epoch 1 Batch 115 Train Loss 0.10519500076770782
Total Times. Batch: 115, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011241436004638672, Forward: 0.08110332489013672, Backward: 0.09403371810913086, Optimizer: 0
Epoch 1 Batch 116 Train Loss 0.0830211192369461
Total Times. Batch: 116, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0014569759368896484, Forward: 0.08164596557617188, Backward: 0.10357332229614258, Optimizer: 0
Epoch 1 Batch 117 Train Loss 0.08596787601709366
Total Times. Batch: 117, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001195669174194336, Forward: 0.08219575881958008, Backward: 0.10333871841430664, Optimizer: 0
Epoch 1 Batch 118 Train Loss 0.09813175350427628
Total Times. Batch: 118, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011749267578125, Forward: 0.08128547668457031, Backward: 0.10339021682739258, Optimizer: 0
Epoch 1 Batch 119 Train Loss 0.10758403688669205
Total Times. Batch: 119, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001239776611328125, Forward: 0.08145499229431152, Backward: 0.10325765609741211, Optimizer: 0.16697931289672852
Epoch 1 Batch 120 Train Loss 0.097224660217762
Total Times. Batch: 120, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011298656463623047, Forward: 0.08141088485717773, Backward: 0.09351229667663574, Optimizer: 0
Epoch 1 Batch 121 Train Loss 0.09784255176782608
Total Times. Batch: 121, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011873245239257812, Forward: 0.08116626739501953, Backward: 0.10345101356506348, Optimizer: 0
Epoch 1 Batch 122 Train Loss 0.10510442405939102
Total Times. Batch: 122, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011794567108154297, Forward: 0.08160090446472168, Backward: 0.10326933860778809, Optimizer: 0
Epoch 1 Batch 123 Train Loss 0.09878212213516235
Total Times. Batch: 123, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011932849884033203, Forward: 0.08127546310424805, Backward: 0.10361123085021973, Optimizer: 0
Epoch 1 Batch 124 Train Loss 0.10309012234210968
Total Times. Batch: 124, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011796951293945312, Forward: 0.0811915397644043, Backward: 0.10323095321655273, Optimizer: 0.16716766357421875
Epoch 1 Batch 125 Train Loss 0.09184397757053375
Total Times. Batch: 125, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011875629425048828, Forward: 0.08132195472717285, Backward: 0.09412050247192383, Optimizer: 0
Epoch 1 Batch 126 Train Loss 0.09808453917503357
Total Times. Batch: 126, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012044906616210938, Forward: 0.08155584335327148, Backward: 0.10350608825683594, Optimizer: 0
Epoch 1 Batch 127 Train Loss 0.0993676409125328
Total Times. Batch: 127, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001253366470336914, Forward: 0.08167815208435059, Backward: 0.10309815406799316, Optimizer: 0
Epoch 1 Batch 128 Train Loss 0.1038636714220047
Total Times. Batch: 128, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012171268463134766, Forward: 0.08144736289978027, Backward: 0.10349488258361816, Optimizer: 0
Epoch 1 Batch 129 Train Loss 0.10288622230291367
Total Times. Batch: 129, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011639595031738281, Forward: 0.0811152458190918, Backward: 0.10341405868530273, Optimizer: 0.1671285629272461
Epoch 1 Batch 130 Train Loss 0.0951491966843605
Total Times. Batch: 130, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001142263412475586, Forward: 0.08108878135681152, Backward: 0.09408855438232422, Optimizer: 0
Epoch 1 Batch 131 Train Loss 0.10178079456090927
Total Times. Batch: 131, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012998580932617188, Forward: 0.08144068717956543, Backward: 0.10326814651489258, Optimizer: 0
Epoch 1 Batch 132 Train Loss 0.10339341312646866
Total Times. Batch: 132, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011868476867675781, Forward: 0.08146858215332031, Backward: 0.10326409339904785, Optimizer: 0
Epoch 1 Batch 133 Train Loss 0.09505162388086319
Total Times. Batch: 133, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012230873107910156, Forward: 0.08107638359069824, Backward: 0.10356736183166504, Optimizer: 0
Epoch 1 Batch 134 Train Loss 0.09567556530237198
Total Times. Batch: 134, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011730194091796875, Forward: 0.0822608470916748, Backward: 0.10332083702087402, Optimizer: 0.1674818992614746
Epoch 1 Batch 135 Train Loss 0.08979139477014542
Total Times. Batch: 135, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011949539184570312, Forward: 0.08104586601257324, Backward: 0.09419417381286621, Optimizer: 0
Epoch 1 Batch 136 Train Loss 0.09664968401193619
Total Times. Batch: 136, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013704299926757812, Forward: 0.08124613761901855, Backward: 0.10329937934875488, Optimizer: 0
Epoch 1 Batch 137 Train Loss 0.10436031967401505
Total Times. Batch: 137, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011937618255615234, Forward: 0.08150386810302734, Backward: 0.1031033992767334, Optimizer: 0
Epoch 1 Batch 138 Train Loss 0.10690774768590927
Total Times. Batch: 138, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001216888427734375, Forward: 0.08146524429321289, Backward: 0.10315275192260742, Optimizer: 0
Epoch 1 Batch 139 Train Loss 0.0986383706331253
Total Times. Batch: 139, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011453628540039062, Forward: 0.08112812042236328, Backward: 0.10347199440002441, Optimizer: 0.16836285591125488
Epoch 1 Batch 140 Train Loss 0.10617519915103912
Total Times. Batch: 140, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011599063873291016, Forward: 0.08140707015991211, Backward: 0.09360074996948242, Optimizer: 0
Epoch 1 Batch 141 Train Loss 0.097512386739254
Total Times. Batch: 141, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011539459228515625, Forward: 0.0812835693359375, Backward: 0.10343670845031738, Optimizer: 0
Epoch 1 Batch 142 Train Loss 0.09637397527694702
Total Times. Batch: 142, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.00110626220703125, Forward: 0.08117389678955078, Backward: 0.10330438613891602, Optimizer: 0
Epoch 1 Batch 143 Train Loss 0.10302224010229111
Total Times. Batch: 143, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011959075927734375, Forward: 0.08122014999389648, Backward: 0.10338544845581055, Optimizer: 0
Epoch 1 Batch 144 Train Loss 0.09825574606657028
Total Times. Batch: 144, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001172780990600586, Forward: 0.08175325393676758, Backward: 0.10364222526550293, Optimizer: 0.16687774658203125
Epoch 1 Batch 145 Train Loss 0.10168611258268356
Total Times. Batch: 145, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011839866638183594, Forward: 0.08130431175231934, Backward: 0.09376239776611328, Optimizer: 0
Epoch 1 Batch 146 Train Loss 0.09885714948177338
Total Times. Batch: 146, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013141632080078125, Forward: 0.08135151863098145, Backward: 0.10294175148010254, Optimizer: 0
Epoch 1 Batch 147 Train Loss 0.10417964309453964
Total Times. Batch: 147, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011136531829833984, Forward: 0.08122873306274414, Backward: 0.103240966796875, Optimizer: 0
Epoch 1 Batch 148 Train Loss 0.10034199804067612
Total Times. Batch: 148, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001138925552368164, Forward: 0.08112120628356934, Backward: 0.10331535339355469, Optimizer: 0
Epoch 1 Batch 149 Train Loss 0.10688110440969467
Total Times. Batch: 149, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011107921600341797, Forward: 0.0811166763305664, Backward: 0.10371255874633789, Optimizer: 0.16756725311279297
Epoch 1 Batch 150 Train Loss 0.09452790021896362
Total Times. Batch: 150, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011799335479736328, Forward: 0.08136439323425293, Backward: 0.09467601776123047, Optimizer: 0
Epoch 1 Batch 151 Train Loss 0.10070466995239258
Total Times. Batch: 151, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013022422790527344, Forward: 0.08123087882995605, Backward: 0.10332012176513672, Optimizer: 0
Epoch 1 Batch 152 Train Loss 0.0990188792347908
Total Times. Batch: 152, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012364387512207031, Forward: 0.08133697509765625, Backward: 0.10309839248657227, Optimizer: 0
Epoch 1 Batch 153 Train Loss 0.1031118631362915
Total Times. Batch: 153, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001127004623413086, Forward: 0.08201098442077637, Backward: 0.10284662246704102, Optimizer: 0
Epoch 1 Batch 154 Train Loss 0.09596912562847137
Total Times. Batch: 154, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011458396911621094, Forward: 0.08146071434020996, Backward: 0.10315155982971191, Optimizer: 0.1667931079864502
Epoch 1 Batch 155 Train Loss 0.09010608494281769
Total Times. Batch: 155, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012180805206298828, Forward: 0.08133077621459961, Backward: 0.09403634071350098, Optimizer: 0
Epoch 1 Batch 156 Train Loss 0.08965549618005753
Total Times. Batch: 156, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013039112091064453, Forward: 0.08116316795349121, Backward: 0.10335803031921387, Optimizer: 0
Epoch 1 Batch 157 Train Loss 0.0996074229478836
Total Times. Batch: 157, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012440681457519531, Forward: 0.08146238327026367, Backward: 0.10336112976074219, Optimizer: 0
Epoch 1 Batch 158 Train Loss 0.09557570517063141
Total Times. Batch: 158, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011985301971435547, Forward: 0.08131742477416992, Backward: 0.1032247543334961, Optimizer: 0
Epoch 1 Batch 159 Train Loss 0.09818311780691147
Total Times. Batch: 159, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001163482666015625, Forward: 0.08120560646057129, Backward: 0.10324859619140625, Optimizer: 0.16817951202392578
Epoch 1 Batch 160 Train Loss 0.08926389366388321
Total Times. Batch: 160, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011396408081054688, Forward: 0.08102846145629883, Backward: 0.09385371208190918, Optimizer: 0
Epoch 1 Batch 161 Train Loss 0.09861630946397781
Total Times. Batch: 161, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012018680572509766, Forward: 0.08128237724304199, Backward: 0.10336899757385254, Optimizer: 0
Epoch 1 Batch 162 Train Loss 0.09631913900375366
Total Times. Batch: 162, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0014057159423828125, Forward: 0.08106279373168945, Backward: 0.10357332229614258, Optimizer: 0
Epoch 1 Batch 163 Train Loss 0.09249831736087799
Total Times. Batch: 163, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012712478637695312, Forward: 0.08164381980895996, Backward: 0.10335564613342285, Optimizer: 0
Epoch 1 Batch 164 Train Loss 0.09143810719251633
Total Times. Batch: 164, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001215219497680664, Forward: 0.08131957054138184, Backward: 0.10338592529296875, Optimizer: 0.16630887985229492
Epoch 1 Batch 165 Train Loss 0.08850635588169098
Total Times. Batch: 165, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011949539184570312, Forward: 0.08131694793701172, Backward: 0.09377074241638184, Optimizer: 0
Epoch 1 Batch 166 Train Loss 0.09503008425235748
Total Times. Batch: 166, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012569427490234375, Forward: 0.08141827583312988, Backward: 0.10318303108215332, Optimizer: 0
Epoch 1 Batch 167 Train Loss 0.09499116986989975
Total Times. Batch: 167, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012142658233642578, Forward: 0.0816030502319336, Backward: 0.10316038131713867, Optimizer: 0
Epoch 1 Batch 168 Train Loss 0.10018439590930939
Total Times. Batch: 168, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001203298568725586, Forward: 0.08127260208129883, Backward: 0.10370850563049316, Optimizer: 0
Epoch 1 Batch 169 Train Loss 0.09911557286977768
Total Times. Batch: 169, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011458396911621094, Forward: 0.08155298233032227, Backward: 0.10304069519042969, Optimizer: 0.16728949546813965
Epoch 1 Batch 170 Train Loss 0.09333635866641998
Total Times. Batch: 170, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012099742889404297, Forward: 0.08124542236328125, Backward: 0.09387040138244629, Optimizer: 0
Epoch 1 Batch 171 Train Loss 0.08618315309286118
Total Times. Batch: 171, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011777877807617188, Forward: 0.08136320114135742, Backward: 0.10314798355102539, Optimizer: 0
Epoch 1 Batch 172 Train Loss 0.09494506567716599
Total Times. Batch: 172, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011501312255859375, Forward: 0.08138465881347656, Backward: 0.10336589813232422, Optimizer: 0
Epoch 1 Batch 173 Train Loss 0.09381552785634995
Total Times. Batch: 173, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001178741455078125, Forward: 0.0814521312713623, Backward: 0.1034553050994873, Optimizer: 0
Epoch 1 Batch 174 Train Loss 0.10179585218429565
Total Times. Batch: 174, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011801719665527344, Forward: 0.08136272430419922, Backward: 0.10392594337463379, Optimizer: 0.16762685775756836
Epoch 1 Batch 175 Train Loss 0.10493350028991699
Total Times. Batch: 175, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011601448059082031, Forward: 0.08144116401672363, Backward: 0.09359860420227051, Optimizer: 0
Epoch 1 Batch 176 Train Loss 0.10153312981128693
Total Times. Batch: 176, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012021064758300781, Forward: 0.08122086524963379, Backward: 0.10354304313659668, Optimizer: 0
Epoch 1 Batch 177 Train Loss 0.10107848793268204
Total Times. Batch: 177, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012028217315673828, Forward: 0.08129763603210449, Backward: 0.10335278511047363, Optimizer: 0
Epoch 1 Batch 178 Train Loss 0.09365431219339371
Total Times. Batch: 178, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011601448059082031, Forward: 0.08127403259277344, Backward: 0.1033029556274414, Optimizer: 0
Epoch 1 Batch 179 Train Loss 0.105204738676548
Total Times. Batch: 179, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001171112060546875, Forward: 0.08149075508117676, Backward: 0.10303449630737305, Optimizer: 0.16696500778198242
Epoch 1 Batch 180 Train Loss 0.10024157911539078
Total Times. Batch: 180, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011754035949707031, Forward: 0.08155274391174316, Backward: 0.0938413143157959, Optimizer: 0
Epoch 1 Batch 181 Train Loss 0.10510467737913132
Total Times. Batch: 181, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012843608856201172, Forward: 0.08140063285827637, Backward: 0.1031649112701416, Optimizer: 0
Epoch 1 Batch 182 Train Loss 0.09577053040266037
Total Times. Batch: 182, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001216888427734375, Forward: 0.08228087425231934, Backward: 0.10344052314758301, Optimizer: 0
Epoch 1 Batch 183 Train Loss 0.1041177436709404
Total Times. Batch: 183, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011942386627197266, Forward: 0.08202910423278809, Backward: 0.1031484603881836, Optimizer: 0
Epoch 1 Batch 184 Train Loss 0.09765809774398804
Total Times. Batch: 184, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001146554946899414, Forward: 0.08122873306274414, Backward: 0.1033174991607666, Optimizer: 0.16673970222473145
Epoch 1 Batch 185 Train Loss 0.10149450600147247
Total Times. Batch: 185, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012159347534179688, Forward: 0.08114790916442871, Backward: 0.09410262107849121, Optimizer: 0
Epoch 1 Batch 186 Train Loss 0.10959070175886154
Total Times. Batch: 186, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0016736984252929688, Forward: 0.08137321472167969, Backward: 0.10328793525695801, Optimizer: 0
Epoch 1 Batch 187 Train Loss 0.08969195932149887
Total Times. Batch: 187, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001154184341430664, Forward: 0.08148074150085449, Backward: 0.1033182144165039, Optimizer: 0
Epoch 1 Batch 188 Train Loss 0.10249795764684677
Total Times. Batch: 188, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010967254638671875, Forward: 0.08187627792358398, Backward: 0.1030280590057373, Optimizer: 0
Epoch 1 Batch 189 Train Loss 0.0942152887582779
Total Times. Batch: 189, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011906623840332031, Forward: 0.08177471160888672, Backward: 0.1031796932220459, Optimizer: 0.16823124885559082
Epoch 1 Batch 190 Train Loss 0.08955990523099899
Total Times. Batch: 190, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011458396911621094, Forward: 0.08137798309326172, Backward: 0.09366202354431152, Optimizer: 0
Epoch 1 Batch 191 Train Loss 0.09939243644475937
Total Times. Batch: 191, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011761188507080078, Forward: 0.08122515678405762, Backward: 0.10322952270507812, Optimizer: 0
Epoch 1 Batch 192 Train Loss 0.09786150604486465
Total Times. Batch: 192, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011980533599853516, Forward: 0.08145642280578613, Backward: 0.10422706604003906, Optimizer: 0
Epoch 1 Batch 193 Train Loss 0.08631011843681335
Total Times. Batch: 193, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012044906616210938, Forward: 0.08164072036743164, Backward: 0.10347843170166016, Optimizer: 0
Epoch 1 Batch 194 Train Loss 0.095963254570961
Total Times. Batch: 194, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011966228485107422, Forward: 0.08116364479064941, Backward: 0.10332322120666504, Optimizer: 0.16691184043884277
Epoch 1 Batch 195 Train Loss 0.09993242472410202
Total Times. Batch: 195, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0014138221740722656, Forward: 0.08126950263977051, Backward: 0.09366917610168457, Optimizer: 0
Epoch 1 Batch 196 Train Loss 0.10300929844379425
Total Times. Batch: 196, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012526512145996094, Forward: 0.08231401443481445, Backward: 0.10319256782531738, Optimizer: 0
Epoch 1 Batch 197 Train Loss 0.09693564474582672
Total Times. Batch: 197, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012314319610595703, Forward: 0.08143115043640137, Backward: 0.10343670845031738, Optimizer: 0
Epoch 1 Batch 198 Train Loss 0.09546428173780441
Total Times. Batch: 198, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011844635009765625, Forward: 0.08109450340270996, Backward: 0.10346007347106934, Optimizer: 0
Epoch 1 Batch 199 Train Loss 0.10436403751373291
Total Times. Batch: 199, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012688636779785156, Forward: 0.08148431777954102, Backward: 0.10321998596191406, Optimizer: 0.1680011749267578
Epoch 1 Batch 200 Train Loss 0.09560126066207886
Total Times. Batch: 200, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012030601501464844, Forward: 0.08138585090637207, Backward: 0.09403777122497559, Optimizer: 0
Epoch 1 Batch 201 Train Loss 0.10740788280963898
Total Times. Batch: 201, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001196146011352539, Forward: 0.08188509941101074, Backward: 0.10353875160217285, Optimizer: 0
Epoch 1 Batch 202 Train Loss 0.09468898177146912
Total Times. Batch: 202, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011432170867919922, Forward: 0.0813753604888916, Backward: 0.10328388214111328, Optimizer: 0
Epoch 1 Batch 203 Train Loss 0.09748383611440659
Total Times. Batch: 203, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011920928955078125, Forward: 0.0813910961151123, Backward: 0.10327529907226562, Optimizer: 0
Epoch 1 Batch 204 Train Loss 0.10281238704919815
Total Times. Batch: 204, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011854171752929688, Forward: 0.08113837242126465, Backward: 0.10327434539794922, Optimizer: 0.1678152084350586
Epoch 1 Batch 205 Train Loss 0.10456087440252304
Total Times. Batch: 205, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012197494506835938, Forward: 0.08154034614562988, Backward: 0.09380912780761719, Optimizer: 0
Epoch 1 Batch 206 Train Loss 0.09865250438451767
Total Times. Batch: 206, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001256704330444336, Forward: 0.08126163482666016, Backward: 0.1033318042755127, Optimizer: 0
Epoch 1 Batch 207 Train Loss 0.0946810394525528
Total Times. Batch: 207, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012087821960449219, Forward: 0.08111429214477539, Backward: 0.10324549674987793, Optimizer: 0
Epoch 1 Batch 208 Train Loss 0.09068077057600021
Total Times. Batch: 208, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011622905731201172, Forward: 0.08125448226928711, Backward: 0.10335302352905273, Optimizer: 0
Epoch 1 Batch 209 Train Loss 0.1055472120642662
Total Times. Batch: 209, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011906623840332031, Forward: 0.08160281181335449, Backward: 0.10324954986572266, Optimizer: 0.16817021369934082
Epoch 1 Batch 210 Train Loss 0.09451790899038315
Total Times. Batch: 210, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.00124359130859375, Forward: 0.0815269947052002, Backward: 0.09415221214294434, Optimizer: 0
Epoch 1 Batch 211 Train Loss 0.09739656001329422
Total Times. Batch: 211, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012469291687011719, Forward: 0.08119630813598633, Backward: 0.1032872200012207, Optimizer: 0
Epoch 1 Batch 212 Train Loss 0.10618292540311813
Total Times. Batch: 212, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011837482452392578, Forward: 0.08141970634460449, Backward: 0.10352158546447754, Optimizer: 0
Epoch 1 Batch 213 Train Loss 0.09723260253667831
Total Times. Batch: 213, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011830329895019531, Forward: 0.08120059967041016, Backward: 0.10345911979675293, Optimizer: 0
Epoch 1 Batch 214 Train Loss 0.09159483015537262
Total Times. Batch: 214, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011105537414550781, Forward: 0.08111405372619629, Backward: 0.10303854942321777, Optimizer: 0.1681690216064453
Epoch 1 Batch 215 Train Loss 0.09764756262302399
Total Times. Batch: 215, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011911392211914062, Forward: 0.08123278617858887, Backward: 0.09376907348632812, Optimizer: 0
Epoch 1 Batch 216 Train Loss 0.09455300867557526
Total Times. Batch: 216, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012924671173095703, Forward: 0.08185124397277832, Backward: 0.10341548919677734, Optimizer: 0
Epoch 1 Batch 217 Train Loss 0.093589186668396
Total Times. Batch: 217, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012094974517822266, Forward: 0.08243083953857422, Backward: 0.10353302955627441, Optimizer: 0
Epoch 1 Batch 218 Train Loss 0.0929904505610466
Total Times. Batch: 218, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012104511260986328, Forward: 0.08140969276428223, Backward: 0.10358047485351562, Optimizer: 0
Epoch 1 Batch 219 Train Loss 0.09904437512159348
Total Times. Batch: 219, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001157999038696289, Forward: 0.08109664916992188, Backward: 0.10325384140014648, Optimizer: 0.16829323768615723
Epoch 1 Batch 220 Train Loss 0.1125219389796257
Total Times. Batch: 220, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011456012725830078, Forward: 0.08158183097839355, Backward: 0.09384727478027344, Optimizer: 0
Epoch 1 Batch 221 Train Loss 0.10365386307239532
Total Times. Batch: 221, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012056827545166016, Forward: 0.08138442039489746, Backward: 0.10318827629089355, Optimizer: 0
Epoch 1 Batch 222 Train Loss 0.08623311668634415
Total Times. Batch: 222, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011510848999023438, Forward: 0.08223485946655273, Backward: 0.10320520401000977, Optimizer: 0
Epoch 1 Batch 223 Train Loss 0.09012981504201889
Total Times. Batch: 223, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011527538299560547, Forward: 0.08191847801208496, Backward: 0.10351347923278809, Optimizer: 0
Epoch 1 Batch 224 Train Loss 0.09297428280115128
Total Times. Batch: 224, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001207590103149414, Forward: 0.08097314834594727, Backward: 0.10337710380554199, Optimizer: 0.16802501678466797
Epoch 1 Batch 225 Train Loss 0.10257747024297714
Total Times. Batch: 225, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011723041534423828, Forward: 0.08108401298522949, Backward: 0.09378433227539062, Optimizer: 0
Epoch 1 Batch 226 Train Loss 0.0973655954003334
Total Times. Batch: 226, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001260995864868164, Forward: 0.08113265037536621, Backward: 0.10336089134216309, Optimizer: 0
Epoch 1 Batch 227 Train Loss 0.09425384551286697
Total Times. Batch: 227, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012066364288330078, Forward: 0.08140039443969727, Backward: 0.1034080982208252, Optimizer: 0
Epoch 1 Batch 228 Train Loss 0.09770631045103073
Total Times. Batch: 228, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011730194091796875, Forward: 0.08161377906799316, Backward: 0.10349822044372559, Optimizer: 0
Epoch 1 Batch 229 Train Loss 0.10057801008224487
Total Times. Batch: 229, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001283884048461914, Forward: 0.08143067359924316, Backward: 0.10320687294006348, Optimizer: 0.16846776008605957
Epoch 1 Batch 230 Train Loss 0.09788651019334793
Total Times. Batch: 230, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011873245239257812, Forward: 0.08082342147827148, Backward: 0.09410929679870605, Optimizer: 0
Epoch 1 Batch 231 Train Loss 0.09266670048236847
Total Times. Batch: 231, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011987686157226562, Forward: 0.08156323432922363, Backward: 0.10297322273254395, Optimizer: 0
Epoch 1 Batch 232 Train Loss 0.10676620155572891
Total Times. Batch: 232, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011410713195800781, Forward: 0.08224296569824219, Backward: 0.10316920280456543, Optimizer: 0
Epoch 1 Batch 233 Train Loss 0.10064893960952759
Total Times. Batch: 233, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011620521545410156, Forward: 0.08141088485717773, Backward: 0.10346484184265137, Optimizer: 0
Epoch 1 Batch 234 Train Loss 0.09668556600809097
Total Times. Batch: 234, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001374959945678711, Forward: 0.08101773262023926, Backward: 0.10356378555297852, Optimizer: 0.16788697242736816
Epoch 1 Batch 235 Train Loss 0.09002860635519028
Total Times. Batch: 235, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012149810791015625, Forward: 0.08115696907043457, Backward: 0.09370255470275879, Optimizer: 0
Epoch 1 Batch 236 Train Loss 0.09632337838411331
Total Times. Batch: 236, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001210927963256836, Forward: 0.0812833309173584, Backward: 0.10340285301208496, Optimizer: 0
Epoch 1 Batch 237 Train Loss 0.08925633877515793
Total Times. Batch: 237, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001192331314086914, Forward: 0.08147287368774414, Backward: 0.10309720039367676, Optimizer: 0
Epoch 1 Batch 238 Train Loss 0.0997302457690239
Total Times. Batch: 238, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011577606201171875, Forward: 0.08117556571960449, Backward: 0.10311603546142578, Optimizer: 0
Epoch 1 Batch 239 Train Loss 0.08761507272720337
Total Times. Batch: 239, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011887550354003906, Forward: 0.08134675025939941, Backward: 0.10355687141418457, Optimizer: 0.16787505149841309
Epoch 1 Batch 240 Train Loss 0.09505461901426315
Total Times. Batch: 240, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012211799621582031, Forward: 0.08112239837646484, Backward: 0.0936276912689209, Optimizer: 0
Epoch 1 Batch 241 Train Loss 0.099608413875103
Total Times. Batch: 241, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012297630310058594, Forward: 0.08183097839355469, Backward: 0.10333490371704102, Optimizer: 0
Epoch 1 Batch 242 Train Loss 0.10328789800405502
Total Times. Batch: 242, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011043548583984375, Forward: 0.0822300910949707, Backward: 0.10365700721740723, Optimizer: 0
Epoch 1 Batch 243 Train Loss 0.0965232327580452
Total Times. Batch: 243, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011131763458251953, Forward: 0.0817253589630127, Backward: 0.10352778434753418, Optimizer: 0
Epoch 1 Batch 244 Train Loss 0.09214992076158524
Total Times. Batch: 244, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011360645294189453, Forward: 0.08152008056640625, Backward: 0.10337281227111816, Optimizer: 0.16800379753112793
Epoch 1 Batch 245 Train Loss 0.10349533706903458
Total Times. Batch: 245, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001195669174194336, Forward: 0.08136773109436035, Backward: 0.09404468536376953, Optimizer: 0
Epoch 1 Batch 246 Train Loss 0.10109994560480118
Total Times. Batch: 246, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0015752315521240234, Forward: 0.08125543594360352, Backward: 0.10322237014770508, Optimizer: 0
Epoch 1 Batch 247 Train Loss 0.1017494946718216
Total Times. Batch: 247, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001142263412475586, Forward: 0.08156228065490723, Backward: 0.10280394554138184, Optimizer: 0
Epoch 1 Batch 248 Train Loss 0.0921168327331543
Total Times. Batch: 248, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011806488037109375, Forward: 0.08116865158081055, Backward: 0.10345125198364258, Optimizer: 0
Epoch 1 Batch 249 Train Loss 0.09676311165094376
Total Times. Batch: 249, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010917186737060547, Forward: 0.08105993270874023, Backward: 0.10375070571899414, Optimizer: 0.16805291175842285
Epoch 1 Batch 250 Train Loss 0.087842658162117
Total Times. Batch: 250, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011410713195800781, Forward: 0.08122992515563965, Backward: 0.09379792213439941, Optimizer: 0
Epoch 1 Batch 251 Train Loss 0.10585995018482208
Total Times. Batch: 251, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0014426708221435547, Forward: 0.08140850067138672, Backward: 0.10264968872070312, Optimizer: 0
Epoch 1 Batch 252 Train Loss 0.09456400573253632
Total Times. Batch: 252, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012810230255126953, Forward: 0.08111763000488281, Backward: 0.10358214378356934, Optimizer: 0
Epoch 1 Batch 253 Train Loss 0.0994662418961525
Total Times. Batch: 253, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011467933654785156, Forward: 0.0815587043762207, Backward: 0.10332894325256348, Optimizer: 0
Epoch 1 Batch 254 Train Loss 0.0949094146490097
Total Times. Batch: 254, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011239051818847656, Forward: 0.08125495910644531, Backward: 0.10344362258911133, Optimizer: 0.16681814193725586
Epoch 1 Batch 255 Train Loss 0.10145924240350723
Total Times. Batch: 255, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010976791381835938, Forward: 0.08112835884094238, Backward: 0.0939788818359375, Optimizer: 0
Epoch 1 Batch 256 Train Loss 0.09541318565607071
Total Times. Batch: 256, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011627674102783203, Forward: 0.08245110511779785, Backward: 0.10336589813232422, Optimizer: 0
Epoch 1 Batch 257 Train Loss 0.09931489080190659
Total Times. Batch: 257, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012845993041992188, Forward: 0.08150720596313477, Backward: 0.10324883460998535, Optimizer: 0
Epoch 1 Batch 258 Train Loss 0.0920230969786644
Total Times. Batch: 258, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011763572692871094, Forward: 0.08112812042236328, Backward: 0.10315155982971191, Optimizer: 0
Epoch 1 Batch 259 Train Loss 0.0981345847249031
Total Times. Batch: 259, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013051033020019531, Forward: 0.08162951469421387, Backward: 0.10337328910827637, Optimizer: 0.167097806930542
Epoch 1 Batch 260 Train Loss 0.10502641648054123
Total Times. Batch: 260, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011878013610839844, Forward: 0.08086061477661133, Backward: 0.09403300285339355, Optimizer: 0
Epoch 1 Batch 261 Train Loss 0.09848741441965103
Total Times. Batch: 261, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012841224670410156, Forward: 0.08118438720703125, Backward: 0.1036837100982666, Optimizer: 0
Epoch 1 Batch 262 Train Loss 0.09318924695253372
Total Times. Batch: 262, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011456012725830078, Forward: 0.08158636093139648, Backward: 0.10270309448242188, Optimizer: 0
Epoch 1 Batch 263 Train Loss 0.09279387444257736
Total Times. Batch: 263, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012302398681640625, Forward: 0.08114242553710938, Backward: 0.10325193405151367, Optimizer: 0
Epoch 1 Batch 264 Train Loss 0.1020236611366272
Total Times. Batch: 264, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012574195861816406, Forward: 0.08144760131835938, Backward: 0.10340404510498047, Optimizer: 0.16847968101501465
Epoch 1 Batch 265 Train Loss 0.10565676540136337
Total Times. Batch: 265, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011920928955078125, Forward: 0.08127117156982422, Backward: 0.09333992004394531, Optimizer: 0
Epoch 1 Batch 266 Train Loss 0.09396924823522568
Total Times. Batch: 266, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012545585632324219, Forward: 0.0811011791229248, Backward: 0.10367321968078613, Optimizer: 0
Epoch 1 Batch 267 Train Loss 0.09081476181745529
Total Times. Batch: 267, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011668205261230469, Forward: 0.0811760425567627, Backward: 0.10333871841430664, Optimizer: 0
Epoch 1 Batch 268 Train Loss 0.09544868767261505
Total Times. Batch: 268, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011773109436035156, Forward: 0.08131814002990723, Backward: 0.10328507423400879, Optimizer: 0
Epoch 1 Batch 269 Train Loss 0.09983380138874054
Total Times. Batch: 269, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012080669403076172, Forward: 0.08142590522766113, Backward: 0.1035008430480957, Optimizer: 0.1669321060180664
Epoch 1 Batch 270 Train Loss 0.09603797644376755
Total Times. Batch: 270, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011589527130126953, Forward: 0.0819244384765625, Backward: 0.09404420852661133, Optimizer: 0
Epoch 1 Batch 271 Train Loss 0.0968342274427414
Total Times. Batch: 271, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012516975402832031, Forward: 0.08171391487121582, Backward: 0.1030728816986084, Optimizer: 0
Epoch 1 Batch 272 Train Loss 0.09039489179849625
Total Times. Batch: 272, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011944770812988281, Forward: 0.08211827278137207, Backward: 0.1034390926361084, Optimizer: 0
Epoch 1 Batch 273 Train Loss 0.09982867538928986
Total Times. Batch: 273, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011684894561767578, Forward: 0.08113265037536621, Backward: 0.1033620834350586, Optimizer: 0
Epoch 1 Batch 274 Train Loss 0.09280729293823242
Total Times. Batch: 274, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011591911315917969, Forward: 0.08114194869995117, Backward: 0.10337042808532715, Optimizer: 0.1686556339263916
Epoch 1 Batch 275 Train Loss 0.0990012064576149
Total Times. Batch: 275, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001194000244140625, Forward: 0.0814976692199707, Backward: 0.09339618682861328, Optimizer: 0
Epoch 1 Batch 276 Train Loss 0.09795460850000381
Total Times. Batch: 276, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011851787567138672, Forward: 0.08127379417419434, Backward: 0.10329675674438477, Optimizer: 0
Epoch 1 Batch 277 Train Loss 0.09970899671316147
Total Times. Batch: 277, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012645721435546875, Forward: 0.08138298988342285, Backward: 0.10354423522949219, Optimizer: 0
Epoch 1 Batch 278 Train Loss 0.08983442187309265
Total Times. Batch: 278, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011909008026123047, Forward: 0.08129000663757324, Backward: 0.10339546203613281, Optimizer: 0
Epoch 1 Batch 279 Train Loss 0.09133492410182953
Total Times. Batch: 279, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011625289916992188, Forward: 0.0812993049621582, Backward: 0.10369706153869629, Optimizer: 0.16736865043640137
Epoch 1 Batch 280 Train Loss 0.09797033667564392
Total Times. Batch: 280, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001165628433227539, Forward: 0.08133459091186523, Backward: 0.09351539611816406, Optimizer: 0
Epoch 1 Batch 281 Train Loss 0.09924079477787018
Total Times. Batch: 281, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012996196746826172, Forward: 0.08132266998291016, Backward: 0.10319280624389648, Optimizer: 0
Epoch 1 Batch 282 Train Loss 0.09554344415664673
Total Times. Batch: 282, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011742115020751953, Forward: 0.08160543441772461, Backward: 0.1038205623626709, Optimizer: 0
Epoch 1 Batch 283 Train Loss 0.09805422276258469
Total Times. Batch: 283, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012319087982177734, Forward: 0.0810539722442627, Backward: 0.10347747802734375, Optimizer: 0
Epoch 1 Batch 284 Train Loss 0.09736688435077667
Total Times. Batch: 284, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012254714965820312, Forward: 0.08097457885742188, Backward: 0.10345745086669922, Optimizer: 0.1674966812133789
Epoch 1 Batch 285 Train Loss 0.09116695076227188
Total Times. Batch: 285, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001089334487915039, Forward: 0.0814509391784668, Backward: 0.09350967407226562, Optimizer: 0
Epoch 1 Batch 286 Train Loss 0.09092873334884644
Total Times. Batch: 286, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011029243469238281, Forward: 0.08116412162780762, Backward: 0.10333991050720215, Optimizer: 0
Epoch 1 Batch 287 Train Loss 0.10655176639556885
Total Times. Batch: 287, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012111663818359375, Forward: 0.0812845230102539, Backward: 0.10355782508850098, Optimizer: 0
Epoch 1 Batch 288 Train Loss 0.09498696774244308
Total Times. Batch: 288, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013680458068847656, Forward: 0.08177304267883301, Backward: 0.10299420356750488, Optimizer: 0
Epoch 1 Batch 289 Train Loss 0.10353245586156845
Total Times. Batch: 289, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001146078109741211, Forward: 0.08125638961791992, Backward: 0.10333681106567383, Optimizer: 0.16842126846313477
Epoch 1 Batch 290 Train Loss 0.08427849411964417
Total Times. Batch: 290, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001180887222290039, Forward: 0.08102607727050781, Backward: 0.09403657913208008, Optimizer: 0
Epoch 1 Batch 291 Train Loss 0.0874079093337059
Total Times. Batch: 291, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012009143829345703, Forward: 0.08131074905395508, Backward: 0.10334944725036621, Optimizer: 0
Epoch 1 Batch 292 Train Loss 0.08906441926956177
Total Times. Batch: 292, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011477470397949219, Forward: 0.08129405975341797, Backward: 0.10340332984924316, Optimizer: 0
Epoch 1 Batch 293 Train Loss 0.09919416159391403
Total Times. Batch: 293, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012311935424804688, Forward: 0.081207275390625, Backward: 0.1033182144165039, Optimizer: 0
Epoch 1 Batch 294 Train Loss 0.10044516623020172
Total Times. Batch: 294, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011138916015625, Forward: 0.08196067810058594, Backward: 0.10379147529602051, Optimizer: 0.1667478084564209
Epoch 1 Batch 295 Train Loss 0.10534007847309113
Total Times. Batch: 295, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011725425720214844, Forward: 0.08131933212280273, Backward: 0.09349465370178223, Optimizer: 0
Epoch 1 Batch 296 Train Loss 0.09469646215438843
Total Times. Batch: 296, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012922286987304688, Forward: 0.08120083808898926, Backward: 0.10339856147766113, Optimizer: 0
Epoch 1 Batch 297 Train Loss 0.09444420039653778
Total Times. Batch: 297, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011684894561767578, Forward: 0.08125185966491699, Backward: 0.10353994369506836, Optimizer: 0
Epoch 1 Batch 298 Train Loss 0.09030032902956009
Total Times. Batch: 298, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001131296157836914, Forward: 0.08172392845153809, Backward: 0.10346007347106934, Optimizer: 0
Epoch 1 Batch 299 Train Loss 0.09423837065696716
Total Times. Batch: 299, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001203298568725586, Forward: 0.08119702339172363, Backward: 0.10357141494750977, Optimizer: 0.16891884803771973
Epoch 1 Batch 300 Train Loss 0.09160641580820084
Total Times. Batch: 300, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012235641479492188, Forward: 0.0814659595489502, Backward: 0.0937960147857666, Optimizer: 0
Epoch 1 Batch 301 Train Loss 0.09939507395029068
Total Times. Batch: 301, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0015316009521484375, Forward: 0.08153319358825684, Backward: 0.10326123237609863, Optimizer: 0
Epoch 1 Batch 302 Train Loss 0.09941210597753525
Total Times. Batch: 302, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001172780990600586, Forward: 0.08096814155578613, Backward: 0.10366582870483398, Optimizer: 0
Epoch 1 Batch 303 Train Loss 0.09979196637868881
Total Times. Batch: 303, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001171112060546875, Forward: 0.08145928382873535, Backward: 0.10340642929077148, Optimizer: 0
Epoch 1 Batch 304 Train Loss 0.09592578560113907
Total Times. Batch: 304, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001138925552368164, Forward: 0.08104419708251953, Backward: 0.10327863693237305, Optimizer: 0.1682906150817871
Epoch 1 Batch 305 Train Loss 0.10150694102048874
Total Times. Batch: 305, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011849403381347656, Forward: 0.08129715919494629, Backward: 0.09399986267089844, Optimizer: 0
Epoch 1 Batch 306 Train Loss 0.09349284321069717
Total Times. Batch: 306, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.00128936767578125, Forward: 0.08211231231689453, Backward: 0.10326695442199707, Optimizer: 0
Epoch 1 Batch 307 Train Loss 0.10267791897058487
Total Times. Batch: 307, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012061595916748047, Forward: 0.0812835693359375, Backward: 0.1035759449005127, Optimizer: 0
Epoch 1 Batch 308 Train Loss 0.09065883606672287
Total Times. Batch: 308, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012164115905761719, Forward: 0.08123445510864258, Backward: 0.10319733619689941, Optimizer: 0
Epoch 1 Batch 309 Train Loss 0.09450451284646988
Total Times. Batch: 309, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011749267578125, Forward: 0.08227348327636719, Backward: 0.1035318374633789, Optimizer: 0.16844630241394043
Epoch 1 Batch 310 Train Loss 0.0983942300081253
Total Times. Batch: 310, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011603832244873047, Forward: 0.08099102973937988, Backward: 0.09400177001953125, Optimizer: 0
Epoch 1 Batch 311 Train Loss 0.10406193882226944
Total Times. Batch: 311, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001270294189453125, Forward: 0.08181166648864746, Backward: 0.10300898551940918, Optimizer: 0
Epoch 1 Batch 312 Train Loss 0.0902336910367012
Total Times. Batch: 312, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012631416320800781, Forward: 0.08139252662658691, Backward: 0.10305356979370117, Optimizer: 0
Epoch 1 Batch 313 Train Loss 0.1030086800456047
Total Times. Batch: 313, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011584758758544922, Forward: 0.08162999153137207, Backward: 0.10323143005371094, Optimizer: 0
Epoch 1 Batch 314 Train Loss 0.095313660800457
Total Times. Batch: 314, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011889934539794922, Forward: 0.0811302661895752, Backward: 0.10376381874084473, Optimizer: 0.16767334938049316
Epoch 1 Batch 315 Train Loss 0.09206408262252808
Total Times. Batch: 315, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011527538299560547, Forward: 0.08098125457763672, Backward: 0.0940091609954834, Optimizer: 0
Epoch 1 Batch 316 Train Loss 0.10824697464704514
Total Times. Batch: 316, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012619495391845703, Forward: 0.08146333694458008, Backward: 0.10315322875976562, Optimizer: 0
Epoch 1 Batch 317 Train Loss 0.09089275449514389
Total Times. Batch: 317, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012166500091552734, Forward: 0.08129215240478516, Backward: 0.1033170223236084, Optimizer: 0
Epoch 1 Batch 318 Train Loss 0.09819822758436203
Total Times. Batch: 318, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012798309326171875, Forward: 0.08134651184082031, Backward: 0.10341501235961914, Optimizer: 0
Epoch 1 Batch 319 Train Loss 0.10092895478010178
Total Times. Batch: 319, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012927055358886719, Forward: 0.08118152618408203, Backward: 0.10328888893127441, Optimizer: 0.16731023788452148
Epoch 1 Batch 320 Train Loss 0.09329324215650558
Total Times. Batch: 320, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011374950408935547, Forward: 0.08103275299072266, Backward: 0.09440922737121582, Optimizer: 0
Epoch 1 Batch 321 Train Loss 0.08826296031475067
Total Times. Batch: 321, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011601448059082031, Forward: 0.08158063888549805, Backward: 0.10338425636291504, Optimizer: 0
Epoch 1 Batch 322 Train Loss 0.09243180602788925
Total Times. Batch: 322, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011408329010009766, Forward: 0.08136630058288574, Backward: 0.10309147834777832, Optimizer: 0
Epoch 1 Batch 323 Train Loss 0.10143059492111206
Total Times. Batch: 323, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011775493621826172, Forward: 0.08108091354370117, Backward: 0.10329174995422363, Optimizer: 0
Epoch 1 Batch 324 Train Loss 0.09136945009231567
Total Times. Batch: 324, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011780261993408203, Forward: 0.08150553703308105, Backward: 0.10320425033569336, Optimizer: 0.16805076599121094
Epoch 1 Batch 325 Train Loss 0.09630686044692993
Total Times. Batch: 325, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012004375457763672, Forward: 0.08104324340820312, Backward: 0.09391188621520996, Optimizer: 0
Epoch 1 Batch 326 Train Loss 0.1026870608329773
Total Times. Batch: 326, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011801719665527344, Forward: 0.08111906051635742, Backward: 0.10399794578552246, Optimizer: 0
Epoch 1 Batch 327 Train Loss 0.09897719323635101
Total Times. Batch: 327, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011615753173828125, Forward: 0.08140373229980469, Backward: 0.10304546356201172, Optimizer: 0
Epoch 1 Batch 328 Train Loss 0.09631627053022385
Total Times. Batch: 328, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011539459228515625, Forward: 0.08117485046386719, Backward: 0.10333418846130371, Optimizer: 0
Epoch 1 Batch 329 Train Loss 0.0890706479549408
Total Times. Batch: 329, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001177072525024414, Forward: 0.08116388320922852, Backward: 0.10321545600891113, Optimizer: 0.16708731651306152
Epoch 1 Batch 330 Train Loss 0.09688331931829453
Total Times. Batch: 330, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012466907501220703, Forward: 0.08124804496765137, Backward: 0.09384799003601074, Optimizer: 0
Epoch 1 Batch 331 Train Loss 0.08999057859182358
Total Times. Batch: 331, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012848377227783203, Forward: 0.08129048347473145, Backward: 0.10338926315307617, Optimizer: 0
Epoch 1 Batch 332 Train Loss 0.0957571491599083
Total Times. Batch: 332, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012364387512207031, Forward: 0.08132266998291016, Backward: 0.10349607467651367, Optimizer: 0
Epoch 1 Batch 333 Train Loss 0.09183900058269501
Total Times. Batch: 333, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011818408966064453, Forward: 0.08131527900695801, Backward: 0.10336780548095703, Optimizer: 0
Epoch 1 Batch 334 Train Loss 0.08979392051696777
Total Times. Batch: 334, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011835098266601562, Forward: 0.0810401439666748, Backward: 0.10333108901977539, Optimizer: 0.1678323745727539
Epoch 1 Batch 335 Train Loss 0.10188128054141998
Total Times. Batch: 335, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011909008026123047, Forward: 0.08131146430969238, Backward: 0.09341216087341309, Optimizer: 0
Epoch 1 Batch 336 Train Loss 0.09047850966453552
Total Times. Batch: 336, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012540817260742188, Forward: 0.08199930191040039, Backward: 0.1030874252319336, Optimizer: 0
Epoch 1 Batch 337 Train Loss 0.09447276592254639
Total Times. Batch: 337, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0014307498931884766, Forward: 0.08122372627258301, Backward: 0.10322356224060059, Optimizer: 0
Epoch 1 Batch 338 Train Loss 0.10459041595458984
Total Times. Batch: 338, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011761188507080078, Forward: 0.08116769790649414, Backward: 0.10365509986877441, Optimizer: 0
Epoch 1 Batch 339 Train Loss 0.09814523905515671
Total Times. Batch: 339, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011754035949707031, Forward: 0.08246684074401855, Backward: 0.10336661338806152, Optimizer: 0.16680598258972168
Epoch 1 Batch 340 Train Loss 0.10149548202753067
Total Times. Batch: 340, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001134634017944336, Forward: 0.08116865158081055, Backward: 0.0939781665802002, Optimizer: 0
Epoch 1 Batch 341 Train Loss 0.10619892925024033
Total Times. Batch: 341, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011317729949951172, Forward: 0.08142280578613281, Backward: 0.10352349281311035, Optimizer: 0
Epoch 1 Batch 342 Train Loss 0.09318818151950836
Total Times. Batch: 342, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011684894561767578, Forward: 0.0816335678100586, Backward: 0.10277462005615234, Optimizer: 0
Epoch 1 Batch 343 Train Loss 0.10128893703222275
Total Times. Batch: 343, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001260519027709961, Forward: 0.0811607837677002, Backward: 0.1032862663269043, Optimizer: 0
Epoch 1 Batch 344 Train Loss 0.09096860885620117
Total Times. Batch: 344, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011587142944335938, Forward: 0.08143901824951172, Backward: 0.10340380668640137, Optimizer: 0.16636371612548828
Epoch 1 Batch 345 Train Loss 0.09756818413734436
Total Times. Batch: 345, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011794567108154297, Forward: 0.08102726936340332, Backward: 0.09397649765014648, Optimizer: 0
Epoch 1 Batch 346 Train Loss 0.09294097870588303
Total Times. Batch: 346, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012369155883789062, Forward: 0.08099889755249023, Backward: 0.1035451889038086, Optimizer: 0
Epoch 1 Batch 347 Train Loss 0.08734920620918274
Total Times. Batch: 347, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011916160583496094, Forward: 0.08166933059692383, Backward: 0.10296392440795898, Optimizer: 0
Epoch 1 Batch 348 Train Loss 0.09467222541570663
Total Times. Batch: 348, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012819766998291016, Forward: 0.08137774467468262, Backward: 0.10343217849731445, Optimizer: 0
Epoch 1 Batch 349 Train Loss 0.09646475315093994
Total Times. Batch: 349, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.00121307373046875, Forward: 0.08122038841247559, Backward: 0.10314249992370605, Optimizer: 0.16729736328125
Epoch 1 Batch 350 Train Loss 0.0957847610116005
Total Times. Batch: 350, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0014531612396240234, Forward: 0.08152890205383301, Backward: 0.09383487701416016, Optimizer: 0
Epoch 1 Batch 351 Train Loss 0.09825338423252106
Total Times. Batch: 351, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012862682342529297, Forward: 0.08179712295532227, Backward: 0.10323691368103027, Optimizer: 0
Epoch 1 Batch 352 Train Loss 0.1093648225069046
Total Times. Batch: 352, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011391639709472656, Forward: 0.0812835693359375, Backward: 0.10339689254760742, Optimizer: 0
Epoch 1 Batch 353 Train Loss 0.09722868353128433
Total Times. Batch: 353, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011997222900390625, Forward: 0.08195352554321289, Backward: 0.10320925712585449, Optimizer: 0
Epoch 1 Batch 354 Train Loss 0.09609536081552505
Total Times. Batch: 354, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011794567108154297, Forward: 0.08155584335327148, Backward: 0.10344171524047852, Optimizer: 0.16693949699401855
Epoch 1 Batch 355 Train Loss 0.09854702651500702
Total Times. Batch: 355, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013294219970703125, Forward: 0.08106279373168945, Backward: 0.09360980987548828, Optimizer: 0
Epoch 1 Batch 356 Train Loss 0.09838634729385376
Total Times. Batch: 356, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011801719665527344, Forward: 0.08142805099487305, Backward: 0.10311031341552734, Optimizer: 0
Epoch 1 Batch 357 Train Loss 0.10354991257190704
Total Times. Batch: 357, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011646747589111328, Forward: 0.08121204376220703, Backward: 0.1032710075378418, Optimizer: 0
Epoch 1 Batch 358 Train Loss 0.09117624908685684
Total Times. Batch: 358, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011584758758544922, Forward: 0.08112311363220215, Backward: 0.10314035415649414, Optimizer: 0
Epoch 1 Batch 359 Train Loss 0.09862220287322998
Total Times. Batch: 359, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001210927963256836, Forward: 0.08138728141784668, Backward: 0.10324597358703613, Optimizer: 0.1683979034423828
Epoch 1 Batch 360 Train Loss 0.09281934797763824
Total Times. Batch: 360, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012624263763427734, Forward: 0.08104443550109863, Backward: 0.09431099891662598, Optimizer: 0
Epoch 1 Batch 361 Train Loss 0.08893505483865738
Total Times. Batch: 361, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013225078582763672, Forward: 0.08167910575866699, Backward: 0.1026003360748291, Optimizer: 0
Epoch 1 Batch 362 Train Loss 0.09786902368068695
Total Times. Batch: 362, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011968612670898438, Forward: 0.08107590675354004, Backward: 0.10339093208312988, Optimizer: 0
Epoch 1 Batch 363 Train Loss 0.1037570983171463
Total Times. Batch: 363, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001172780990600586, Forward: 0.0812387466430664, Backward: 0.10344505310058594, Optimizer: 0
Epoch 1 Batch 364 Train Loss 0.09293424338102341
Total Times. Batch: 364, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011706352233886719, Forward: 0.08133506774902344, Backward: 0.1033012866973877, Optimizer: 0.16656279563903809
Epoch 1 Batch 365 Train Loss 0.09293665736913681
Total Times. Batch: 365, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001195669174194336, Forward: 0.08115124702453613, Backward: 0.09369778633117676, Optimizer: 0
Epoch 1 Batch 366 Train Loss 0.10565643757581711
Total Times. Batch: 366, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.00127410888671875, Forward: 0.08163785934448242, Backward: 0.10358977317810059, Optimizer: 0
Epoch 1 Batch 367 Train Loss 0.09454738348722458
Total Times. Batch: 367, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012552738189697266, Forward: 0.08153939247131348, Backward: 0.10337543487548828, Optimizer: 0
Epoch 1 Batch 368 Train Loss 0.08488604426383972
Total Times. Batch: 368, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011625289916992188, Forward: 0.08098983764648438, Backward: 0.10362482070922852, Optimizer: 0
Epoch 1 Batch 369 Train Loss 0.09457631409168243
Total Times. Batch: 369, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011818408966064453, Forward: 0.08132600784301758, Backward: 0.10324907302856445, Optimizer: 0.1668555736541748
Epoch 1 Batch 370 Train Loss 0.08550137281417847
Total Times. Batch: 370, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001089334487915039, Forward: 0.08109927177429199, Backward: 0.09397315979003906, Optimizer: 0
Epoch 1 Batch 371 Train Loss 0.09178240597248077
Total Times. Batch: 371, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012156963348388672, Forward: 0.08121037483215332, Backward: 0.1031043529510498, Optimizer: 0
Epoch 1 Batch 372 Train Loss 0.09727700799703598
Total Times. Batch: 372, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012116432189941406, Forward: 0.08128833770751953, Backward: 0.10342884063720703, Optimizer: 0
Epoch 1 Batch 373 Train Loss 0.09348606318235397
Total Times. Batch: 373, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012035369873046875, Forward: 0.08113718032836914, Backward: 0.10352921485900879, Optimizer: 0
Epoch 1 Batch 374 Train Loss 0.09335204213857651
Total Times. Batch: 374, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011763572692871094, Forward: 0.08127999305725098, Backward: 0.10332870483398438, Optimizer: 0.1668095588684082
Epoch 1 Batch 375 Train Loss 0.09487427771091461
Total Times. Batch: 375, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011730194091796875, Forward: 0.08127188682556152, Backward: 0.0935361385345459, Optimizer: 0
Epoch 1 Batch 376 Train Loss 0.09887295216321945
Total Times. Batch: 376, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011556148529052734, Forward: 0.08147597312927246, Backward: 0.10328102111816406, Optimizer: 0
Epoch 1 Batch 377 Train Loss 0.10237377882003784
Total Times. Batch: 377, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012116432189941406, Forward: 0.08108949661254883, Backward: 0.10358023643493652, Optimizer: 0
Epoch 1 Batch 378 Train Loss 0.09582970291376114
Total Times. Batch: 378, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001253366470336914, Forward: 0.08124804496765137, Backward: 0.10346317291259766, Optimizer: 0
Epoch 1 Batch 379 Train Loss 0.09439372271299362
Total Times. Batch: 379, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012292861938476562, Forward: 0.0812532901763916, Backward: 0.10316824913024902, Optimizer: 0.16714262962341309
Epoch 1 Batch 380 Train Loss 0.09510017186403275
Total Times. Batch: 380, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013341903686523438, Forward: 0.08101797103881836, Backward: 0.0939035415649414, Optimizer: 0
Epoch 1 Batch 381 Train Loss 0.09966278076171875
Total Times. Batch: 381, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012679100036621094, Forward: 0.0812385082244873, Backward: 0.10335445404052734, Optimizer: 0
Epoch 1 Batch 382 Train Loss 0.09348582476377487
Total Times. Batch: 382, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011444091796875, Forward: 0.08128666877746582, Backward: 0.1031956672668457, Optimizer: 0
Epoch 1 Batch 383 Train Loss 0.08996053040027618
Total Times. Batch: 383, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011990070343017578, Forward: 0.08143448829650879, Backward: 0.10339879989624023, Optimizer: 0
Epoch 1 Batch 384 Train Loss 0.09174767881631851
Total Times. Batch: 384, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012733936309814453, Forward: 0.08142399787902832, Backward: 0.10294556617736816, Optimizer: 0.16747760772705078
Epoch 1 Batch 385 Train Loss 0.09764347225427628
Total Times. Batch: 385, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011897087097167969, Forward: 0.08119916915893555, Backward: 0.09383463859558105, Optimizer: 0
Epoch 1 Batch 386 Train Loss 0.10691936314105988
Total Times. Batch: 386, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012981891632080078, Forward: 0.08140230178833008, Backward: 0.1033778190612793, Optimizer: 0
Epoch 1 Batch 387 Train Loss 0.08841164410114288
Total Times. Batch: 387, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001142263412475586, Forward: 0.08112525939941406, Backward: 0.10326838493347168, Optimizer: 0
Epoch 1 Batch 388 Train Loss 0.087653748691082
Total Times. Batch: 388, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001153707504272461, Forward: 0.08135271072387695, Backward: 0.1032717227935791, Optimizer: 0
Epoch 1 Batch 389 Train Loss 0.10141284763813019
Total Times. Batch: 389, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011959075927734375, Forward: 0.08148837089538574, Backward: 0.10330605506896973, Optimizer: 0.16742444038391113
Epoch 1 Batch 390 Train Loss 0.09205596894025803
Total Times. Batch: 390, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012326240539550781, Forward: 0.08093094825744629, Backward: 0.09420156478881836, Optimizer: 0
Epoch 1 Batch 391 Train Loss 0.09141155332326889
Total Times. Batch: 391, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001176595687866211, Forward: 0.08192586898803711, Backward: 0.10288190841674805, Optimizer: 0
Epoch 1 Batch 392 Train Loss 0.09903886169195175
Total Times. Batch: 392, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011458396911621094, Forward: 0.08250546455383301, Backward: 0.10353469848632812, Optimizer: 0
Epoch 1 Batch 393 Train Loss 0.10743000358343124
Total Times. Batch: 393, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012025833129882812, Forward: 0.08126473426818848, Backward: 0.10335493087768555, Optimizer: 0
Epoch 1 Batch 394 Train Loss 0.10139884799718857
Total Times. Batch: 394, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011737346649169922, Forward: 0.08114814758300781, Backward: 0.10333418846130371, Optimizer: 0.16674160957336426
Epoch 1 Batch 395 Train Loss 0.09162440150976181
Total Times. Batch: 395, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011775493621826172, Forward: 0.08107352256774902, Backward: 0.09403204917907715, Optimizer: 0
Epoch 1 Batch 396 Train Loss 0.09424127638339996
Total Times. Batch: 396, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011827945709228516, Forward: 0.08145856857299805, Backward: 0.10338687896728516, Optimizer: 0
Epoch 1 Batch 397 Train Loss 0.09210603684186935
Total Times. Batch: 397, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012433528900146484, Forward: 0.08168673515319824, Backward: 0.10295629501342773, Optimizer: 0
Epoch 1 Batch 398 Train Loss 0.09197526425123215
Total Times. Batch: 398, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001171112060546875, Forward: 0.08124399185180664, Backward: 0.10328030586242676, Optimizer: 0
Epoch 1 Batch 399 Train Loss 0.098517045378685
Total Times. Batch: 399, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011835098266601562, Forward: 0.08148884773254395, Backward: 0.10386419296264648, Optimizer: 0.1666109561920166
Epoch 1 Batch 400 Train Loss 0.10940505564212799
Total Times. Batch: 400, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011563301086425781, Forward: 0.08138895034790039, Backward: 0.09399843215942383, Optimizer: 0
Epoch 1 Batch 401 Train Loss 0.0998046025633812
Total Times. Batch: 401, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012936592102050781, Forward: 0.08108854293823242, Backward: 0.10340523719787598, Optimizer: 0
Epoch 1 Batch 402 Train Loss 0.0938655361533165
Total Times. Batch: 402, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001255035400390625, Forward: 0.08135724067687988, Backward: 0.10306429862976074, Optimizer: 0
Epoch 1 Batch 403 Train Loss 0.1017032042145729
Total Times. Batch: 403, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012278556823730469, Forward: 0.08139419555664062, Backward: 0.10342550277709961, Optimizer: 0
Epoch 1 Batch 404 Train Loss 0.09890200942754745
Total Times. Batch: 404, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011887550354003906, Forward: 0.08113265037536621, Backward: 0.10349202156066895, Optimizer: 0.1670668125152588
Epoch 1 Batch 405 Train Loss 0.09566672891378403
Total Times. Batch: 405, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011556148529052734, Forward: 0.08112049102783203, Backward: 0.09372925758361816, Optimizer: 0
Epoch 1 Batch 406 Train Loss 0.09167388826608658
Total Times. Batch: 406, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012607574462890625, Forward: 0.08112263679504395, Backward: 0.10330438613891602, Optimizer: 0
Epoch 1 Batch 407 Train Loss 0.09901730716228485
Total Times. Batch: 407, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012080669403076172, Forward: 0.08129596710205078, Backward: 0.10349082946777344, Optimizer: 0
Epoch 1 Batch 408 Train Loss 0.09692250937223434
Total Times. Batch: 408, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012671947479248047, Forward: 0.08124232292175293, Backward: 0.10322785377502441, Optimizer: 0
Epoch 1 Batch 409 Train Loss 0.09373738616704941
Total Times. Batch: 409, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011701583862304688, Forward: 0.0811913013458252, Backward: 0.10346794128417969, Optimizer: 0.16687488555908203
Epoch 1 Batch 410 Train Loss 0.08820389211177826
Total Times. Batch: 410, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011723041534423828, Forward: 0.0810694694519043, Backward: 0.09422063827514648, Optimizer: 0
Epoch 1 Batch 411 Train Loss 0.09373446553945541
Total Times. Batch: 411, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012886524200439453, Forward: 0.08110332489013672, Backward: 0.10329818725585938, Optimizer: 0
Epoch 1 Batch 412 Train Loss 0.09631729871034622
Total Times. Batch: 412, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012388229370117188, Forward: 0.0814206600189209, Backward: 0.10317158699035645, Optimizer: 0
Epoch 1 Batch 413 Train Loss 0.08946219831705093
Total Times. Batch: 413, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012269020080566406, Forward: 0.08111453056335449, Backward: 0.10311436653137207, Optimizer: 0
Epoch 1 Batch 414 Train Loss 0.09921755641698837
Total Times. Batch: 414, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012636184692382812, Forward: 0.08141040802001953, Backward: 0.10329031944274902, Optimizer: 0.16712474822998047
Epoch 1 Batch 415 Train Loss 0.09217441082000732
Total Times. Batch: 415, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001142740249633789, Forward: 0.08155369758605957, Backward: 0.09353327751159668, Optimizer: 0
Epoch 1 Batch 416 Train Loss 0.09575608372688293
Total Times. Batch: 416, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012748241424560547, Forward: 0.08179354667663574, Backward: 0.10362029075622559, Optimizer: 0
Epoch 1 Batch 417 Train Loss 0.10052062571048737
Total Times. Batch: 417, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011224746704101562, Forward: 0.08126473426818848, Backward: 0.1034853458404541, Optimizer: 0
Epoch 1 Batch 418 Train Loss 0.09492837637662888
Total Times. Batch: 418, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011417865753173828, Forward: 0.08133888244628906, Backward: 0.10326719284057617, Optimizer: 0
Epoch 1 Batch 419 Train Loss 0.09470715373754501
Total Times. Batch: 419, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012030601501464844, Forward: 0.08122611045837402, Backward: 0.10316729545593262, Optimizer: 0.1683201789855957
Epoch 1 Batch 420 Train Loss 0.08524695783853531
Total Times. Batch: 420, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012373924255371094, Forward: 0.08156514167785645, Backward: 0.0936284065246582, Optimizer: 0
Epoch 1 Batch 421 Train Loss 0.08712787926197052
Total Times. Batch: 421, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010914802551269531, Forward: 0.08184957504272461, Backward: 0.10311126708984375, Optimizer: 0
Epoch 1 Batch 422 Train Loss 0.09438883513212204
Total Times. Batch: 422, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012104511260986328, Forward: 0.08098316192626953, Backward: 0.10370802879333496, Optimizer: 0
Epoch 1 Batch 423 Train Loss 0.0904606506228447
Total Times. Batch: 423, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011768341064453125, Forward: 0.08116650581359863, Backward: 0.10344052314758301, Optimizer: 0
Epoch 1 Batch 424 Train Loss 0.09450318664312363
Total Times. Batch: 424, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011339187622070312, Forward: 0.08221888542175293, Backward: 0.1032419204711914, Optimizer: 0.16696453094482422
Epoch 1 Batch 425 Train Loss 0.10077472031116486
Total Times. Batch: 425, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011887550354003906, Forward: 0.08103346824645996, Backward: 0.09378576278686523, Optimizer: 0
Epoch 1 Batch 426 Train Loss 0.0868992954492569
Total Times. Batch: 426, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011749267578125, Forward: 0.08131766319274902, Backward: 0.10365676879882812, Optimizer: 0
Epoch 1 Batch 427 Train Loss 0.10539694875478745
Total Times. Batch: 427, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013043880462646484, Forward: 0.08166670799255371, Backward: 0.10323667526245117, Optimizer: 0
Epoch 1 Batch 428 Train Loss 0.09694864600896835
Total Times. Batch: 428, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011761188507080078, Forward: 0.08132457733154297, Backward: 0.10331320762634277, Optimizer: 0
Epoch 1 Batch 429 Train Loss 0.09677641093730927
Total Times. Batch: 429, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011599063873291016, Forward: 0.08143043518066406, Backward: 0.10312962532043457, Optimizer: 0.1670072078704834
Epoch 1 Batch 430 Train Loss 0.1068115383386612
Total Times. Batch: 430, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011398792266845703, Forward: 0.08101987838745117, Backward: 0.09370970726013184, Optimizer: 0
Epoch 1 Batch 431 Train Loss 0.08974804729223251
Total Times. Batch: 431, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001310586929321289, Forward: 0.08147239685058594, Backward: 0.1033625602722168, Optimizer: 0
Epoch 1 Batch 432 Train Loss 0.09269573539495468
Total Times. Batch: 432, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012938976287841797, Forward: 0.08141016960144043, Backward: 0.10333847999572754, Optimizer: 0
Epoch 1 Batch 433 Train Loss 0.09309711307287216
Total Times. Batch: 433, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012159347534179688, Forward: 0.08110904693603516, Backward: 0.10324430465698242, Optimizer: 0
Epoch 1 Batch 434 Train Loss 0.10097255557775497
Total Times. Batch: 434, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011730194091796875, Forward: 0.08102726936340332, Backward: 0.10362386703491211, Optimizer: 0.16657423973083496
Epoch 1 Batch 435 Train Loss 0.09428173303604126
Total Times. Batch: 435, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011756420135498047, Forward: 0.08123064041137695, Backward: 0.0939474105834961, Optimizer: 0
Epoch 1 Batch 436 Train Loss 0.0922035500407219
Total Times. Batch: 436, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011401176452636719, Forward: 0.0812530517578125, Backward: 0.10347199440002441, Optimizer: 0
Epoch 1 Batch 437 Train Loss 0.10060091316699982
Total Times. Batch: 437, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012230873107910156, Forward: 0.08118391036987305, Backward: 0.10319042205810547, Optimizer: 0
Epoch 1 Batch 438 Train Loss 0.09170456230640411
Total Times. Batch: 438, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012607574462890625, Forward: 0.08131122589111328, Backward: 0.10334181785583496, Optimizer: 0
Epoch 1 Batch 439 Train Loss 0.10105514526367188
Total Times. Batch: 439, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001245737075805664, Forward: 0.08126568794250488, Backward: 0.1034238338470459, Optimizer: 0.16718482971191406
Epoch 1 Batch 440 Train Loss 0.0951307937502861
Total Times. Batch: 440, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011920928955078125, Forward: 0.08104658126831055, Backward: 0.09387803077697754, Optimizer: 0
Epoch 1 Batch 441 Train Loss 0.10099755972623825
Total Times. Batch: 441, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011110305786132812, Forward: 0.08110880851745605, Backward: 0.10326290130615234, Optimizer: 0
Epoch 1 Batch 442 Train Loss 0.09593170136213303
Total Times. Batch: 442, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001155853271484375, Forward: 0.08111071586608887, Backward: 0.10305619239807129, Optimizer: 0
Epoch 1 Batch 443 Train Loss 0.09849032014608383
Total Times. Batch: 443, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012602806091308594, Forward: 0.08147692680358887, Backward: 0.1033477783203125, Optimizer: 0
Epoch 1 Batch 444 Train Loss 0.09542018175125122
Total Times. Batch: 444, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001245737075805664, Forward: 0.08135271072387695, Backward: 0.10351395606994629, Optimizer: 0.16777920722961426
Epoch 1 Batch 445 Train Loss 0.10155810415744781
Total Times. Batch: 445, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0014157295227050781, Forward: 0.08127546310424805, Backward: 0.09414815902709961, Optimizer: 0
Epoch 1 Batch 446 Train Loss 0.10259240865707397
Total Times. Batch: 446, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0015263557434082031, Forward: 0.08118700981140137, Backward: 0.10291528701782227, Optimizer: 0
Epoch 1 Batch 447 Train Loss 0.09877331554889679
Total Times. Batch: 447, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011675357818603516, Forward: 0.08145856857299805, Backward: 0.10335803031921387, Optimizer: 0
Epoch 1 Batch 448 Train Loss 0.09926364570856094
Total Times. Batch: 448, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011441707611083984, Forward: 0.08125805854797363, Backward: 0.10335469245910645, Optimizer: 0
Epoch 1 Batch 449 Train Loss 0.11001407355070114
Total Times. Batch: 449, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001188039779663086, Forward: 0.08107662200927734, Backward: 0.10315108299255371, Optimizer: 0.1668851375579834
Epoch 1 Batch 450 Train Loss 0.09513000398874283
Total Times. Batch: 450, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011973381042480469, Forward: 0.08133244514465332, Backward: 0.09367942810058594, Optimizer: 0
Epoch 1 Batch 451 Train Loss 0.09723381698131561
Total Times. Batch: 451, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012557506561279297, Forward: 0.08124041557312012, Backward: 0.10347437858581543, Optimizer: 0
Epoch 1 Batch 452 Train Loss 0.09525278210639954
Total Times. Batch: 452, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011811256408691406, Forward: 0.08122420310974121, Backward: 0.10336995124816895, Optimizer: 0
Epoch 1 Batch 453 Train Loss 0.10424216091632843
Total Times. Batch: 453, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011181831359863281, Forward: 0.08111214637756348, Backward: 0.10342001914978027, Optimizer: 0
Epoch 1 Batch 454 Train Loss 0.09596702456474304
Total Times. Batch: 454, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011470317840576172, Forward: 0.08141732215881348, Backward: 0.10336709022521973, Optimizer: 0.16690683364868164
Epoch 1 Batch 455 Train Loss 0.09129897505044937
Total Times. Batch: 455, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011935234069824219, Forward: 0.08106017112731934, Backward: 0.09385514259338379, Optimizer: 0
Epoch 1 Batch 456 Train Loss 0.08860155940055847
Total Times. Batch: 456, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013949871063232422, Forward: 0.0814058780670166, Backward: 0.10352587699890137, Optimizer: 0
Epoch 1 Batch 457 Train Loss 0.09401939064264297
Total Times. Batch: 457, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012195110321044922, Forward: 0.0815272331237793, Backward: 0.10323858261108398, Optimizer: 0
Epoch 1 Batch 458 Train Loss 0.09644008427858353
Total Times. Batch: 458, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011782646179199219, Forward: 0.08105111122131348, Backward: 0.10338044166564941, Optimizer: 0
Epoch 1 Batch 459 Train Loss 0.08825530111789703
Total Times. Batch: 459, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011589527130126953, Forward: 0.08130121231079102, Backward: 0.10325932502746582, Optimizer: 0.16660118103027344
Epoch 1 Batch 460 Train Loss 0.09394455701112747
Total Times. Batch: 460, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011370182037353516, Forward: 0.08115696907043457, Backward: 0.093841552734375, Optimizer: 0
Epoch 1 Batch 461 Train Loss 0.09556517750024796
Total Times. Batch: 461, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012514591217041016, Forward: 0.08111047744750977, Backward: 0.10326051712036133, Optimizer: 0
Epoch 1 Batch 462 Train Loss 0.09932198375463486
Total Times. Batch: 462, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012660026550292969, Forward: 0.08151745796203613, Backward: 0.10323715209960938, Optimizer: 0
Epoch 1 Batch 463 Train Loss 0.09012938290834427
Total Times. Batch: 463, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010983943939208984, Forward: 0.08190560340881348, Backward: 0.10319781303405762, Optimizer: 0
Epoch 1 Batch 464 Train Loss 0.0989631935954094
Total Times. Batch: 464, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011870861053466797, Forward: 0.08117246627807617, Backward: 0.10323905944824219, Optimizer: 0.16672873497009277
Epoch 1 Batch 465 Train Loss 0.09618353098630905
Total Times. Batch: 465, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010917186737060547, Forward: 0.08150386810302734, Backward: 0.09351110458374023, Optimizer: 0
Epoch 1 Batch 466 Train Loss 0.09046027809381485
Total Times. Batch: 466, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012683868408203125, Forward: 0.08124971389770508, Backward: 0.10361671447753906, Optimizer: 0
Epoch 1 Batch 467 Train Loss 0.09637626260519028
Total Times. Batch: 467, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012087821960449219, Forward: 0.0811161994934082, Backward: 0.10324358940124512, Optimizer: 0
Epoch 1 Batch 468 Train Loss 0.09032319486141205
Total Times. Batch: 468, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012278556823730469, Forward: 0.08130240440368652, Backward: 0.10325837135314941, Optimizer: 0
Epoch 1 Batch 469 Train Loss 0.10054860264062881
Total Times. Batch: 469, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012094974517822266, Forward: 0.08133935928344727, Backward: 0.1031486988067627, Optimizer: 0.16750645637512207
Epoch 1 Batch 470 Train Loss 0.09718199074268341
Total Times. Batch: 470, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011785030364990234, Forward: 0.08107447624206543, Backward: 0.0937037467956543, Optimizer: 0
Epoch 1 Batch 471 Train Loss 0.09907441586256027
Total Times. Batch: 471, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011086463928222656, Forward: 0.08130264282226562, Backward: 0.10330677032470703, Optimizer: 0
Epoch 1 Batch 472 Train Loss 0.08999864012002945
Total Times. Batch: 472, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011432170867919922, Forward: 0.08130431175231934, Backward: 0.10323429107666016, Optimizer: 0
Epoch 1 Batch 473 Train Loss 0.097706139087677
Total Times. Batch: 473, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012173652648925781, Forward: 0.0814826488494873, Backward: 0.10319113731384277, Optimizer: 0
Epoch 1 Batch 474 Train Loss 0.09088313579559326
Total Times. Batch: 474, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0017218589782714844, Forward: 0.08134961128234863, Backward: 0.10314106941223145, Optimizer: 0.16738200187683105
Epoch 1 Batch 475 Train Loss 0.09534354507923126
Total Times. Batch: 475, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011649131774902344, Forward: 0.08122682571411133, Backward: 0.09351491928100586, Optimizer: 0
Epoch 1 Batch 476 Train Loss 0.10154324024915695
Total Times. Batch: 476, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011742115020751953, Forward: 0.0815267562866211, Backward: 0.10339641571044922, Optimizer: 0
Epoch 1 Batch 477 Train Loss 0.09704235196113586
Total Times. Batch: 477, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011546611785888672, Forward: 0.08102059364318848, Backward: 0.1032571792602539, Optimizer: 0
Epoch 1 Batch 478 Train Loss 0.09654595702886581
Total Times. Batch: 478, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001155853271484375, Forward: 0.08137822151184082, Backward: 0.1032712459564209, Optimizer: 0
Epoch 1 Batch 479 Train Loss 0.09740393608808517
Total Times. Batch: 479, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011916160583496094, Forward: 0.08147621154785156, Backward: 0.1029970645904541, Optimizer: 0.1670362949371338
Epoch 1 Batch 480 Train Loss 0.09336094558238983
Total Times. Batch: 480, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012066364288330078, Forward: 0.08138775825500488, Backward: 0.09396243095397949, Optimizer: 0
Epoch 1 Batch 481 Train Loss 0.09907563030719757
Total Times. Batch: 481, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001337289810180664, Forward: 0.08124971389770508, Backward: 0.1031191349029541, Optimizer: 0
Epoch 1 Batch 482 Train Loss 0.0881965234875679
Total Times. Batch: 482, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011758804321289062, Forward: 0.0813591480255127, Backward: 0.10331583023071289, Optimizer: 0
Epoch 1 Batch 483 Train Loss 0.09795405715703964
Total Times. Batch: 483, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011026859283447266, Forward: 0.08136844635009766, Backward: 0.10348677635192871, Optimizer: 0
Epoch 1 Batch 484 Train Loss 0.08926883339881897
Total Times. Batch: 484, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011396408081054688, Forward: 0.08109283447265625, Backward: 0.10314083099365234, Optimizer: 0.1672840118408203
Epoch 1 Batch 485 Train Loss 0.10475490242242813
Total Times. Batch: 485, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011725425720214844, Forward: 0.08119821548461914, Backward: 0.09392619132995605, Optimizer: 0
Epoch 1 Batch 486 Train Loss 0.0957045704126358
Total Times. Batch: 486, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013751983642578125, Forward: 0.08135032653808594, Backward: 0.10330653190612793, Optimizer: 0
Epoch 1 Batch 487 Train Loss 0.09121960401535034
Total Times. Batch: 487, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001222372055053711, Forward: 0.08132648468017578, Backward: 0.10329842567443848, Optimizer: 0
Epoch 1 Batch 488 Train Loss 0.09766913205385208
Total Times. Batch: 488, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012102127075195312, Forward: 0.08148503303527832, Backward: 0.10330986976623535, Optimizer: 0
Epoch 1 Batch 489 Train Loss 0.10058871656656265
Total Times. Batch: 489, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011568069458007812, Forward: 0.08106589317321777, Backward: 0.10344624519348145, Optimizer: 0.1661994457244873
Epoch 1 Batch 490 Train Loss 0.08853694796562195
Total Times. Batch: 490, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011568069458007812, Forward: 0.08133339881896973, Backward: 0.09404945373535156, Optimizer: 0
Epoch 1 Batch 491 Train Loss 0.0863046869635582
Total Times. Batch: 491, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012116432189941406, Forward: 0.08162665367126465, Backward: 0.1034398078918457, Optimizer: 0
Epoch 1 Batch 492 Train Loss 0.09114429354667664
Total Times. Batch: 492, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012607574462890625, Forward: 0.0812077522277832, Backward: 0.10355424880981445, Optimizer: 0
Epoch 1 Batch 493 Train Loss 0.09746415168046951
Total Times. Batch: 493, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012097358703613281, Forward: 0.08141708374023438, Backward: 0.10372805595397949, Optimizer: 0
Epoch 1 Batch 494 Train Loss 0.08758379518985748
Total Times. Batch: 494, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011751651763916016, Forward: 0.08145356178283691, Backward: 0.10311031341552734, Optimizer: 0.1671595573425293
Epoch 1 Batch 495 Train Loss 0.0890016257762909
Total Times. Batch: 495, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010876655578613281, Forward: 0.08139514923095703, Backward: 0.09363865852355957, Optimizer: 0
Epoch 1 Batch 496 Train Loss 0.0965132787823677
Total Times. Batch: 496, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012691020965576172, Forward: 0.08142495155334473, Backward: 0.10295605659484863, Optimizer: 0
Epoch 1 Batch 497 Train Loss 0.10041822493076324
Total Times. Batch: 497, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001201629638671875, Forward: 0.08134222030639648, Backward: 0.10320091247558594, Optimizer: 0
Epoch 1 Batch 498 Train Loss 0.09440822899341583
Total Times. Batch: 498, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013148784637451172, Forward: 0.08132266998291016, Backward: 0.1035163402557373, Optimizer: 0
Epoch 1 Batch 499 Train Loss 0.08747812360525131
Total Times. Batch: 499, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001222372055053711, Forward: 0.08155965805053711, Backward: 0.1033334732055664, Optimizer: 0.16727089881896973
Epoch 1 Batch 500 Train Loss 0.0993550643324852
Total Times. Batch: 500, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011751651763916016, Forward: 0.0812833309173584, Backward: 0.09374165534973145, Optimizer: 0
Epoch 1 Batch 501 Train Loss 0.0832543894648552
Total Times. Batch: 501, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001285552978515625, Forward: 0.08120059967041016, Backward: 0.10354351997375488, Optimizer: 0
Epoch 1 Batch 502 Train Loss 0.08922988176345825
Total Times. Batch: 502, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001142263412475586, Forward: 0.08109855651855469, Backward: 0.1031961441040039, Optimizer: 0
Epoch 1 Batch 503 Train Loss 0.08975545316934586
Total Times. Batch: 503, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012133121490478516, Forward: 0.0818336009979248, Backward: 0.10325813293457031, Optimizer: 0
ng: 2 lr: 0.9937826120005453 dlr: 0.00012446063190843553 d_hat: 0.0018777656685304712, d: 0.0001315012578463041. sksq_weighted=3.1e-10 sk_l1=1.5e-05 gsq_weighted=1.9e-09
Epoch 1 Batch 504 Train Loss 0.0927976593375206
Total Times. Batch: 504, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012090206146240234, Forward: 0.08136487007141113, Backward: 0.10300374031066895, Optimizer: 0.16791367530822754
Epoch 1 Batch 505 Train Loss 0.08600258827209473
Total Times. Batch: 505, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013477802276611328, Forward: 0.0812983512878418, Backward: 0.09398436546325684, Optimizer: 0
Epoch 1 Batch 506 Train Loss 0.08934585750102997
Total Times. Batch: 506, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011737346649169922, Forward: 0.08135080337524414, Backward: 0.10342860221862793, Optimizer: 0
Epoch 1 Batch 507 Train Loss 0.08995460718870163
Total Times. Batch: 507, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011138916015625, Forward: 0.08129119873046875, Backward: 0.1035761833190918, Optimizer: 0
Epoch 1 Batch 508 Train Loss 0.07931242138147354
Total Times. Batch: 508, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011553764343261719, Forward: 0.08109116554260254, Backward: 0.10327410697937012, Optimizer: 0
Epoch 1 Batch 509 Train Loss 0.09364356845617294
Total Times. Batch: 509, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012085437774658203, Forward: 0.08116269111633301, Backward: 0.10356903076171875, Optimizer: 0.16773438453674316
Epoch 1 Batch 510 Train Loss 0.09134934097528458
Total Times. Batch: 510, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011472702026367188, Forward: 0.08146095275878906, Backward: 0.09349989891052246, Optimizer: 0
Epoch 1 Batch 511 Train Loss 0.08427038043737411
Total Times. Batch: 511, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013461112976074219, Forward: 0.08143281936645508, Backward: 0.10315728187561035, Optimizer: 0
Epoch 1 Batch 512 Train Loss 0.09627767652273178
Total Times. Batch: 512, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012218952178955078, Forward: 0.08105111122131348, Backward: 0.1032705307006836, Optimizer: 0
Epoch 1 Batch 513 Train Loss 0.10108361393213272
Total Times. Batch: 513, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012028217315673828, Forward: 0.08109498023986816, Backward: 0.10357284545898438, Optimizer: 0
Epoch 1 Batch 514 Train Loss 0.09049130976200104
Total Times. Batch: 514, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011553764343261719, Forward: 0.08184361457824707, Backward: 0.10317778587341309, Optimizer: 0.16697287559509277
Epoch 1 Batch 515 Train Loss 0.10197168588638306
Total Times. Batch: 515, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011849403381347656, Forward: 0.08120560646057129, Backward: 0.09396958351135254, Optimizer: 0
Epoch 1 Batch 516 Train Loss 0.09155718237161636
Total Times. Batch: 516, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013937950134277344, Forward: 0.08155941963195801, Backward: 0.10323381423950195, Optimizer: 0
Epoch 1 Batch 517 Train Loss 0.08750467747449875
Total Times. Batch: 517, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0014390945434570312, Forward: 0.08143830299377441, Backward: 0.10343813896179199, Optimizer: 0
Epoch 1 Batch 518 Train Loss 0.09670352190732956
Total Times. Batch: 518, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011930465698242188, Forward: 0.08106732368469238, Backward: 0.10328054428100586, Optimizer: 0
Epoch 1 Batch 519 Train Loss 0.08827406167984009
Total Times. Batch: 519, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011696815490722656, Forward: 0.0818328857421875, Backward: 0.10346460342407227, Optimizer: 0.16709232330322266
Epoch 1 Batch 520 Train Loss 0.096997931599617
Total Times. Batch: 520, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011408329010009766, Forward: 0.0812983512878418, Backward: 0.09398150444030762, Optimizer: 0
Epoch 1 Batch 521 Train Loss 0.08846721053123474
Total Times. Batch: 521, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012009143829345703, Forward: 0.08115124702453613, Backward: 0.10360598564147949, Optimizer: 0
Epoch 1 Batch 522 Train Loss 0.09040285646915436
Total Times. Batch: 522, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001283407211303711, Forward: 0.08161139488220215, Backward: 0.10297179222106934, Optimizer: 0
Epoch 1 Batch 523 Train Loss 0.10068807750940323
Total Times. Batch: 523, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001199960708618164, Forward: 0.08143115043640137, Backward: 0.10323858261108398, Optimizer: 0
Epoch 1 Batch 524 Train Loss 0.09633085876703262
Total Times. Batch: 524, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001238107681274414, Forward: 0.08125758171081543, Backward: 0.10340046882629395, Optimizer: 0.16701149940490723
Epoch 1 Batch 525 Train Loss 0.10424382984638214
Total Times. Batch: 525, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010917186737060547, Forward: 0.08090829849243164, Backward: 0.09415388107299805, Optimizer: 0
Epoch 1 Batch 526 Train Loss 0.09417619556188583
Total Times. Batch: 526, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011696815490722656, Forward: 0.08141922950744629, Backward: 0.10323381423950195, Optimizer: 0
Epoch 1 Batch 527 Train Loss 0.1001872792840004
Total Times. Batch: 527, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012156963348388672, Forward: 0.08153963088989258, Backward: 0.10290908813476562, Optimizer: 0
Epoch 1 Batch 528 Train Loss 0.10092321783304214
Total Times. Batch: 528, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0014934539794921875, Forward: 0.08128952980041504, Backward: 0.10347485542297363, Optimizer: 0
Epoch 1 Batch 529 Train Loss 0.10574642568826675
Total Times. Batch: 529, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012080669403076172, Forward: 0.08163309097290039, Backward: 0.10303235054016113, Optimizer: 0.1676015853881836
Epoch 1 Batch 530 Train Loss 0.09054701775312424
Total Times. Batch: 530, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011980533599853516, Forward: 0.08123373985290527, Backward: 0.09398269653320312, Optimizer: 0
Epoch 1 Batch 531 Train Loss 0.09311123937368393
Total Times. Batch: 531, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012936592102050781, Forward: 0.08100223541259766, Backward: 0.10352969169616699, Optimizer: 0
Epoch 1 Batch 532 Train Loss 0.08699514716863632
Total Times. Batch: 532, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011365413665771484, Forward: 0.08160662651062012, Backward: 0.10299158096313477, Optimizer: 0
Epoch 1 Batch 533 Train Loss 0.09838686138391495
Total Times. Batch: 533, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011959075927734375, Forward: 0.0813438892364502, Backward: 0.10350680351257324, Optimizer: 0
Epoch 1 Batch 534 Train Loss 0.10041949898004532
Total Times. Batch: 534, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011518001556396484, Forward: 0.0809783935546875, Backward: 0.10318112373352051, Optimizer: 0.16721105575561523
Epoch 1 Batch 535 Train Loss 0.09092950075864792
Total Times. Batch: 535, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011868476867675781, Forward: 0.08109068870544434, Backward: 0.09396529197692871, Optimizer: 0
Epoch 1 Batch 536 Train Loss 0.08906648308038712
Total Times. Batch: 536, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012142658233642578, Forward: 0.08139944076538086, Backward: 0.1030738353729248, Optimizer: 0
Epoch 1 Batch 537 Train Loss 0.08975207060575485
Total Times. Batch: 537, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011951923370361328, Forward: 0.08108878135681152, Backward: 0.10338664054870605, Optimizer: 0
Epoch 1 Batch 538 Train Loss 0.08246377110481262
Total Times. Batch: 538, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001146078109741211, Forward: 0.08124160766601562, Backward: 0.10339117050170898, Optimizer: 0
Epoch 1 Batch 539 Train Loss 0.09115415811538696
Total Times. Batch: 539, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011916160583496094, Forward: 0.08110475540161133, Backward: 0.10310578346252441, Optimizer: 0.16730976104736328
Epoch 1 Batch 540 Train Loss 0.09327534586191177
Total Times. Batch: 540, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013573169708251953, Forward: 0.08136248588562012, Backward: 0.09392213821411133, Optimizer: 0
Epoch 1 Batch 541 Train Loss 0.09539428353309631
Total Times. Batch: 541, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012471675872802734, Forward: 0.08159756660461426, Backward: 0.10342526435852051, Optimizer: 0
Epoch 1 Batch 542 Train Loss 0.09948491305112839
Total Times. Batch: 542, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011222362518310547, Forward: 0.08151388168334961, Backward: 0.10275769233703613, Optimizer: 0
Epoch 1 Batch 543 Train Loss 0.09215670078992844
Total Times. Batch: 543, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013685226440429688, Forward: 0.08103394508361816, Backward: 0.10330557823181152, Optimizer: 0
Epoch 1 Batch 544 Train Loss 0.09458615630865097
Total Times. Batch: 544, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011513233184814453, Forward: 0.08138346672058105, Backward: 0.10332870483398438, Optimizer: 0.16709375381469727
Epoch 1 Batch 545 Train Loss 0.08837243169546127
Total Times. Batch: 545, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001177072525024414, Forward: 0.08099746704101562, Backward: 0.09383320808410645, Optimizer: 0
Epoch 1 Batch 546 Train Loss 0.0951651781797409
Total Times. Batch: 546, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012805461883544922, Forward: 0.08159589767456055, Backward: 0.10315275192260742, Optimizer: 0
Epoch 1 Batch 547 Train Loss 0.09144359827041626
Total Times. Batch: 547, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012347698211669922, Forward: 0.0813136100769043, Backward: 0.1030576229095459, Optimizer: 0
Epoch 1 Batch 548 Train Loss 0.09010054916143417
Total Times. Batch: 548, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011844635009765625, Forward: 0.08134293556213379, Backward: 0.10319685935974121, Optimizer: 0
Epoch 1 Batch 549 Train Loss 0.09793422371149063
Total Times. Batch: 549, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011818408966064453, Forward: 0.0814509391784668, Backward: 0.10347175598144531, Optimizer: 0.1668388843536377
Epoch 1 Batch 550 Train Loss 0.09207143634557724
Total Times. Batch: 550, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013403892517089844, Forward: 0.08123922348022461, Backward: 0.09390592575073242, Optimizer: 0
Epoch 1 Batch 551 Train Loss 0.0973067507147789
Total Times. Batch: 551, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012927055358886719, Forward: 0.08099865913391113, Backward: 0.10331368446350098, Optimizer: 0
Epoch 1 Batch 552 Train Loss 0.09714587032794952
Total Times. Batch: 552, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012645721435546875, Forward: 0.0814673900604248, Backward: 0.10334515571594238, Optimizer: 0
Epoch 1 Batch 553 Train Loss 0.09219967573881149
Total Times. Batch: 553, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012214183807373047, Forward: 0.0814669132232666, Backward: 0.10351204872131348, Optimizer: 0
Epoch 1 Batch 554 Train Loss 0.09146725386381149
Total Times. Batch: 554, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012021064758300781, Forward: 0.08155035972595215, Backward: 0.10337281227111816, Optimizer: 0.16706490516662598
Epoch 1 Batch 555 Train Loss 0.09805082529783249
Total Times. Batch: 555, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011625289916992188, Forward: 0.08240342140197754, Backward: 0.09340810775756836, Optimizer: 0
Epoch 1 Batch 556 Train Loss 0.09434208273887634
Total Times. Batch: 556, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012476444244384766, Forward: 0.08184647560119629, Backward: 0.10335135459899902, Optimizer: 0
Epoch 1 Batch 557 Train Loss 0.09013020247220993
Total Times. Batch: 557, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011701583862304688, Forward: 0.08110213279724121, Backward: 0.10352730751037598, Optimizer: 0
Epoch 1 Batch 558 Train Loss 0.08624254167079926
Total Times. Batch: 558, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.00127410888671875, Forward: 0.08138155937194824, Backward: 0.10307693481445312, Optimizer: 0
Epoch 1 Batch 559 Train Loss 0.09144937992095947
Total Times. Batch: 559, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012433528900146484, Forward: 0.08125042915344238, Backward: 0.1031956672668457, Optimizer: 0.16685009002685547
Epoch 1 Batch 560 Train Loss 0.09663426131010056
Total Times. Batch: 560, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011725425720214844, Forward: 0.08140158653259277, Backward: 0.09353756904602051, Optimizer: 0
Epoch 1 Batch 561 Train Loss 0.0954108014702797
Total Times. Batch: 561, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012068748474121094, Forward: 0.08115148544311523, Backward: 0.10337233543395996, Optimizer: 0
Epoch 1 Batch 562 Train Loss 0.08574243634939194
Total Times. Batch: 562, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011363029479980469, Forward: 0.08188247680664062, Backward: 0.10318374633789062, Optimizer: 0
Epoch 1 Batch 563 Train Loss 0.09420426934957504
Total Times. Batch: 563, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001190185546875, Forward: 0.08109450340270996, Backward: 0.10334062576293945, Optimizer: 0
Epoch 1 Batch 564 Train Loss 0.09335734695196152
Total Times. Batch: 564, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012679100036621094, Forward: 0.08128476142883301, Backward: 0.10335111618041992, Optimizer: 0.1677870750427246
Epoch 1 Batch 565 Train Loss 0.08693714439868927
Total Times. Batch: 565, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011391639709472656, Forward: 0.08123350143432617, Backward: 0.09392690658569336, Optimizer: 0
Epoch 1 Batch 566 Train Loss 0.1004660502076149
Total Times. Batch: 566, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012786388397216797, Forward: 0.08125829696655273, Backward: 0.10337471961975098, Optimizer: 0
Epoch 1 Batch 567 Train Loss 0.09427478909492493
Total Times. Batch: 567, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012600421905517578, Forward: 0.08104467391967773, Backward: 0.10341954231262207, Optimizer: 0
Epoch 1 Batch 568 Train Loss 0.10297422856092453
Total Times. Batch: 568, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011479854583740234, Forward: 0.0813748836517334, Backward: 0.10315656661987305, Optimizer: 0
Epoch 1 Batch 569 Train Loss 0.09046168625354767
Total Times. Batch: 569, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001184225082397461, Forward: 0.08215951919555664, Backward: 0.1029970645904541, Optimizer: 0.16719794273376465
Epoch 1 Batch 570 Train Loss 0.09099217504262924
Total Times. Batch: 570, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012176036834716797, Forward: 0.08121657371520996, Backward: 0.09375309944152832, Optimizer: 0
Epoch 1 Batch 571 Train Loss 0.09476547688245773
Total Times. Batch: 571, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012230873107910156, Forward: 0.08149409294128418, Backward: 0.10325241088867188, Optimizer: 0
Epoch 1 Batch 572 Train Loss 0.08808925002813339
Total Times. Batch: 572, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011818408966064453, Forward: 0.08133625984191895, Backward: 0.10346579551696777, Optimizer: 0
Epoch 1 Batch 573 Train Loss 0.09045302867889404
Total Times. Batch: 573, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011746883392333984, Forward: 0.08115100860595703, Backward: 0.10296750068664551, Optimizer: 0
Epoch 1 Batch 574 Train Loss 0.09716657549142838
Total Times. Batch: 574, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011386871337890625, Forward: 0.08146524429321289, Backward: 0.10316824913024902, Optimizer: 0.16692352294921875
Epoch 1 Batch 575 Train Loss 0.08620753139257431
Total Times. Batch: 575, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011806488037109375, Forward: 0.08134198188781738, Backward: 0.0941462516784668, Optimizer: 0
Epoch 1 Batch 576 Train Loss 0.09996244311332703
Total Times. Batch: 576, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012598037719726562, Forward: 0.08126211166381836, Backward: 0.10345602035522461, Optimizer: 0
Epoch 1 Batch 577 Train Loss 0.09680330753326416
Total Times. Batch: 577, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.00116729736328125, Forward: 0.08139967918395996, Backward: 0.10320186614990234, Optimizer: 0
Epoch 1 Batch 578 Train Loss 0.08567900955677032
Total Times. Batch: 578, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011153221130371094, Forward: 0.08100509643554688, Backward: 0.10348820686340332, Optimizer: 0
Epoch 1 Batch 579 Train Loss 0.09218917787075043
Total Times. Batch: 579, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.00118255615234375, Forward: 0.08126091957092285, Backward: 0.10327005386352539, Optimizer: 0.16704130172729492
Epoch 1 Batch 580 Train Loss 0.08840884268283844
Total Times. Batch: 580, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011425018310546875, Forward: 0.08135533332824707, Backward: 0.09347009658813477, Optimizer: 0
Epoch 1 Batch 581 Train Loss 0.07876216620206833
Total Times. Batch: 581, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013325214385986328, Forward: 0.08132600784301758, Backward: 0.10333538055419922, Optimizer: 0
Epoch 1 Batch 582 Train Loss 0.08869575709104538
Total Times. Batch: 582, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012698173522949219, Forward: 0.08165669441223145, Backward: 0.1031792163848877, Optimizer: 0
Epoch 1 Batch 583 Train Loss 0.08703284710645676
Total Times. Batch: 583, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012123584747314453, Forward: 0.08140158653259277, Backward: 0.10329508781433105, Optimizer: 0
Epoch 1 Batch 584 Train Loss 0.0902022048830986
Total Times. Batch: 584, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001172780990600586, Forward: 0.0811147689819336, Backward: 0.1030435562133789, Optimizer: 0.16741466522216797
Epoch 1 Batch 585 Train Loss 0.09265831857919693
Total Times. Batch: 585, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011053085327148438, Forward: 0.08094453811645508, Backward: 0.09425163269042969, Optimizer: 0
Epoch 1 Batch 586 Train Loss 0.08800861239433289
Total Times. Batch: 586, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012693405151367188, Forward: 0.08147430419921875, Backward: 0.10305070877075195, Optimizer: 0
Epoch 1 Batch 587 Train Loss 0.09549210220575333
Total Times. Batch: 587, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012173652648925781, Forward: 0.08124518394470215, Backward: 0.10335516929626465, Optimizer: 0
Epoch 1 Batch 588 Train Loss 0.09202855080366135
Total Times. Batch: 588, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012354850769042969, Forward: 0.08099150657653809, Backward: 0.10350608825683594, Optimizer: 0
Epoch 1 Batch 589 Train Loss 0.09494149684906006
Total Times. Batch: 589, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011489391326904297, Forward: 0.08188796043395996, Backward: 0.10301518440246582, Optimizer: 0.16764354705810547
Epoch 1 Batch 590 Train Loss 0.09263043850660324
Total Times. Batch: 590, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011737346649169922, Forward: 0.08142638206481934, Backward: 0.09362912178039551, Optimizer: 0
Epoch 1 Batch 591 Train Loss 0.09637471288442612
Total Times. Batch: 591, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011796951293945312, Forward: 0.08148837089538574, Backward: 0.10330843925476074, Optimizer: 0
Epoch 1 Batch 592 Train Loss 0.08602302521467209
Total Times. Batch: 592, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011610984802246094, Forward: 0.08121037483215332, Backward: 0.10339069366455078, Optimizer: 0
Epoch 1 Batch 593 Train Loss 0.09099184721708298
Total Times. Batch: 593, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012125968933105469, Forward: 0.0816349983215332, Backward: 0.10327744483947754, Optimizer: 0
Epoch 1 Batch 594 Train Loss 0.09795504808425903
Total Times. Batch: 594, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012180805206298828, Forward: 0.08123445510864258, Backward: 0.10347485542297363, Optimizer: 0.16657495498657227
Epoch 1 Batch 595 Train Loss 0.09962303936481476
Total Times. Batch: 595, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011861324310302734, Forward: 0.08112144470214844, Backward: 0.09397077560424805, Optimizer: 0
Epoch 1 Batch 596 Train Loss 0.09098488092422485
Total Times. Batch: 596, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001316070556640625, Forward: 0.08110427856445312, Backward: 0.1034853458404541, Optimizer: 0
Epoch 1 Batch 597 Train Loss 0.08882541954517365
Total Times. Batch: 597, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011913776397705078, Forward: 0.08139896392822266, Backward: 0.10314130783081055, Optimizer: 0
Epoch 1 Batch 598 Train Loss 0.09029147028923035
Total Times. Batch: 598, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011587142944335938, Forward: 0.08137106895446777, Backward: 0.10342621803283691, Optimizer: 0
Epoch 1 Batch 599 Train Loss 0.09057186543941498
Total Times. Batch: 599, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011932849884033203, Forward: 0.08112668991088867, Backward: 0.10331010818481445, Optimizer: 0.16727781295776367
Epoch 1 Batch 600 Train Loss 0.09237944334745407
Total Times. Batch: 600, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012087821960449219, Forward: 0.08129358291625977, Backward: 0.09392356872558594, Optimizer: 0
Epoch 1 Batch 601 Train Loss 0.08057063817977905
Total Times. Batch: 601, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001314401626586914, Forward: 0.08128094673156738, Backward: 0.1034090518951416, Optimizer: 0
Epoch 1 Batch 602 Train Loss 0.08609511703252792
Total Times. Batch: 602, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012083053588867188, Forward: 0.08119463920593262, Backward: 0.10341763496398926, Optimizer: 0
Epoch 1 Batch 603 Train Loss 0.08501368761062622
Total Times. Batch: 603, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011806488037109375, Forward: 0.08120036125183105, Backward: 0.10352945327758789, Optimizer: 0
Epoch 1 Batch 604 Train Loss 0.09315399080514908
Total Times. Batch: 604, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011615753173828125, Forward: 0.08145523071289062, Backward: 0.1031649112701416, Optimizer: 0.16678738594055176
Epoch 1 Batch 605 Train Loss 0.07703524082899094
Total Times. Batch: 605, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011806488037109375, Forward: 0.08136534690856934, Backward: 0.0937337875366211, Optimizer: 0
Epoch 1 Batch 606 Train Loss 0.08305150270462036
Total Times. Batch: 606, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001363515853881836, Forward: 0.08137941360473633, Backward: 0.10351037979125977, Optimizer: 0
Epoch 1 Batch 607 Train Loss 0.08320152014493942
Total Times. Batch: 607, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012395381927490234, Forward: 0.0814206600189209, Backward: 0.10338282585144043, Optimizer: 0
Epoch 1 Batch 608 Train Loss 0.085615374147892
Total Times. Batch: 608, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011882781982421875, Forward: 0.08113241195678711, Backward: 0.10326457023620605, Optimizer: 0
Epoch 1 Batch 609 Train Loss 0.09171829372644424
Total Times. Batch: 609, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011699199676513672, Forward: 0.08132433891296387, Backward: 0.1029355525970459, Optimizer: 0.1673579216003418
Epoch 1 Batch 610 Train Loss 0.09879987686872482
Total Times. Batch: 610, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011336803436279297, Forward: 0.08131217956542969, Backward: 0.09385561943054199, Optimizer: 0
Epoch 1 Batch 611 Train Loss 0.09228938817977905
Total Times. Batch: 611, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012247562408447266, Forward: 0.08105158805847168, Backward: 0.10315299034118652, Optimizer: 0
Epoch 1 Batch 612 Train Loss 0.08964993059635162
Total Times. Batch: 612, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011763572692871094, Forward: 0.08121895790100098, Backward: 0.10343050956726074, Optimizer: 0
Epoch 1 Batch 613 Train Loss 0.08939536660909653
Total Times. Batch: 613, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012578964233398438, Forward: 0.08132696151733398, Backward: 0.10334539413452148, Optimizer: 0
Epoch 1 Batch 614 Train Loss 0.08662961423397064
Total Times. Batch: 614, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001165151596069336, Forward: 0.08135843276977539, Backward: 0.10325312614440918, Optimizer: 0.16670966148376465
Epoch 1 Batch 615 Train Loss 0.08747820556163788
Total Times. Batch: 615, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011706352233886719, Forward: 0.08103227615356445, Backward: 0.09422135353088379, Optimizer: 0
Epoch 1 Batch 616 Train Loss 0.08372481912374496
Total Times. Batch: 616, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001245737075805664, Forward: 0.08147668838500977, Backward: 0.10347223281860352, Optimizer: 0
Epoch 1 Batch 617 Train Loss 0.09433712810277939
Total Times. Batch: 617, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012199878692626953, Forward: 0.0814504623413086, Backward: 0.10294771194458008, Optimizer: 0
Epoch 1 Batch 618 Train Loss 0.09386096149682999
Total Times. Batch: 618, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012810230255126953, Forward: 0.0819234848022461, Backward: 0.10325169563293457, Optimizer: 0
Epoch 1 Batch 619 Train Loss 0.09403529018163681
Total Times. Batch: 619, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011758804321289062, Forward: 0.08142280578613281, Backward: 0.1029500961303711, Optimizer: 0.1671586036682129
Epoch 1 Batch 620 Train Loss 0.0954577624797821
Total Times. Batch: 620, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001348733901977539, Forward: 0.08089923858642578, Backward: 0.0940089225769043, Optimizer: 0
Epoch 1 Batch 621 Train Loss 0.08644592761993408
Total Times. Batch: 621, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011992454528808594, Forward: 0.0821385383605957, Backward: 0.10326766967773438, Optimizer: 0
Epoch 1 Batch 622 Train Loss 0.08742212504148483
Total Times. Batch: 622, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011436939239501953, Forward: 0.08122539520263672, Backward: 0.10325789451599121, Optimizer: 0
Epoch 1 Batch 623 Train Loss 0.08570709079504013
Total Times. Batch: 623, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011281967163085938, Forward: 0.08167266845703125, Backward: 0.1033332347869873, Optimizer: 0
Epoch 1 Batch 624 Train Loss 0.08379870653152466
Total Times. Batch: 624, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012104511260986328, Forward: 0.08107471466064453, Backward: 0.10324358940124512, Optimizer: 0.16717815399169922
Epoch 1 Batch 625 Train Loss 0.08937811106443405
Total Times. Batch: 625, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011813640594482422, Forward: 0.08131241798400879, Backward: 0.0940256118774414, Optimizer: 0
Epoch 1 Batch 626 Train Loss 0.08801537752151489
Total Times. Batch: 626, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011048316955566406, Forward: 0.08114075660705566, Backward: 0.10348916053771973, Optimizer: 0
Epoch 1 Batch 627 Train Loss 0.0788889080286026
Total Times. Batch: 627, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011510848999023438, Forward: 0.08131599426269531, Backward: 0.10311293601989746, Optimizer: 0
Epoch 1 Batch 628 Train Loss 0.08786147832870483
Total Times. Batch: 628, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011489391326904297, Forward: 0.08142685890197754, Backward: 0.10303688049316406, Optimizer: 0
Epoch 1 Batch 629 Train Loss 0.0961093008518219
Total Times. Batch: 629, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001314401626586914, Forward: 0.0811319351196289, Backward: 0.10338211059570312, Optimizer: 0.16681146621704102
Epoch 1 Batch 630 Train Loss 0.08665858954191208
Total Times. Batch: 630, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012078285217285156, Forward: 0.08118319511413574, Backward: 0.09390544891357422, Optimizer: 0
Epoch 1 Batch 631 Train Loss 0.0883076936006546
Total Times. Batch: 631, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001512765884399414, Forward: 0.08178496360778809, Backward: 0.1031196117401123, Optimizer: 0
Epoch 1 Batch 632 Train Loss 0.0852523073554039
Total Times. Batch: 632, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011966228485107422, Forward: 0.0813589096069336, Backward: 0.10300302505493164, Optimizer: 0
Epoch 1 Batch 633 Train Loss 0.09067229926586151
Total Times. Batch: 633, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001119375228881836, Forward: 0.08109688758850098, Backward: 0.10337162017822266, Optimizer: 0
Epoch 1 Batch 634 Train Loss 0.08438082784414291
Total Times. Batch: 634, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011374950408935547, Forward: 0.08128094673156738, Backward: 0.10322833061218262, Optimizer: 0.166978120803833
Epoch 1 Batch 635 Train Loss 0.08560607582330704
Total Times. Batch: 635, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012166500091552734, Forward: 0.08111572265625, Backward: 0.09420609474182129, Optimizer: 0
Epoch 1 Batch 636 Train Loss 0.08259733766317368
Total Times. Batch: 636, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013844966888427734, Forward: 0.08147907257080078, Backward: 0.10326409339904785, Optimizer: 0
Epoch 1 Batch 637 Train Loss 0.08635825663805008
Total Times. Batch: 637, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001232147216796875, Forward: 0.08153223991394043, Backward: 0.10326790809631348, Optimizer: 0
Epoch 1 Batch 638 Train Loss 0.09021944552659988
Total Times. Batch: 638, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001169443130493164, Forward: 0.08142781257629395, Backward: 0.10341525077819824, Optimizer: 0
Epoch 1 Batch 639 Train Loss 0.09313581138849258
Total Times. Batch: 639, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001323699951171875, Forward: 0.08101391792297363, Backward: 0.10361504554748535, Optimizer: 0.16700029373168945
Epoch 1 Batch 640 Train Loss 0.0821392685174942
Total Times. Batch: 640, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011432170867919922, Forward: 0.08109617233276367, Backward: 0.09391260147094727, Optimizer: 0
Epoch 1 Batch 641 Train Loss 0.08197097480297089
Total Times. Batch: 641, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001207113265991211, Forward: 0.08153581619262695, Backward: 0.10308265686035156, Optimizer: 0
Epoch 1 Batch 642 Train Loss 0.07997822016477585
Total Times. Batch: 642, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001276254653930664, Forward: 0.08137011528015137, Backward: 0.10330986976623535, Optimizer: 0
Epoch 1 Batch 643 Train Loss 0.0768221840262413
Total Times. Batch: 643, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012078285217285156, Forward: 0.0812523365020752, Backward: 0.10358619689941406, Optimizer: 0
Epoch 1 Batch 644 Train Loss 0.08197259157896042
Total Times. Batch: 644, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011589527130126953, Forward: 0.08118224143981934, Backward: 0.10334348678588867, Optimizer: 0.16729426383972168
Epoch 1 Batch 645 Train Loss 0.08754463493824005
Total Times. Batch: 645, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001081228256225586, Forward: 0.08124303817749023, Backward: 0.09384703636169434, Optimizer: 0
Epoch 1 Batch 646 Train Loss 0.08279134333133698
Total Times. Batch: 646, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012578964233398438, Forward: 0.0812070369720459, Backward: 0.10330867767333984, Optimizer: 0
Epoch 1 Batch 647 Train Loss 0.08463478088378906
Total Times. Batch: 647, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012156963348388672, Forward: 0.08175921440124512, Backward: 0.10340285301208496, Optimizer: 0
Epoch 1 Batch 648 Train Loss 0.08172736316919327
Total Times. Batch: 648, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012142658233642578, Forward: 0.0810539722442627, Backward: 0.10335350036621094, Optimizer: 0
Epoch 1 Batch 649 Train Loss 0.08255442976951599
Total Times. Batch: 649, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011110305786132812, Forward: 0.08141350746154785, Backward: 0.10307860374450684, Optimizer: 0.16681885719299316
Epoch 1 Batch 650 Train Loss 0.09170754998922348
Total Times. Batch: 650, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011584758758544922, Forward: 0.08099031448364258, Backward: 0.09433603286743164, Optimizer: 0
Epoch 1 Batch 651 Train Loss 0.08353087306022644
Total Times. Batch: 651, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012679100036621094, Forward: 0.0814969539642334, Backward: 0.1031038761138916, Optimizer: 0
Epoch 1 Batch 652 Train Loss 0.0823572427034378
Total Times. Batch: 652, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011467933654785156, Forward: 0.08135867118835449, Backward: 0.10326313972473145, Optimizer: 0
Epoch 1 Batch 653 Train Loss 0.0829484686255455
Total Times. Batch: 653, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012009143829345703, Forward: 0.08120536804199219, Backward: 0.10314178466796875, Optimizer: 0
Epoch 1 Batch 654 Train Loss 0.08640120178461075
Total Times. Batch: 654, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012836456298828125, Forward: 0.08150625228881836, Backward: 0.10325455665588379, Optimizer: 0.16674280166625977
Epoch 1 Batch 655 Train Loss 0.08183421194553375
Total Times. Batch: 655, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011892318725585938, Forward: 0.08144378662109375, Backward: 0.09361529350280762, Optimizer: 0
Epoch 1 Batch 656 Train Loss 0.08588419109582901
Total Times. Batch: 656, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012929439544677734, Forward: 0.08110356330871582, Backward: 0.10348963737487793, Optimizer: 0
Epoch 1 Batch 657 Train Loss 0.07916273176670074
Total Times. Batch: 657, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011067390441894531, Forward: 0.08109712600708008, Backward: 0.10327839851379395, Optimizer: 0
Epoch 1 Batch 658 Train Loss 0.08018169552087784
Total Times. Batch: 658, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011382102966308594, Forward: 0.08141112327575684, Backward: 0.1032717227935791, Optimizer: 0
Epoch 1 Batch 659 Train Loss 0.08515661209821701
Total Times. Batch: 659, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011870861053466797, Forward: 0.08164238929748535, Backward: 0.10324311256408691, Optimizer: 0.1664104461669922
Epoch 1 Batch 660 Train Loss 0.08109190315008163
Total Times. Batch: 660, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0014233589172363281, Forward: 0.08164119720458984, Backward: 0.09380030632019043, Optimizer: 0
Epoch 1 Batch 661 Train Loss 0.08095957338809967
Total Times. Batch: 661, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012798309326171875, Forward: 0.0812382698059082, Backward: 0.10301971435546875, Optimizer: 0
Epoch 1 Batch 662 Train Loss 0.07864657044410706
Total Times. Batch: 662, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001142740249633789, Forward: 0.08115172386169434, Backward: 0.10358953475952148, Optimizer: 0
Epoch 1 Batch 663 Train Loss 0.0842440277338028
Total Times. Batch: 663, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010967254638671875, Forward: 0.08110928535461426, Backward: 0.10349512100219727, Optimizer: 0
Epoch 1 Batch 664 Train Loss 0.07919218391180038
Total Times. Batch: 664, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011458396911621094, Forward: 0.08160638809204102, Backward: 0.10336875915527344, Optimizer: 0.16672301292419434
Epoch 1 Batch 665 Train Loss 0.083835169672966
Total Times. Batch: 665, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011739730834960938, Forward: 0.08145952224731445, Backward: 0.09352850914001465, Optimizer: 0
Epoch 1 Batch 666 Train Loss 0.08164071291685104
Total Times. Batch: 666, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012187957763671875, Forward: 0.08129048347473145, Backward: 0.10326242446899414, Optimizer: 0
Epoch 1 Batch 667 Train Loss 0.08233578503131866
Total Times. Batch: 667, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012547969818115234, Forward: 0.08143329620361328, Backward: 0.10365700721740723, Optimizer: 0
Epoch 1 Batch 668 Train Loss 0.07988691329956055
Total Times. Batch: 668, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011112689971923828, Forward: 0.08149003982543945, Backward: 0.1031947135925293, Optimizer: 0
Epoch 1 Batch 669 Train Loss 0.07420303672552109
Total Times. Batch: 669, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011637210845947266, Forward: 0.08152651786804199, Backward: 0.10349702835083008, Optimizer: 0.16683602333068848
Epoch 1 Batch 670 Train Loss 0.07713600248098373
Total Times. Batch: 670, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011327266693115234, Forward: 0.0813453197479248, Backward: 0.09374475479125977, Optimizer: 0
Epoch 1 Batch 671 Train Loss 0.07808536291122437
Total Times. Batch: 671, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011897087097167969, Forward: 0.08199143409729004, Backward: 0.10318470001220703, Optimizer: 0
Epoch 1 Batch 672 Train Loss 0.07877527177333832
Total Times. Batch: 672, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012059211730957031, Forward: 0.0817413330078125, Backward: 0.10332512855529785, Optimizer: 0
Epoch 1 Batch 673 Train Loss 0.08236885070800781
Total Times. Batch: 673, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011875629425048828, Forward: 0.08201360702514648, Backward: 0.10339164733886719, Optimizer: 0
Epoch 1 Batch 674 Train Loss 0.08157932013273239
Total Times. Batch: 674, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011568069458007812, Forward: 0.08105349540710449, Backward: 0.10357880592346191, Optimizer: 0.16726303100585938
Epoch 1 Batch 675 Train Loss 0.08059677481651306
Total Times. Batch: 675, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011653900146484375, Forward: 0.08135700225830078, Backward: 0.09378576278686523, Optimizer: 0
Epoch 1 Batch 676 Train Loss 0.07609197497367859
Total Times. Batch: 676, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011472702026367188, Forward: 0.08155202865600586, Backward: 0.10327029228210449, Optimizer: 0
Epoch 1 Batch 677 Train Loss 0.08207961916923523
Total Times. Batch: 677, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012009143829345703, Forward: 0.08101654052734375, Backward: 0.10331964492797852, Optimizer: 0
Epoch 1 Batch 678 Train Loss 0.08212441205978394
Total Times. Batch: 678, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012443065643310547, Forward: 0.08141160011291504, Backward: 0.10326170921325684, Optimizer: 0
Epoch 1 Batch 679 Train Loss 0.08110916614532471
Total Times. Batch: 679, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012068748474121094, Forward: 0.08143830299377441, Backward: 0.1032097339630127, Optimizer: 0.16676974296569824
Epoch 1 Batch 680 Train Loss 0.07451900839805603
Total Times. Batch: 680, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013644695281982422, Forward: 0.08089780807495117, Backward: 0.09390616416931152, Optimizer: 0
Epoch 1 Batch 681 Train Loss 0.07824679464101791
Total Times. Batch: 681, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012879371643066406, Forward: 0.08140778541564941, Backward: 0.1034402847290039, Optimizer: 0
Epoch 1 Batch 682 Train Loss 0.08273180574178696
Total Times. Batch: 682, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.00115203857421875, Forward: 0.0815887451171875, Backward: 0.10268831253051758, Optimizer: 0
Epoch 1 Batch 683 Train Loss 0.08460034430027008
Total Times. Batch: 683, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011954307556152344, Forward: 0.08135247230529785, Backward: 0.10347533226013184, Optimizer: 0
Epoch 1 Batch 684 Train Loss 0.07822267711162567
Total Times. Batch: 684, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011875629425048828, Forward: 0.08127069473266602, Backward: 0.10334658622741699, Optimizer: 0.16716861724853516
Epoch 1 Batch 685 Train Loss 0.08168240636587143
Total Times. Batch: 685, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012004375457763672, Forward: 0.08129453659057617, Backward: 0.09416532516479492, Optimizer: 0
Epoch 1 Batch 686 Train Loss 0.08083796501159668
Total Times. Batch: 686, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012416839599609375, Forward: 0.08137774467468262, Backward: 0.10324859619140625, Optimizer: 0
Epoch 1 Batch 687 Train Loss 0.07719150185585022
Total Times. Batch: 687, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011756420135498047, Forward: 0.08130764961242676, Backward: 0.10343146324157715, Optimizer: 0
Epoch 1 Batch 688 Train Loss 0.07695069164037704
Total Times. Batch: 688, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011632442474365234, Forward: 0.08182597160339355, Backward: 0.10335683822631836, Optimizer: 0
Epoch 1 Batch 689 Train Loss 0.07374745607376099
Total Times. Batch: 689, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011966228485107422, Forward: 0.08121705055236816, Backward: 0.10357546806335449, Optimizer: 0.16718268394470215
Epoch 1 Batch 690 Train Loss 0.07163864374160767
Total Times. Batch: 690, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001149892807006836, Forward: 0.08145380020141602, Backward: 0.09376931190490723, Optimizer: 0
Epoch 1 Batch 691 Train Loss 0.08173531293869019
Total Times. Batch: 691, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012590885162353516, Forward: 0.08155465126037598, Backward: 0.10334563255310059, Optimizer: 0
Epoch 1 Batch 692 Train Loss 0.0759575292468071
Total Times. Batch: 692, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011911392211914062, Forward: 0.08152270317077637, Backward: 0.10322213172912598, Optimizer: 0
Epoch 1 Batch 693 Train Loss 0.07243849337100983
Total Times. Batch: 693, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011227130889892578, Forward: 0.08133649826049805, Backward: 0.10340714454650879, Optimizer: 0
Epoch 1 Batch 694 Train Loss 0.07136352360248566
Total Times. Batch: 694, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011553764343261719, Forward: 0.08129739761352539, Backward: 0.10320544242858887, Optimizer: 0.1668238639831543
Epoch 1 Batch 695 Train Loss 0.07471403479576111
Total Times. Batch: 695, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011610984802246094, Forward: 0.08091259002685547, Backward: 0.09412074089050293, Optimizer: 0
Epoch 1 Batch 696 Train Loss 0.07499776035547256
Total Times. Batch: 696, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012865066528320312, Forward: 0.08130002021789551, Backward: 0.1033623218536377, Optimizer: 0
Epoch 1 Batch 697 Train Loss 0.07028990983963013
Total Times. Batch: 697, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001260995864868164, Forward: 0.08129692077636719, Backward: 0.10335421562194824, Optimizer: 0
Epoch 1 Batch 698 Train Loss 0.08056797832250595
Total Times. Batch: 698, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011072158813476562, Forward: 0.08112883567810059, Backward: 0.10350704193115234, Optimizer: 0
Epoch 1 Batch 699 Train Loss 0.06868117302656174
Total Times. Batch: 699, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011603832244873047, Forward: 0.08134937286376953, Backward: 0.1035165786743164, Optimizer: 0.16663908958435059
Epoch 1 Batch 700 Train Loss 0.07014960050582886
Total Times. Batch: 700, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011539459228515625, Forward: 0.08131933212280273, Backward: 0.0940713882446289, Optimizer: 0
Epoch 1 Batch 701 Train Loss 0.0667237639427185
Total Times. Batch: 701, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013015270233154297, Forward: 0.08114409446716309, Backward: 0.10358405113220215, Optimizer: 0
Epoch 1 Batch 702 Train Loss 0.07422471791505814
Total Times. Batch: 702, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012781620025634766, Forward: 0.08164834976196289, Backward: 0.10313940048217773, Optimizer: 0
Epoch 1 Batch 703 Train Loss 0.07295644283294678
Total Times. Batch: 703, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012352466583251953, Forward: 0.08185768127441406, Backward: 0.10351300239562988, Optimizer: 0
Epoch 1 Batch 704 Train Loss 0.07787256687879562
Total Times. Batch: 704, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012052059173583984, Forward: 0.0812063217163086, Backward: 0.1036839485168457, Optimizer: 0.16707468032836914
Epoch 1 Batch 705 Train Loss 0.07181397825479507
Total Times. Batch: 705, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011692047119140625, Forward: 0.08097195625305176, Backward: 0.0941307544708252, Optimizer: 0
Epoch 1 Batch 706 Train Loss 0.07450844347476959
Total Times. Batch: 706, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012636184692382812, Forward: 0.08148813247680664, Backward: 0.1032555103302002, Optimizer: 0
Epoch 1 Batch 707 Train Loss 0.06795523315668106
Total Times. Batch: 707, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012028217315673828, Forward: 0.08139967918395996, Backward: 0.10321187973022461, Optimizer: 0
Epoch 1 Batch 708 Train Loss 0.07790176570415497
Total Times. Batch: 708, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012826919555664062, Forward: 0.08170199394226074, Backward: 0.10294222831726074, Optimizer: 0
Epoch 1 Batch 709 Train Loss 0.06779976934194565
Total Times. Batch: 709, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012242794036865234, Forward: 0.08120012283325195, Backward: 0.10341739654541016, Optimizer: 0.16710281372070312
Epoch 1 Batch 710 Train Loss 0.07071118801832199
Total Times. Batch: 710, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011429786682128906, Forward: 0.08131861686706543, Backward: 0.09366178512573242, Optimizer: 0
Epoch 1 Batch 711 Train Loss 0.07272328436374664
Total Times. Batch: 711, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011749267578125, Forward: 0.08141446113586426, Backward: 0.10368680953979492, Optimizer: 0
Epoch 1 Batch 712 Train Loss 0.07145699113607407
Total Times. Batch: 712, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011546611785888672, Forward: 0.08111906051635742, Backward: 0.10348153114318848, Optimizer: 0
Epoch 1 Batch 713 Train Loss 0.06720706075429916
Total Times. Batch: 713, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011851787567138672, Forward: 0.08138394355773926, Backward: 0.10361361503601074, Optimizer: 0
Epoch 1 Batch 714 Train Loss 0.07426625490188599
Total Times. Batch: 714, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012774467468261719, Forward: 0.08156633377075195, Backward: 0.10340094566345215, Optimizer: 0.1667194366455078
Epoch 1 Batch 715 Train Loss 0.06962916254997253
Total Times. Batch: 715, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011234283447265625, Forward: 0.08133053779602051, Backward: 0.09427356719970703, Optimizer: 0
Epoch 1 Batch 716 Train Loss 0.06945838779211044
Total Times. Batch: 716, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001302480697631836, Forward: 0.08122682571411133, Backward: 0.10341978073120117, Optimizer: 0
Epoch 1 Batch 717 Train Loss 0.07169965654611588
Total Times. Batch: 717, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011644363403320312, Forward: 0.08144736289978027, Backward: 0.10336971282958984, Optimizer: 0
Epoch 1 Batch 718 Train Loss 0.07099708169698715
Total Times. Batch: 718, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011410713195800781, Forward: 0.08153700828552246, Backward: 0.10313749313354492, Optimizer: 0
Epoch 1 Batch 719 Train Loss 0.07154498249292374
Total Times. Batch: 719, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011982917785644531, Forward: 0.08109545707702637, Backward: 0.10348939895629883, Optimizer: 0.1671450138092041
Epoch 1 Batch 720 Train Loss 0.07089420408010483
Total Times. Batch: 720, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012242794036865234, Forward: 0.08133196830749512, Backward: 0.09382033348083496, Optimizer: 0
Epoch 1 Batch 721 Train Loss 0.07326026260852814
Total Times. Batch: 721, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001218557357788086, Forward: 0.08135533332824707, Backward: 0.10344362258911133, Optimizer: 0
Epoch 1 Batch 722 Train Loss 0.07213334739208221
Total Times. Batch: 722, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001302480697631836, Forward: 0.08150434494018555, Backward: 0.10289907455444336, Optimizer: 0
Epoch 1 Batch 723 Train Loss 0.06213949993252754
Total Times. Batch: 723, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001176595687866211, Forward: 0.08127093315124512, Backward: 0.10360860824584961, Optimizer: 0
Epoch 1 Batch 724 Train Loss 0.0687529519200325
Total Times. Batch: 724, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011515617370605469, Forward: 0.08127903938293457, Backward: 0.10327935218811035, Optimizer: 0.16669011116027832
Epoch 1 Batch 725 Train Loss 0.06780803948640823
Total Times. Batch: 725, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013842582702636719, Forward: 0.08182764053344727, Backward: 0.09398078918457031, Optimizer: 0
Epoch 1 Batch 726 Train Loss 0.07269541174173355
Total Times. Batch: 726, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001390218734741211, Forward: 0.08120346069335938, Backward: 0.10338258743286133, Optimizer: 0
Epoch 1 Batch 727 Train Loss 0.06854330748319626
Total Times. Batch: 727, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011546611785888672, Forward: 0.08142542839050293, Backward: 0.10275864601135254, Optimizer: 0
Epoch 1 Batch 728 Train Loss 0.06456108391284943
Total Times. Batch: 728, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011687278747558594, Forward: 0.0814058780670166, Backward: 0.10351777076721191, Optimizer: 0
Epoch 1 Batch 729 Train Loss 0.06660827249288559
Total Times. Batch: 729, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011761188507080078, Forward: 0.08127856254577637, Backward: 0.10343265533447266, Optimizer: 0.16797757148742676
Epoch 1 Batch 730 Train Loss 0.06368040293455124
Total Times. Batch: 730, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011429786682128906, Forward: 0.08115935325622559, Backward: 0.09420323371887207, Optimizer: 0
Epoch 1 Batch 731 Train Loss 0.06640741229057312
Total Times. Batch: 731, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013039112091064453, Forward: 0.08107948303222656, Backward: 0.10353803634643555, Optimizer: 0
Epoch 1 Batch 732 Train Loss 0.07139088958501816
Total Times. Batch: 732, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012781620025634766, Forward: 0.08240246772766113, Backward: 0.10329365730285645, Optimizer: 0
Epoch 1 Batch 733 Train Loss 0.06692128628492355
Total Times. Batch: 733, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011937618255615234, Forward: 0.08113408088684082, Backward: 0.10332393646240234, Optimizer: 0
Epoch 1 Batch 734 Train Loss 0.06736286729574203
Total Times. Batch: 734, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011730194091796875, Forward: 0.08113551139831543, Backward: 0.10354495048522949, Optimizer: 0.16691303253173828
Epoch 1 Batch 735 Train Loss 0.06654130667448044
Total Times. Batch: 735, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011501312255859375, Forward: 0.08129191398620605, Backward: 0.09394168853759766, Optimizer: 0
Epoch 1 Batch 736 Train Loss 0.0666586384177208
Total Times. Batch: 736, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012485980987548828, Forward: 0.08119893074035645, Backward: 0.10332083702087402, Optimizer: 0
Epoch 1 Batch 737 Train Loss 0.059392184019088745
Total Times. Batch: 737, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011963844299316406, Forward: 0.08133864402770996, Backward: 0.10365843772888184, Optimizer: 0
Epoch 1 Batch 738 Train Loss 0.06364281475543976
Total Times. Batch: 738, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012664794921875, Forward: 0.0816354751586914, Backward: 0.10339474678039551, Optimizer: 0
Epoch 1 Batch 739 Train Loss 0.06613141298294067
Total Times. Batch: 739, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012364387512207031, Forward: 0.08113527297973633, Backward: 0.1036534309387207, Optimizer: 0.16770529747009277
Epoch 1 Batch 740 Train Loss 0.06517238169908524
Total Times. Batch: 740, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011775493621826172, Forward: 0.08115363121032715, Backward: 0.09365653991699219, Optimizer: 0
Epoch 1 Batch 741 Train Loss 0.06368666142225266
Total Times. Batch: 741, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001173257827758789, Forward: 0.08203315734863281, Backward: 0.10368680953979492, Optimizer: 0
Epoch 1 Batch 742 Train Loss 0.07385432720184326
Total Times. Batch: 742, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010967254638671875, Forward: 0.08138370513916016, Backward: 0.10346341133117676, Optimizer: 0
Epoch 1 Batch 743 Train Loss 0.06310581415891647
Total Times. Batch: 743, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011096000671386719, Forward: 0.08149862289428711, Backward: 0.10353422164916992, Optimizer: 0
Epoch 1 Batch 744 Train Loss 0.058994878083467484
Total Times. Batch: 744, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001287221908569336, Forward: 0.08150482177734375, Backward: 0.10352635383605957, Optimizer: 0.1673574447631836
Epoch 1 Batch 745 Train Loss 0.06676342338323593
Total Times. Batch: 745, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011930465698242188, Forward: 0.0814826488494873, Backward: 0.09368300437927246, Optimizer: 0
Epoch 1 Batch 746 Train Loss 0.055069662630558014
Total Times. Batch: 746, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011882781982421875, Forward: 0.08117461204528809, Backward: 0.10345649719238281, Optimizer: 0
Epoch 1 Batch 747 Train Loss 0.06715698540210724
Total Times. Batch: 747, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011060237884521484, Forward: 0.08113670349121094, Backward: 0.10321211814880371, Optimizer: 0
Epoch 1 Batch 748 Train Loss 0.06203192472457886
Total Times. Batch: 748, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001161336898803711, Forward: 0.08152532577514648, Backward: 0.10335588455200195, Optimizer: 0
Epoch 1 Batch 749 Train Loss 0.058660831302404404
Total Times. Batch: 749, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0014164447784423828, Forward: 0.08109283447265625, Backward: 0.10332608222961426, Optimizer: 0.16710519790649414
Epoch 1 Batch 750 Train Loss 0.060296881943941116
Total Times. Batch: 750, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001220703125, Forward: 0.08121347427368164, Backward: 0.09379744529724121, Optimizer: 0
Epoch 1 Batch 751 Train Loss 0.05775889754295349
Total Times. Batch: 751, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001325368881225586, Forward: 0.08133769035339355, Backward: 0.10353398323059082, Optimizer: 0
Epoch 1 Batch 752 Train Loss 0.06212707981467247
Total Times. Batch: 752, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011706352233886719, Forward: 0.08182001113891602, Backward: 0.10276961326599121, Optimizer: 0
Epoch 1 Batch 753 Train Loss 0.06206120178103447
Total Times. Batch: 753, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001293182373046875, Forward: 0.08136391639709473, Backward: 0.10334992408752441, Optimizer: 0
Epoch 1 Batch 754 Train Loss 0.05933285504579544
Total Times. Batch: 754, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011501312255859375, Forward: 0.08126235008239746, Backward: 0.10342717170715332, Optimizer: 0.16677188873291016
Epoch 1 Batch 755 Train Loss 0.06131032854318619
Total Times. Batch: 755, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011882781982421875, Forward: 0.08126473426818848, Backward: 0.09383726119995117, Optimizer: 0
Epoch 1 Batch 756 Train Loss 0.059107471257448196
Total Times. Batch: 756, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012862682342529297, Forward: 0.08163762092590332, Backward: 0.10316324234008789, Optimizer: 0
Epoch 1 Batch 757 Train Loss 0.05839128419756889
Total Times. Batch: 757, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012044906616210938, Forward: 0.08222317695617676, Backward: 0.10324239730834961, Optimizer: 0
Epoch 1 Batch 758 Train Loss 0.05349569395184517
Total Times. Batch: 758, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011887550354003906, Forward: 0.08114099502563477, Backward: 0.10338354110717773, Optimizer: 0
Epoch 1 Batch 759 Train Loss 0.05918803811073303
Total Times. Batch: 759, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011861324310302734, Forward: 0.08137893676757812, Backward: 0.10311579704284668, Optimizer: 0.1675872802734375
Epoch 1 Batch 760 Train Loss 0.0651959776878357
Total Times. Batch: 760, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011713504791259766, Forward: 0.08136534690856934, Backward: 0.09413719177246094, Optimizer: 0
Epoch 1 Batch 761 Train Loss 0.05756385251879692
Total Times. Batch: 761, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0015094280242919922, Forward: 0.08157920837402344, Backward: 0.10319328308105469, Optimizer: 0
Epoch 1 Batch 762 Train Loss 0.058759331703186035
Total Times. Batch: 762, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012218952178955078, Forward: 0.08214783668518066, Backward: 0.10291504859924316, Optimizer: 0
Epoch 1 Batch 763 Train Loss 0.061517130583524704
Total Times. Batch: 763, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011622905731201172, Forward: 0.08118939399719238, Backward: 0.10347557067871094, Optimizer: 0
Epoch 1 Batch 764 Train Loss 0.06387850642204285
Total Times. Batch: 764, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011489391326904297, Forward: 0.08137845993041992, Backward: 0.10370087623596191, Optimizer: 0.16715764999389648
Epoch 1 Batch 765 Train Loss 0.0565679669380188
Total Times. Batch: 765, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011136531829833984, Forward: 0.08090639114379883, Backward: 0.09399914741516113, Optimizer: 0
Epoch 1 Batch 766 Train Loss 0.05477994307875633
Total Times. Batch: 766, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012712478637695312, Forward: 0.0814363956451416, Backward: 0.1033475399017334, Optimizer: 0
Epoch 1 Batch 767 Train Loss 0.05900741368532181
Total Times. Batch: 767, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011556148529052734, Forward: 0.08159971237182617, Backward: 0.10337281227111816, Optimizer: 0
Epoch 1 Batch 768 Train Loss 0.05978189781308174
Total Times. Batch: 768, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001199960708618164, Forward: 0.08109521865844727, Backward: 0.10299992561340332, Optimizer: 0
Epoch 1 Batch 769 Train Loss 0.05672459676861763
Total Times. Batch: 769, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012183189392089844, Forward: 0.08157181739807129, Backward: 0.10347604751586914, Optimizer: 0.16726326942443848
Epoch 1 Batch 770 Train Loss 0.055891115218400955
Total Times. Batch: 770, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011708736419677734, Forward: 0.08108186721801758, Backward: 0.09431910514831543, Optimizer: 0
Epoch 1 Batch 771 Train Loss 0.05262833833694458
Total Times. Batch: 771, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011868476867675781, Forward: 0.08098602294921875, Backward: 0.10337162017822266, Optimizer: 0
Epoch 1 Batch 772 Train Loss 0.05486869812011719
Total Times. Batch: 772, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011401176452636719, Forward: 0.08157157897949219, Backward: 0.10314512252807617, Optimizer: 0
Epoch 1 Batch 773 Train Loss 0.05430702492594719
Total Times. Batch: 773, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001216888427734375, Forward: 0.08147764205932617, Backward: 0.1034555435180664, Optimizer: 0
Epoch 1 Batch 774 Train Loss 0.05589466169476509
Total Times. Batch: 774, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012881755828857422, Forward: 0.08146953582763672, Backward: 0.10312604904174805, Optimizer: 0.16758990287780762
Epoch 1 Batch 775 Train Loss 0.05466017872095108
Total Times. Batch: 775, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011038780212402344, Forward: 0.08124065399169922, Backward: 0.09381628036499023, Optimizer: 0
Epoch 1 Batch 776 Train Loss 0.053923893719911575
Total Times. Batch: 776, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011723041534423828, Forward: 0.08160209655761719, Backward: 0.1029810905456543, Optimizer: 0
Epoch 1 Batch 777 Train Loss 0.0618140809237957
Total Times. Batch: 777, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011818408966064453, Forward: 0.08138322830200195, Backward: 0.10343742370605469, Optimizer: 0
Epoch 1 Batch 778 Train Loss 0.051588065922260284
Total Times. Batch: 778, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011725425720214844, Forward: 0.08128833770751953, Backward: 0.10348367691040039, Optimizer: 0
Epoch 1 Batch 779 Train Loss 0.05569883808493614
Total Times. Batch: 779, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012001991271972656, Forward: 0.0811011791229248, Backward: 0.10333943367004395, Optimizer: 0.1678025722503662
Epoch 1 Batch 780 Train Loss 0.05343148857355118
Total Times. Batch: 780, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011415481567382812, Forward: 0.08115124702453613, Backward: 0.0935971736907959, Optimizer: 0
Epoch 1 Batch 781 Train Loss 0.05716002732515335
Total Times. Batch: 781, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012698173522949219, Forward: 0.08114027976989746, Backward: 0.10322904586791992, Optimizer: 0
Epoch 1 Batch 782 Train Loss 0.05315237119793892
Total Times. Batch: 782, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012347698211669922, Forward: 0.08149003982543945, Backward: 0.10321235656738281, Optimizer: 0
Epoch 1 Batch 783 Train Loss 0.05245131254196167
Total Times. Batch: 783, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012052059173583984, Forward: 0.08124828338623047, Backward: 0.10379791259765625, Optimizer: 0
Epoch 1 Batch 784 Train Loss 0.05744190141558647
Total Times. Batch: 784, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012214183807373047, Forward: 0.08108806610107422, Backward: 0.10355210304260254, Optimizer: 0.16713237762451172
Epoch 1 Batch 785 Train Loss 0.05741901323199272
Total Times. Batch: 785, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011832714080810547, Forward: 0.08179688453674316, Backward: 0.09363389015197754, Optimizer: 0
Epoch 1 Batch 786 Train Loss 0.05145230516791344
Total Times. Batch: 786, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001333475112915039, Forward: 0.08168411254882812, Backward: 0.10341382026672363, Optimizer: 0
Epoch 1 Batch 787 Train Loss 0.0585775263607502
Total Times. Batch: 787, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012271404266357422, Forward: 0.08145785331726074, Backward: 0.10340452194213867, Optimizer: 0
Epoch 1 Batch 788 Train Loss 0.05737800523638725
Total Times. Batch: 788, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011715888977050781, Forward: 0.08109116554260254, Backward: 0.10347390174865723, Optimizer: 0
Epoch 1 Batch 789 Train Loss 0.05524907261133194
Total Times. Batch: 789, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011832714080810547, Forward: 0.08120965957641602, Backward: 0.10325121879577637, Optimizer: 0.16685247421264648
Epoch 1 Batch 790 Train Loss 0.05018866807222366
Total Times. Batch: 790, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011515617370605469, Forward: 0.0813455581665039, Backward: 0.09363245964050293, Optimizer: 0
Epoch 1 Batch 791 Train Loss 0.052895184606313705
Total Times. Batch: 791, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001207113265991211, Forward: 0.0811467170715332, Backward: 0.10354042053222656, Optimizer: 0
Epoch 1 Batch 792 Train Loss 0.05192851647734642
Total Times. Batch: 792, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012583732604980469, Forward: 0.08162426948547363, Backward: 0.10311222076416016, Optimizer: 0
Epoch 1 Batch 793 Train Loss 0.05367445945739746
Total Times. Batch: 793, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012333393096923828, Forward: 0.08146500587463379, Backward: 0.10344839096069336, Optimizer: 0
Epoch 1 Batch 794 Train Loss 0.056097764521837234
Total Times. Batch: 794, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011723041534423828, Forward: 0.08140397071838379, Backward: 0.10320043563842773, Optimizer: 0.16744613647460938
Epoch 1 Batch 795 Train Loss 0.05368395522236824
Total Times. Batch: 795, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011756420135498047, Forward: 0.08114838600158691, Backward: 0.09367656707763672, Optimizer: 0
Epoch 1 Batch 796 Train Loss 0.04968732222914696
Total Times. Batch: 796, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012738704681396484, Forward: 0.08105325698852539, Backward: 0.10345196723937988, Optimizer: 0
Epoch 1 Batch 797 Train Loss 0.048387061804533005
Total Times. Batch: 797, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011799335479736328, Forward: 0.08126544952392578, Backward: 0.10324430465698242, Optimizer: 0
Epoch 1 Batch 798 Train Loss 0.052835799753665924
Total Times. Batch: 798, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001172780990600586, Forward: 0.08156490325927734, Backward: 0.10312747955322266, Optimizer: 0
Epoch 1 Batch 799 Train Loss 0.052107978612184525
Total Times. Batch: 799, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012018680572509766, Forward: 0.08115887641906738, Backward: 0.10332274436950684, Optimizer: 0.1672039031982422
Epoch 1 Batch 800 Train Loss 0.049845438450574875
Total Times. Batch: 800, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011377334594726562, Forward: 0.08149480819702148, Backward: 0.09383559226989746, Optimizer: 0
Epoch 1 Batch 801 Train Loss 0.0527818500995636
Total Times. Batch: 801, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013306140899658203, Forward: 0.08113574981689453, Backward: 0.10350751876831055, Optimizer: 0
Epoch 1 Batch 802 Train Loss 0.06058058142662048
Total Times. Batch: 802, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011436939239501953, Forward: 0.08125090599060059, Backward: 0.10332608222961426, Optimizer: 0
Epoch 1 Batch 803 Train Loss 0.04860934242606163
Total Times. Batch: 803, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011029243469238281, Forward: 0.08128523826599121, Backward: 0.10303473472595215, Optimizer: 0
Epoch 1 Batch 804 Train Loss 0.05509655550122261
Total Times. Batch: 804, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012202262878417969, Forward: 0.08158040046691895, Backward: 0.10297346115112305, Optimizer: 0.16828107833862305
Epoch 1 Batch 805 Train Loss 0.0494314469397068
Total Times. Batch: 805, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011870861053466797, Forward: 0.08113956451416016, Backward: 0.09376668930053711, Optimizer: 0
Epoch 1 Batch 806 Train Loss 0.05012885481119156
Total Times. Batch: 806, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011017322540283203, Forward: 0.08118367195129395, Backward: 0.10342931747436523, Optimizer: 0
Epoch 1 Batch 807 Train Loss 0.055737532675266266
Total Times. Batch: 807, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011730194091796875, Forward: 0.08156442642211914, Backward: 0.10353374481201172, Optimizer: 0
Epoch 1 Batch 808 Train Loss 0.052615560591220856
Total Times. Batch: 808, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011453628540039062, Forward: 0.08132195472717285, Backward: 0.10311007499694824, Optimizer: 0
Epoch 1 Batch 809 Train Loss 0.0548233762383461
Total Times. Batch: 809, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011203289031982422, Forward: 0.08133172988891602, Backward: 0.1033933162689209, Optimizer: 0.16833090782165527
Epoch 1 Batch 810 Train Loss 0.05376194790005684
Total Times. Batch: 810, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012214183807373047, Forward: 0.08104968070983887, Backward: 0.09397268295288086, Optimizer: 0
Epoch 1 Batch 811 Train Loss 0.048802152276039124
Total Times. Batch: 811, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011515617370605469, Forward: 0.08144688606262207, Backward: 0.10336756706237793, Optimizer: 0
Epoch 1 Batch 812 Train Loss 0.053679026663303375
Total Times. Batch: 812, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011577606201171875, Forward: 0.08120536804199219, Backward: 0.10331416130065918, Optimizer: 0
Epoch 1 Batch 813 Train Loss 0.0539495013654232
Total Times. Batch: 813, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011124610900878906, Forward: 0.08095312118530273, Backward: 0.10343599319458008, Optimizer: 0
Epoch 1 Batch 814 Train Loss 0.04955872520804405
Total Times. Batch: 814, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011348724365234375, Forward: 0.0818169116973877, Backward: 0.10351061820983887, Optimizer: 0.16666507720947266
Epoch 1 Batch 815 Train Loss 0.044011056423187256
Total Times. Batch: 815, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0014150142669677734, Forward: 0.08169746398925781, Backward: 0.09382867813110352, Optimizer: 0
Epoch 1 Batch 816 Train Loss 0.05203401669859886
Total Times. Batch: 816, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011658668518066406, Forward: 0.0814981460571289, Backward: 0.10317587852478027, Optimizer: 0
Epoch 1 Batch 817 Train Loss 0.051900994032621384
Total Times. Batch: 817, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012059211730957031, Forward: 0.08146858215332031, Backward: 0.10330724716186523, Optimizer: 0
Epoch 1 Batch 818 Train Loss 0.04421088099479675
Total Times. Batch: 818, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011653900146484375, Forward: 0.08139467239379883, Backward: 0.1036524772644043, Optimizer: 0
Epoch 1 Batch 819 Train Loss 0.051856447011232376
Total Times. Batch: 819, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001192331314086914, Forward: 0.0814058780670166, Backward: 0.10309982299804688, Optimizer: 0.1685502529144287
Epoch 1 Batch 820 Train Loss 0.04971474036574364
Total Times. Batch: 820, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011434555053710938, Forward: 0.08127284049987793, Backward: 0.09438633918762207, Optimizer: 0
Epoch 1 Batch 821 Train Loss 0.047425802797079086
Total Times. Batch: 821, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001192331314086914, Forward: 0.08117508888244629, Backward: 0.10342788696289062, Optimizer: 0
Epoch 1 Batch 822 Train Loss 0.049291547387838364
Total Times. Batch: 822, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012879371643066406, Forward: 0.08132481575012207, Backward: 0.10343050956726074, Optimizer: 0
Epoch 1 Batch 823 Train Loss 0.04618697613477707
Total Times. Batch: 823, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010995864868164062, Forward: 0.08137154579162598, Backward: 0.10303854942321777, Optimizer: 0
Epoch 1 Batch 824 Train Loss 0.05225146934390068
Total Times. Batch: 824, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012497901916503906, Forward: 0.08110618591308594, Backward: 0.1036217212677002, Optimizer: 0.16797280311584473
Epoch 1 Batch 825 Train Loss 0.05410248041152954
Total Times. Batch: 825, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012059211730957031, Forward: 0.08117556571960449, Backward: 0.09433722496032715, Optimizer: 0
Epoch 1 Batch 826 Train Loss 0.045668233186006546
Total Times. Batch: 826, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011527538299560547, Forward: 0.0814824104309082, Backward: 0.1033470630645752, Optimizer: 0
Epoch 1 Batch 827 Train Loss 0.05183346942067146
Total Times. Batch: 827, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011677742004394531, Forward: 0.08152914047241211, Backward: 0.10297012329101562, Optimizer: 0
Epoch 1 Batch 828 Train Loss 0.045790646225214005
Total Times. Batch: 828, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011975765228271484, Forward: 0.08222031593322754, Backward: 0.10341572761535645, Optimizer: 0
Epoch 1 Batch 829 Train Loss 0.049668196588754654
Total Times. Batch: 829, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012390613555908203, Forward: 0.08154892921447754, Backward: 0.10316324234008789, Optimizer: 0.1672520637512207
Epoch 1 Batch 830 Train Loss 0.05101010948419571
Total Times. Batch: 830, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011911392211914062, Forward: 0.08129405975341797, Backward: 0.09410405158996582, Optimizer: 0
Epoch 1 Batch 831 Train Loss 0.047040726989507675
Total Times. Batch: 831, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0014081001281738281, Forward: 0.08143067359924316, Backward: 0.10342097282409668, Optimizer: 0
Epoch 1 Batch 832 Train Loss 0.043705474585294724
Total Times. Batch: 832, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011582374572753906, Forward: 0.08114242553710938, Backward: 0.10332345962524414, Optimizer: 0
Epoch 1 Batch 833 Train Loss 0.044108789414167404
Total Times. Batch: 833, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012118816375732422, Forward: 0.08132171630859375, Backward: 0.10329341888427734, Optimizer: 0
Epoch 1 Batch 834 Train Loss 0.04869144782423973
Total Times. Batch: 834, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011568069458007812, Forward: 0.08127260208129883, Backward: 0.103118896484375, Optimizer: 0.16843128204345703
Epoch 1 Batch 835 Train Loss 0.047436024993658066
Total Times. Batch: 835, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011982917785644531, Forward: 0.08137226104736328, Backward: 0.09393310546875, Optimizer: 0
Epoch 1 Batch 836 Train Loss 0.0441303513944149
Total Times. Batch: 836, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013513565063476562, Forward: 0.08126020431518555, Backward: 0.10384917259216309, Optimizer: 0
Epoch 1 Batch 837 Train Loss 0.04874827340245247
Total Times. Batch: 837, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011506080627441406, Forward: 0.08120131492614746, Backward: 0.1033937931060791, Optimizer: 0
Epoch 1 Batch 838 Train Loss 0.04604020342230797
Total Times. Batch: 838, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011463165283203125, Forward: 0.08113455772399902, Backward: 0.10355329513549805, Optimizer: 0
Epoch 1 Batch 839 Train Loss 0.04373808577656746
Total Times. Batch: 839, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001085519790649414, Forward: 0.08165645599365234, Backward: 0.1029043197631836, Optimizer: 0.16805577278137207
Epoch 1 Batch 840 Train Loss 0.044501811265945435
Total Times. Batch: 840, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011501312255859375, Forward: 0.08112287521362305, Backward: 0.09430050849914551, Optimizer: 0
Epoch 1 Batch 841 Train Loss 0.04595763236284256
Total Times. Batch: 841, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011823177337646484, Forward: 0.08119583129882812, Backward: 0.10375809669494629, Optimizer: 0
Epoch 1 Batch 842 Train Loss 0.04361211881041527
Total Times. Batch: 842, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011703968048095703, Forward: 0.08228874206542969, Backward: 0.1037135124206543, Optimizer: 0
Epoch 1 Batch 843 Train Loss 0.045522693544626236
Total Times. Batch: 843, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012767314910888672, Forward: 0.08159303665161133, Backward: 0.10324287414550781, Optimizer: 0
Epoch 1 Batch 844 Train Loss 0.043034788221120834
Total Times. Batch: 844, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011434555053710938, Forward: 0.08220410346984863, Backward: 0.10341000556945801, Optimizer: 0.16690731048583984
Epoch 1 Batch 845 Train Loss 0.044328466057777405
Total Times. Batch: 845, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011739730834960938, Forward: 0.08105349540710449, Backward: 0.09358882904052734, Optimizer: 0
Epoch 1 Batch 846 Train Loss 0.04206964001059532
Total Times. Batch: 846, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013890266418457031, Forward: 0.08151602745056152, Backward: 0.10304903984069824, Optimizer: 0
Epoch 1 Batch 847 Train Loss 0.04061867669224739
Total Times. Batch: 847, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.00122833251953125, Forward: 0.08130502700805664, Backward: 0.10325932502746582, Optimizer: 0
Epoch 1 Batch 848 Train Loss 0.04352785274386406
Total Times. Batch: 848, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012104511260986328, Forward: 0.08197379112243652, Backward: 0.10370469093322754, Optimizer: 0
Epoch 1 Batch 849 Train Loss 0.044312428683042526
Total Times. Batch: 849, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011715888977050781, Forward: 0.08220100402832031, Backward: 0.10330080986022949, Optimizer: 0.16662859916687012
Epoch 1 Batch 850 Train Loss 0.04446825385093689
Total Times. Batch: 850, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011360645294189453, Forward: 0.08135581016540527, Backward: 0.09407615661621094, Optimizer: 0
Epoch 1 Batch 851 Train Loss 0.04568341374397278
Total Times. Batch: 851, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013225078582763672, Forward: 0.08177852630615234, Backward: 0.10324597358703613, Optimizer: 0
Epoch 1 Batch 852 Train Loss 0.048123981803655624
Total Times. Batch: 852, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012276172637939453, Forward: 0.08185124397277832, Backward: 0.10316586494445801, Optimizer: 0
Epoch 1 Batch 853 Train Loss 0.04651184007525444
Total Times. Batch: 853, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011959075927734375, Forward: 0.08124995231628418, Backward: 0.10343289375305176, Optimizer: 0
Epoch 1 Batch 854 Train Loss 0.04358001425862312
Total Times. Batch: 854, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011191368103027344, Forward: 0.08101582527160645, Backward: 0.10341024398803711, Optimizer: 0.16720366477966309
Epoch 1 Batch 855 Train Loss 0.04185471683740616
Total Times. Batch: 855, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011851787567138672, Forward: 0.08142280578613281, Backward: 0.09372258186340332, Optimizer: 0
Epoch 1 Batch 856 Train Loss 0.042196013033390045
Total Times. Batch: 856, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012557506561279297, Forward: 0.08192110061645508, Backward: 0.10337972640991211, Optimizer: 0
Epoch 1 Batch 857 Train Loss 0.049813639372587204
Total Times. Batch: 857, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011131763458251953, Forward: 0.0811619758605957, Backward: 0.10318541526794434, Optimizer: 0
Epoch 1 Batch 858 Train Loss 0.053308095782995224
Total Times. Batch: 858, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011019706726074219, Forward: 0.08165764808654785, Backward: 0.10318803787231445, Optimizer: 0
Epoch 1 Batch 859 Train Loss 0.04742783308029175
Total Times. Batch: 859, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001196146011352539, Forward: 0.08132362365722656, Backward: 0.10314798355102539, Optimizer: 0.16666746139526367
Epoch 1 Batch 860 Train Loss 0.04737580940127373
Total Times. Batch: 860, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011744499206542969, Forward: 0.08099079132080078, Backward: 0.09399724006652832, Optimizer: 0
Epoch 1 Batch 861 Train Loss 0.04478379711508751
Total Times. Batch: 861, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012836456298828125, Forward: 0.08192157745361328, Backward: 0.10359334945678711, Optimizer: 0
Epoch 1 Batch 862 Train Loss 0.044438593089580536
Total Times. Batch: 862, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011525154113769531, Forward: 0.08108210563659668, Backward: 0.1038055419921875, Optimizer: 0
Epoch 1 Batch 863 Train Loss 0.049192000180482864
Total Times. Batch: 863, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012052059173583984, Forward: 0.08149194717407227, Backward: 0.10336542129516602, Optimizer: 0
Epoch 1 Batch 864 Train Loss 0.04800118878483772
Total Times. Batch: 864, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001257181167602539, Forward: 0.08153223991394043, Backward: 0.10326957702636719, Optimizer: 0.16715455055236816
Epoch 1 Batch 865 Train Loss 0.050956618040800095
Total Times. Batch: 865, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011293888092041016, Forward: 0.08124780654907227, Backward: 0.09388422966003418, Optimizer: 0
Epoch 1 Batch 866 Train Loss 0.04995955154299736
Total Times. Batch: 866, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012879371643066406, Forward: 0.08135700225830078, Backward: 0.10329222679138184, Optimizer: 0
Epoch 1 Batch 867 Train Loss 0.048050861805677414
Total Times. Batch: 867, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011756420135498047, Forward: 0.08135247230529785, Backward: 0.10354495048522949, Optimizer: 0
Epoch 1 Batch 868 Train Loss 0.049213800579309464
Total Times. Batch: 868, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001140594482421875, Forward: 0.08141899108886719, Backward: 0.10338354110717773, Optimizer: 0
Epoch 1 Batch 869 Train Loss 0.05119127035140991
Total Times. Batch: 869, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001392364501953125, Forward: 0.0818169116973877, Backward: 0.10303258895874023, Optimizer: 0.1671431064605713
Epoch 1 Batch 870 Train Loss 0.049540530890226364
Total Times. Batch: 870, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001190185546875, Forward: 0.0818793773651123, Backward: 0.09361004829406738, Optimizer: 0
Epoch 1 Batch 871 Train Loss 0.052804090082645416
Total Times. Batch: 871, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012857913970947266, Forward: 0.0815420150756836, Backward: 0.10322785377502441, Optimizer: 0
Epoch 1 Batch 872 Train Loss 0.054362665861845016
Total Times. Batch: 872, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011713504791259766, Forward: 0.08100771903991699, Backward: 0.10364151000976562, Optimizer: 0
Epoch 1 Batch 873 Train Loss 0.05137261748313904
Total Times. Batch: 873, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011775493621826172, Forward: 0.08138680458068848, Backward: 0.10382223129272461, Optimizer: 0
Epoch 1 Batch 874 Train Loss 0.0530986487865448
Total Times. Batch: 874, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011475086212158203, Forward: 0.08129596710205078, Backward: 0.10336065292358398, Optimizer: 0.16667795181274414
Epoch 1 Batch 875 Train Loss 0.04029403254389763
Total Times. Batch: 875, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011708736419677734, Forward: 0.0820159912109375, Backward: 0.09382987022399902, Optimizer: 0
Epoch 1 Batch 876 Train Loss 0.0438544787466526
Total Times. Batch: 876, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012187957763671875, Forward: 0.08164572715759277, Backward: 0.10343098640441895, Optimizer: 0
Epoch 1 Batch 877 Train Loss 0.044201575219631195
Total Times. Batch: 877, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001149892807006836, Forward: 0.08138656616210938, Backward: 0.10363054275512695, Optimizer: 0
Epoch 1 Batch 878 Train Loss 0.04400397464632988
Total Times. Batch: 878, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011031627655029297, Forward: 0.08101129531860352, Backward: 0.10375165939331055, Optimizer: 0
Epoch 1 Batch 879 Train Loss 0.04937724396586418
Total Times. Batch: 879, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011472702026367188, Forward: 0.08173155784606934, Backward: 0.10385346412658691, Optimizer: 0.16689157485961914
Epoch 1 Batch 880 Train Loss 0.044826690107584
Total Times. Batch: 880, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011234283447265625, Forward: 0.08121180534362793, Backward: 0.09428977966308594, Optimizer: 0
Epoch 1 Batch 881 Train Loss 0.0413774810731411
Total Times. Batch: 881, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001425027847290039, Forward: 0.08222365379333496, Backward: 0.1035313606262207, Optimizer: 0
Epoch 1 Batch 882 Train Loss 0.04004596546292305
Total Times. Batch: 882, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012040138244628906, Forward: 0.08195710182189941, Backward: 0.10290193557739258, Optimizer: 0
Epoch 1 Batch 883 Train Loss 0.04146292060613632
Total Times. Batch: 883, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011377334594726562, Forward: 0.0822453498840332, Backward: 0.10344791412353516, Optimizer: 0
Epoch 1 Batch 884 Train Loss 0.042246442288160324
Total Times. Batch: 884, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011935234069824219, Forward: 0.08117341995239258, Backward: 0.10337972640991211, Optimizer: 0.1669330596923828
Epoch 1 Batch 885 Train Loss 0.03670554235577583
Total Times. Batch: 885, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011985301971435547, Forward: 0.08141517639160156, Backward: 0.09402990341186523, Optimizer: 0
Epoch 1 Batch 886 Train Loss 0.039113156497478485
Total Times. Batch: 886, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012369155883789062, Forward: 0.08196306228637695, Backward: 0.10305452346801758, Optimizer: 0
Epoch 1 Batch 887 Train Loss 0.03872203826904297
Total Times. Batch: 887, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011973381042480469, Forward: 0.0814204216003418, Backward: 0.10328316688537598, Optimizer: 0
Epoch 1 Batch 888 Train Loss 0.04066888615489006
Total Times. Batch: 888, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011973381042480469, Forward: 0.08152198791503906, Backward: 0.10334134101867676, Optimizer: 0
Epoch 1 Batch 889 Train Loss 0.0383782722055912
Total Times. Batch: 889, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012059211730957031, Forward: 0.08144187927246094, Backward: 0.10332822799682617, Optimizer: 0.16659164428710938
Epoch 1 Batch 890 Train Loss 0.03957192599773407
Total Times. Batch: 890, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011749267578125, Forward: 0.08094954490661621, Backward: 0.09399771690368652, Optimizer: 0
Epoch 1 Batch 891 Train Loss 0.04053044319152832
Total Times. Batch: 891, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011801719665527344, Forward: 0.08179426193237305, Backward: 0.10325479507446289, Optimizer: 0
Epoch 1 Batch 892 Train Loss 0.038679786026477814
Total Times. Batch: 892, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001129150390625, Forward: 0.08166265487670898, Backward: 0.10342073440551758, Optimizer: 0
Epoch 1 Batch 893 Train Loss 0.03620212897658348
Total Times. Batch: 893, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011892318725585938, Forward: 0.0814666748046875, Backward: 0.10331416130065918, Optimizer: 0
Epoch 1 Batch 894 Train Loss 0.039812423288822174
Total Times. Batch: 894, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011849403381347656, Forward: 0.08134078979492188, Backward: 0.10327386856079102, Optimizer: 0.16669201850891113
Epoch 1 Batch 895 Train Loss 0.03667348623275757
Total Times. Batch: 895, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010938644409179688, Forward: 0.08108997344970703, Backward: 0.09382438659667969, Optimizer: 0
Epoch 1 Batch 896 Train Loss 0.03220140561461449
Total Times. Batch: 896, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012252330780029297, Forward: 0.08144950866699219, Backward: 0.10329747200012207, Optimizer: 0
Epoch 1 Batch 897 Train Loss 0.038869231939315796
Total Times. Batch: 897, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001172780990600586, Forward: 0.08145523071289062, Backward: 0.10359358787536621, Optimizer: 0
Epoch 1 Batch 898 Train Loss 0.03805221989750862
Total Times. Batch: 898, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011439323425292969, Forward: 0.08145904541015625, Backward: 0.10353374481201172, Optimizer: 0
Epoch 1 Batch 899 Train Loss 0.03666513040661812
Total Times. Batch: 899, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012137889862060547, Forward: 0.08196401596069336, Backward: 0.1031804084777832, Optimizer: 0.16695809364318848
Epoch 1 Batch 900 Train Loss 0.039061885327100754
Total Times. Batch: 900, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011715888977050781, Forward: 0.0811154842376709, Backward: 0.0935220718383789, Optimizer: 0
Epoch 1 Batch 901 Train Loss 0.038677800446748734
Total Times. Batch: 901, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011775493621826172, Forward: 0.08158326148986816, Backward: 0.1031639575958252, Optimizer: 0
Epoch 1 Batch 902 Train Loss 0.03587604686617851
Total Times. Batch: 902, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011644363403320312, Forward: 0.08143353462219238, Backward: 0.10326004028320312, Optimizer: 0
Epoch 1 Batch 903 Train Loss 0.034229472279548645
Total Times. Batch: 903, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012049674987792969, Forward: 0.08108234405517578, Backward: 0.10355710983276367, Optimizer: 0
Epoch 1 Batch 904 Train Loss 0.03345206007361412
Total Times. Batch: 904, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011303424835205078, Forward: 0.08239364624023438, Backward: 0.10326242446899414, Optimizer: 0.1668558120727539
Epoch 1 Batch 905 Train Loss 0.035504963248968124
Total Times. Batch: 905, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011584758758544922, Forward: 0.08137202262878418, Backward: 0.0938119888305664, Optimizer: 0
Epoch 1 Batch 906 Train Loss 0.03384463116526604
Total Times. Batch: 906, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012586116790771484, Forward: 0.08114433288574219, Backward: 0.1032710075378418, Optimizer: 0
Epoch 1 Batch 907 Train Loss 0.034002285450696945
Total Times. Batch: 907, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011298656463623047, Forward: 0.08147764205932617, Backward: 0.10341572761535645, Optimizer: 0
Epoch 1 Batch 908 Train Loss 0.036269523203372955
Total Times. Batch: 908, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011854171752929688, Forward: 0.08143162727355957, Backward: 0.10345697402954102, Optimizer: 0
Epoch 1 Batch 909 Train Loss 0.03586535155773163
Total Times. Batch: 909, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011675357818603516, Forward: 0.0814371109008789, Backward: 0.10384011268615723, Optimizer: 0.16660618782043457
Epoch 1 Batch 910 Train Loss 0.03383086249232292
Total Times. Batch: 910, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011806488037109375, Forward: 0.08125829696655273, Backward: 0.09399175643920898, Optimizer: 0
Epoch 1 Batch 911 Train Loss 0.03721100091934204
Total Times. Batch: 911, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012240409851074219, Forward: 0.08121323585510254, Backward: 0.10352373123168945, Optimizer: 0
Epoch 1 Batch 912 Train Loss 0.03505447506904602
Total Times. Batch: 912, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011663436889648438, Forward: 0.08148002624511719, Backward: 0.1033792495727539, Optimizer: 0
Epoch 1 Batch 913 Train Loss 0.03577883914113045
Total Times. Batch: 913, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011568069458007812, Forward: 0.08156204223632812, Backward: 0.10347986221313477, Optimizer: 0
Epoch 1 Batch 914 Train Loss 0.03525564447045326
Total Times. Batch: 914, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011625289916992188, Forward: 0.08114385604858398, Backward: 0.10346055030822754, Optimizer: 0.16672158241271973
Epoch 1 Batch 915 Train Loss 0.035141315311193466
Total Times. Batch: 915, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011806488037109375, Forward: 0.08101129531860352, Backward: 0.09370946884155273, Optimizer: 0
Epoch 1 Batch 916 Train Loss 0.036033809185028076
Total Times. Batch: 916, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001252889633178711, Forward: 0.08262991905212402, Backward: 0.10347104072570801, Optimizer: 0
Epoch 1 Batch 917 Train Loss 0.033574581146240234
Total Times. Batch: 917, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012402534484863281, Forward: 0.08128118515014648, Backward: 0.10321378707885742, Optimizer: 0
Epoch 1 Batch 918 Train Loss 0.03380153700709343
Total Times. Batch: 918, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012383460998535156, Forward: 0.08176898956298828, Backward: 0.10340476036071777, Optimizer: 0
Epoch 1 Batch 919 Train Loss 0.03556476905941963
Total Times. Batch: 919, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011031627655029297, Forward: 0.08140134811401367, Backward: 0.10315799713134766, Optimizer: 0.16730475425720215
Epoch 1 Batch 920 Train Loss 0.0332893505692482
Total Times. Batch: 920, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011112689971923828, Forward: 0.08221316337585449, Backward: 0.09375572204589844, Optimizer: 0
Epoch 1 Batch 921 Train Loss 0.03432261571288109
Total Times. Batch: 921, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010924339294433594, Forward: 0.08144116401672363, Backward: 0.10308074951171875, Optimizer: 0
Epoch 1 Batch 922 Train Loss 0.03420809656381607
Total Times. Batch: 922, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011556148529052734, Forward: 0.08108353614807129, Backward: 0.10348749160766602, Optimizer: 0
Epoch 1 Batch 923 Train Loss 0.03347688913345337
Total Times. Batch: 923, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012104511260986328, Forward: 0.08131980895996094, Backward: 0.10341644287109375, Optimizer: 0
Epoch 1 Batch 924 Train Loss 0.03342272341251373
Total Times. Batch: 924, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011737346649169922, Forward: 0.08123564720153809, Backward: 0.10334062576293945, Optimizer: 0.16668987274169922
Epoch 1 Batch 925 Train Loss 0.03063676692545414
Total Times. Batch: 925, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011458396911621094, Forward: 0.08174705505371094, Backward: 0.0941767692565918, Optimizer: 0
Epoch 1 Batch 926 Train Loss 0.033285461366176605
Total Times. Batch: 926, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011065006256103516, Forward: 0.08116316795349121, Backward: 0.10360574722290039, Optimizer: 0
Epoch 1 Batch 927 Train Loss 0.032621290534734726
Total Times. Batch: 927, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011794567108154297, Forward: 0.08192610740661621, Backward: 0.10280370712280273, Optimizer: 0
Epoch 1 Batch 928 Train Loss 0.030001461505889893
Total Times. Batch: 928, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011413097381591797, Forward: 0.08157587051391602, Backward: 0.10326385498046875, Optimizer: 0
Epoch 1 Batch 929 Train Loss 0.03356708213686943
Total Times. Batch: 929, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012013912200927734, Forward: 0.08165502548217773, Backward: 0.10322785377502441, Optimizer: 0.16672468185424805
Epoch 1 Batch 930 Train Loss 0.029601294547319412
Total Times. Batch: 930, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011572837829589844, Forward: 0.08131575584411621, Backward: 0.09326791763305664, Optimizer: 0
Epoch 1 Batch 931 Train Loss 0.03079777956008911
Total Times. Batch: 931, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012221336364746094, Forward: 0.08119082450866699, Backward: 0.10355520248413086, Optimizer: 0
Epoch 1 Batch 932 Train Loss 0.03146412596106529
Total Times. Batch: 932, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011501312255859375, Forward: 0.08104991912841797, Backward: 0.10322022438049316, Optimizer: 0
Epoch 1 Batch 933 Train Loss 0.03201047331094742
Total Times. Batch: 933, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011751651763916016, Forward: 0.08205771446228027, Backward: 0.10322046279907227, Optimizer: 0
Epoch 1 Batch 934 Train Loss 0.030196739360690117
Total Times. Batch: 934, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001134634017944336, Forward: 0.08263373374938965, Backward: 0.10313105583190918, Optimizer: 0.1673121452331543
Epoch 1 Batch 935 Train Loss 0.03167536482214928
Total Times. Batch: 935, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001173257827758789, Forward: 0.08165144920349121, Backward: 0.09373164176940918, Optimizer: 0
Epoch 1 Batch 936 Train Loss 0.030395179986953735
Total Times. Batch: 936, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013422966003417969, Forward: 0.08150482177734375, Backward: 0.103424072265625, Optimizer: 0
Epoch 1 Batch 937 Train Loss 0.03172736242413521
Total Times. Batch: 937, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001190185546875, Forward: 0.08212423324584961, Backward: 0.10356807708740234, Optimizer: 0
Epoch 1 Batch 938 Train Loss 0.030522450804710388
Total Times. Batch: 938, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010974407196044922, Forward: 0.08161020278930664, Backward: 0.10349678993225098, Optimizer: 0
Epoch 1 Batch 939 Train Loss 0.030955825001001358
Total Times. Batch: 939, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011942386627197266, Forward: 0.08133149147033691, Backward: 0.10325956344604492, Optimizer: 0.16718077659606934
Epoch 1 Batch 940 Train Loss 0.028054898604750633
Total Times. Batch: 940, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011310577392578125, Forward: 0.08162903785705566, Backward: 0.09336495399475098, Optimizer: 0
Epoch 1 Batch 941 Train Loss 0.02880345843732357
Total Times. Batch: 941, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011932849884033203, Forward: 0.08198928833007812, Backward: 0.10305976867675781, Optimizer: 0
Epoch 1 Batch 942 Train Loss 0.02981647290289402
Total Times. Batch: 942, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011107921600341797, Forward: 0.08150744438171387, Backward: 0.10335350036621094, Optimizer: 0
Epoch 1 Batch 943 Train Loss 0.029030805453658104
Total Times. Batch: 943, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001186370849609375, Forward: 0.08134078979492188, Backward: 0.10344290733337402, Optimizer: 0
Epoch 1 Batch 944 Train Loss 0.029951388016343117
Total Times. Batch: 944, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001119852066040039, Forward: 0.08099770545959473, Backward: 0.10407853126525879, Optimizer: 0.16659069061279297
Epoch 1 Batch 945 Train Loss 0.028415992856025696
Total Times. Batch: 945, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011570453643798828, Forward: 0.08107900619506836, Backward: 0.09380626678466797, Optimizer: 0
Epoch 1 Batch 946 Train Loss 0.029511332511901855
Total Times. Batch: 946, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012640953063964844, Forward: 0.08198285102844238, Backward: 0.10268974304199219, Optimizer: 0
Epoch 1 Batch 947 Train Loss 0.02730986475944519
Total Times. Batch: 947, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012221336364746094, Forward: 0.08114075660705566, Backward: 0.10334300994873047, Optimizer: 0
Epoch 1 Batch 948 Train Loss 0.02948659099638462
Total Times. Batch: 948, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011887550354003906, Forward: 0.08145976066589355, Backward: 0.10329985618591309, Optimizer: 0
Epoch 1 Batch 949 Train Loss 0.0286633912473917
Total Times. Batch: 949, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001146554946899414, Forward: 0.08106708526611328, Backward: 0.10378837585449219, Optimizer: 0.1666107177734375
Epoch 1 Batch 950 Train Loss 0.02770775556564331
Total Times. Batch: 950, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011584758758544922, Forward: 0.08122634887695312, Backward: 0.09411096572875977, Optimizer: 0
Epoch 1 Batch 951 Train Loss 0.02859288454055786
Total Times. Batch: 951, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012638568878173828, Forward: 0.08155941963195801, Backward: 0.1031031608581543, Optimizer: 0
Epoch 1 Batch 952 Train Loss 0.029483838006854057
Total Times. Batch: 952, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011436939239501953, Forward: 0.08235311508178711, Backward: 0.10334372520446777, Optimizer: 0
Epoch 1 Batch 953 Train Loss 0.028334403410553932
Total Times. Batch: 953, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012257099151611328, Forward: 0.08205914497375488, Backward: 0.10339212417602539, Optimizer: 0
Epoch 1 Batch 954 Train Loss 0.029768500477075577
Total Times. Batch: 954, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011909008026123047, Forward: 0.08172464370727539, Backward: 0.10338091850280762, Optimizer: 0.16722989082336426
Epoch 1 Batch 955 Train Loss 0.027612706646323204
Total Times. Batch: 955, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011601448059082031, Forward: 0.08101987838745117, Backward: 0.09420323371887207, Optimizer: 0
Epoch 1 Batch 956 Train Loss 0.028172189369797707
Total Times. Batch: 956, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012798309326171875, Forward: 0.08130502700805664, Backward: 0.10357904434204102, Optimizer: 0
Epoch 1 Batch 957 Train Loss 0.028195781633257866
Total Times. Batch: 957, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011942386627197266, Forward: 0.08143281936645508, Backward: 0.10361814498901367, Optimizer: 0
Epoch 1 Batch 958 Train Loss 0.028464505448937416
Total Times. Batch: 958, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011582374572753906, Forward: 0.08150339126586914, Backward: 0.10290646553039551, Optimizer: 0
Epoch 1 Batch 959 Train Loss 0.028321832418441772
Total Times. Batch: 959, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011527538299560547, Forward: 0.08117151260375977, Backward: 0.10355830192565918, Optimizer: 0.16750192642211914
Epoch 1 Batch 960 Train Loss 0.026707855984568596
Total Times. Batch: 960, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001176595687866211, Forward: 0.0811319351196289, Backward: 0.09391474723815918, Optimizer: 0
Epoch 1 Batch 961 Train Loss 0.025368142873048782
Total Times. Batch: 961, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012545585632324219, Forward: 0.08122754096984863, Backward: 0.10315513610839844, Optimizer: 0
Epoch 1 Batch 962 Train Loss 0.026735443621873856
Total Times. Batch: 962, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001180410385131836, Forward: 0.08198213577270508, Backward: 0.10353493690490723, Optimizer: 0
Epoch 1 Batch 963 Train Loss 0.029287904500961304
Total Times. Batch: 963, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011754035949707031, Forward: 0.08122992515563965, Backward: 0.10393524169921875, Optimizer: 0
Epoch 1 Batch 964 Train Loss 0.02887231670320034
Total Times. Batch: 964, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011513233184814453, Forward: 0.08148550987243652, Backward: 0.10352611541748047, Optimizer: 0.16710734367370605
Epoch 1 Batch 965 Train Loss 0.02663898840546608
Total Times. Batch: 965, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011768341064453125, Forward: 0.08135080337524414, Backward: 0.0938270092010498, Optimizer: 0
Epoch 1 Batch 966 Train Loss 0.02600603736937046
Total Times. Batch: 966, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012774467468261719, Forward: 0.08119416236877441, Backward: 0.10329532623291016, Optimizer: 0
Epoch 1 Batch 967 Train Loss 0.02728106640279293
Total Times. Batch: 967, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001149892807006836, Forward: 0.08121109008789062, Backward: 0.10311102867126465, Optimizer: 0
Epoch 1 Batch 968 Train Loss 0.027036596089601517
Total Times. Batch: 968, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001161336898803711, Forward: 0.08160209655761719, Backward: 0.10309171676635742, Optimizer: 0
Epoch 1 Batch 969 Train Loss 0.02654547058045864
Total Times. Batch: 969, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001165151596069336, Forward: 0.08103322982788086, Backward: 0.10371279716491699, Optimizer: 0.16713476181030273
Epoch 1 Batch 970 Train Loss 0.027235424146056175
Total Times. Batch: 970, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001336812973022461, Forward: 0.08229708671569824, Backward: 0.09363174438476562, Optimizer: 0
Epoch 1 Batch 971 Train Loss 0.02569153718650341
Total Times. Batch: 971, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012319087982177734, Forward: 0.08129405975341797, Backward: 0.1034548282623291, Optimizer: 0
Epoch 1 Batch 972 Train Loss 0.026817215606570244
Total Times. Batch: 972, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011031627655029297, Forward: 0.08123564720153809, Backward: 0.10354924201965332, Optimizer: 0
Epoch 1 Batch 973 Train Loss 0.026479577645659447
Total Times. Batch: 973, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001096963882446289, Forward: 0.08119416236877441, Backward: 0.10349273681640625, Optimizer: 0
Epoch 1 Batch 974 Train Loss 0.026160378009080887
Total Times. Batch: 974, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011646747589111328, Forward: 0.08120107650756836, Backward: 0.10333847999572754, Optimizer: 0.16715621948242188
Epoch 1 Batch 975 Train Loss 0.025845101103186607
Total Times. Batch: 975, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011703968048095703, Forward: 0.08087801933288574, Backward: 0.09384965896606445, Optimizer: 0
Epoch 1 Batch 976 Train Loss 0.025425231084227562
Total Times. Batch: 976, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011336803436279297, Forward: 0.08182787895202637, Backward: 0.10357093811035156, Optimizer: 0
Epoch 1 Batch 977 Train Loss 0.02671542763710022
Total Times. Batch: 977, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001214742660522461, Forward: 0.08231997489929199, Backward: 0.10288023948669434, Optimizer: 0
Epoch 1 Batch 978 Train Loss 0.02536291815340519
Total Times. Batch: 978, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012392997741699219, Forward: 0.08131980895996094, Backward: 0.10360527038574219, Optimizer: 0
Epoch 1 Batch 979 Train Loss 0.025741690769791603
Total Times. Batch: 979, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011975765228271484, Forward: 0.08165478706359863, Backward: 0.10373568534851074, Optimizer: 0.16707563400268555
Epoch 1 Batch 980 Train Loss 0.0244308989495039
Total Times. Batch: 980, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011076927185058594, Forward: 0.08132672309875488, Backward: 0.09409880638122559, Optimizer: 0
Epoch 1 Batch 981 Train Loss 0.025156570598483086
Total Times. Batch: 981, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012104511260986328, Forward: 0.08139467239379883, Backward: 0.1032569408416748, Optimizer: 0
Epoch 1 Batch 982 Train Loss 0.024827241897583008
Total Times. Batch: 982, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001135110855102539, Forward: 0.08130955696105957, Backward: 0.10327649116516113, Optimizer: 0
Epoch 1 Batch 983 Train Loss 0.025894835591316223
Total Times. Batch: 983, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011682510375976562, Forward: 0.08156561851501465, Backward: 0.10318756103515625, Optimizer: 0
Epoch 1 Batch 984 Train Loss 0.0259981881827116
Total Times. Batch: 984, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011544227600097656, Forward: 0.08119940757751465, Backward: 0.1033322811126709, Optimizer: 0.16687893867492676
Epoch 1 Batch 985 Train Loss 0.02522875741124153
Total Times. Batch: 985, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011060237884521484, Forward: 0.08120155334472656, Backward: 0.09415388107299805, Optimizer: 0
Epoch 1 Batch 986 Train Loss 0.02558298222720623
Total Times. Batch: 986, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012612342834472656, Forward: 0.08102154731750488, Backward: 0.10355997085571289, Optimizer: 0
Epoch 1 Batch 987 Train Loss 0.02541103959083557
Total Times. Batch: 987, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011053085327148438, Forward: 0.08138418197631836, Backward: 0.1029348373413086, Optimizer: 0
Epoch 1 Batch 988 Train Loss 0.026119867339730263
Total Times. Batch: 988, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001134634017944336, Forward: 0.08149266242980957, Backward: 0.10305142402648926, Optimizer: 0
Epoch 1 Batch 989 Train Loss 0.025854796171188354
Total Times. Batch: 989, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012030601501464844, Forward: 0.0816185474395752, Backward: 0.10332131385803223, Optimizer: 0.16818690299987793
Epoch 1 Batch 990 Train Loss 0.02474023960530758
Total Times. Batch: 990, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011754035949707031, Forward: 0.08111882209777832, Backward: 0.09408330917358398, Optimizer: 0
Epoch 1 Batch 991 Train Loss 0.02555629424750805
Total Times. Batch: 991, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011823177337646484, Forward: 0.08165550231933594, Backward: 0.10314583778381348, Optimizer: 0
Epoch 1 Batch 992 Train Loss 0.026951400563120842
Total Times. Batch: 992, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011572837829589844, Forward: 0.08130002021789551, Backward: 0.10362100601196289, Optimizer: 0
Epoch 1 Batch 993 Train Loss 0.024883095175027847
Total Times. Batch: 993, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011599063873291016, Forward: 0.08162450790405273, Backward: 0.1035916805267334, Optimizer: 0
Epoch 1 Batch 994 Train Loss 0.025910604745149612
Total Times. Batch: 994, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011456012725830078, Forward: 0.08154177665710449, Backward: 0.10318279266357422, Optimizer: 0.16713929176330566
Epoch 1 Batch 995 Train Loss 0.024176597595214844
Total Times. Batch: 995, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011730194091796875, Forward: 0.0813596248626709, Backward: 0.09384965896606445, Optimizer: 0
Epoch 1 Batch 996 Train Loss 0.02618544176220894
Total Times. Batch: 996, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0014717578887939453, Forward: 0.08176040649414062, Backward: 0.1031186580657959, Optimizer: 0
Epoch 1 Batch 997 Train Loss 0.024968458339571953
Total Times. Batch: 997, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012118816375732422, Forward: 0.08169245719909668, Backward: 0.10328483581542969, Optimizer: 0
Epoch 1 Batch 998 Train Loss 0.024764901027083397
Total Times. Batch: 998, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010993480682373047, Forward: 0.08119797706604004, Backward: 0.1033940315246582, Optimizer: 0
Epoch 1 Batch 999 Train Loss 0.024323217570781708
Total Times. Batch: 999, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001188516616821289, Forward: 0.08128094673156738, Backward: 0.1032876968383789, Optimizer: 0.16727471351623535
Epoch 1 Batch 1000 Train Loss 0.025685405358672142
Total Times. Batch: 1000, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011386871337890625, Forward: 0.08132147789001465, Backward: 0.09384965896606445, Optimizer: 0
Epoch 1 Batch 1001 Train Loss 0.02531278133392334
Total Times. Batch: 1001, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013134479522705078, Forward: 0.08150625228881836, Backward: 0.10289907455444336, Optimizer: 0
Epoch 1 Batch 1002 Train Loss 0.024688277393579483
Total Times. Batch: 1002, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011398792266845703, Forward: 0.08108210563659668, Backward: 0.10342955589294434, Optimizer: 0
Epoch 1 Batch 1003 Train Loss 0.024229979142546654
Total Times. Batch: 1003, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010912418365478516, Forward: 0.08129405975341797, Backward: 0.1034090518951416, Optimizer: 0
ng: 2 lr: 0.9752835407290538 dlr: 0.004517307285699026 d_hat: -0.002378409630263416, d: 0.004631788702516401. sksq_weighted=9.9e-08 sk_l1=4.7e-04 gsq_weighted=1.1e-05
Epoch 1 Batch 1004 Train Loss 0.02542959153652191
Total Times. Batch: 1004, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011072158813476562, Forward: 0.08154606819152832, Backward: 0.1033027172088623, Optimizer: 0.16705989837646484
Epoch 1 Batch 1005 Train Loss 0.025048261508345604
Total Times. Batch: 1005, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011425018310546875, Forward: 0.08108711242675781, Backward: 0.09394478797912598, Optimizer: 0
Epoch 1 Batch 1006 Train Loss 0.02560420334339142
Total Times. Batch: 1006, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012500286102294922, Forward: 0.08149838447570801, Backward: 0.10319948196411133, Optimizer: 0
Epoch 1 Batch 1007 Train Loss 0.02455306425690651
Total Times. Batch: 1007, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001233816146850586, Forward: 0.08135128021240234, Backward: 0.10309576988220215, Optimizer: 0
Epoch 1 Batch 1008 Train Loss 0.02514287643134594
Total Times. Batch: 1008, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011088848114013672, Forward: 0.08190488815307617, Backward: 0.10328292846679688, Optimizer: 0
Epoch 1 Batch 1009 Train Loss 0.025468194857239723
Total Times. Batch: 1009, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011744499206542969, Forward: 0.08158659934997559, Backward: 0.10340499877929688, Optimizer: 0.16744065284729004
Epoch 1 Batch 1010 Train Loss 0.024883894249796867
Total Times. Batch: 1010, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010800361633300781, Forward: 0.08130431175231934, Backward: 0.09365630149841309, Optimizer: 0
Epoch 1 Batch 1011 Train Loss 0.02548552118241787
Total Times. Batch: 1011, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011510848999023438, Forward: 0.08125996589660645, Backward: 0.10365509986877441, Optimizer: 0
Epoch 1 Batch 1012 Train Loss 0.02436874248087406
Total Times. Batch: 1012, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011920928955078125, Forward: 0.08106374740600586, Backward: 0.10347700119018555, Optimizer: 0
Epoch 1 Batch 1013 Train Loss 0.02450268529355526
Total Times. Batch: 1013, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012018680572509766, Forward: 0.08141779899597168, Backward: 0.1032569408416748, Optimizer: 0
Epoch 1 Batch 1014 Train Loss 0.023560281842947006
Total Times. Batch: 1014, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010991096496582031, Forward: 0.0813133716583252, Backward: 0.10316801071166992, Optimizer: 0.16687631607055664
Epoch 1 Batch 1015 Train Loss 0.023394042626023293
Total Times. Batch: 1015, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001146554946899414, Forward: 0.08179903030395508, Backward: 0.09382367134094238, Optimizer: 0
Epoch 1 Batch 1016 Train Loss 0.02543225884437561
Total Times. Batch: 1016, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012655258178710938, Forward: 0.08138227462768555, Backward: 0.10354065895080566, Optimizer: 0
Epoch 1 Batch 1017 Train Loss 0.0251536313444376
Total Times. Batch: 1017, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011696815490722656, Forward: 0.08148407936096191, Backward: 0.10366344451904297, Optimizer: 0
Epoch 1 Batch 1018 Train Loss 0.023296577855944633
Total Times. Batch: 1018, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011363029479980469, Forward: 0.08165359497070312, Backward: 0.1031653881072998, Optimizer: 0
Epoch 1 Batch 1019 Train Loss 0.025198865681886673
Total Times. Batch: 1019, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011937618255615234, Forward: 0.08124089241027832, Backward: 0.1033775806427002, Optimizer: 0.16687440872192383
Epoch 1 Batch 1020 Train Loss 0.023551540449261665
Total Times. Batch: 1020, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011544227600097656, Forward: 0.08129143714904785, Backward: 0.09360098838806152, Optimizer: 0
Epoch 1 Batch 1021 Train Loss 0.025434795767068863
Total Times. Batch: 1021, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010900497436523438, Forward: 0.08105230331420898, Backward: 0.10336947441101074, Optimizer: 0
Epoch 1 Batch 1022 Train Loss 0.024639440700411797
Total Times. Batch: 1022, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011668205261230469, Forward: 0.08117246627807617, Backward: 0.10343742370605469, Optimizer: 0
Epoch 1 Batch 1023 Train Loss 0.023816442117094994
Total Times. Batch: 1023, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011649131774902344, Forward: 0.08118438720703125, Backward: 0.10338234901428223, Optimizer: 0
Epoch 1 Batch 1024 Train Loss 0.02399699203670025
Total Times. Batch: 1024, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001138448715209961, Forward: 0.08162260055541992, Backward: 0.10312819480895996, Optimizer: 0.16810202598571777
Epoch 1 Batch 1025 Train Loss 0.02342413365840912
Total Times. Batch: 1025, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011866092681884766, Forward: 0.08107614517211914, Backward: 0.09407424926757812, Optimizer: 0
Epoch 1 Batch 1026 Train Loss 0.023817112669348717
Total Times. Batch: 1026, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011501312255859375, Forward: 0.08127069473266602, Backward: 0.10358190536499023, Optimizer: 0
Epoch 1 Batch 1027 Train Loss 0.022862160578370094
Total Times. Batch: 1027, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011563301086425781, Forward: 0.08112549781799316, Backward: 0.10384321212768555, Optimizer: 0
Epoch 1 Batch 1028 Train Loss 0.023491716012358665
Total Times. Batch: 1028, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001188039779663086, Forward: 0.08101415634155273, Backward: 0.1037449836730957, Optimizer: 0
Epoch 1 Batch 1029 Train Loss 0.023099662736058235
Total Times. Batch: 1029, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011582374572753906, Forward: 0.08145785331726074, Backward: 0.10310482978820801, Optimizer: 0.16657662391662598
Epoch 1 Batch 1030 Train Loss 0.024555301293730736
Total Times. Batch: 1030, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011336803436279297, Forward: 0.08131098747253418, Backward: 0.09394550323486328, Optimizer: 0
Epoch 1 Batch 1031 Train Loss 0.023908477276563644
Total Times. Batch: 1031, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013468265533447266, Forward: 0.08159041404724121, Backward: 0.10317039489746094, Optimizer: 0
Epoch 1 Batch 1032 Train Loss 0.02372797019779682
Total Times. Batch: 1032, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012552738189697266, Forward: 0.08145427703857422, Backward: 0.10315513610839844, Optimizer: 0
Epoch 1 Batch 1033 Train Loss 0.024646274745464325
Total Times. Batch: 1033, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011458396911621094, Forward: 0.08121490478515625, Backward: 0.10352540016174316, Optimizer: 0
Epoch 1 Batch 1034 Train Loss 0.024105129763484
Total Times. Batch: 1034, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011682510375976562, Forward: 0.0812067985534668, Backward: 0.10375332832336426, Optimizer: 0.16690993309020996
Epoch 1 Batch 1035 Train Loss 0.023214561864733696
Total Times. Batch: 1035, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001110076904296875, Forward: 0.0810854434967041, Backward: 0.09386539459228516, Optimizer: 0
Epoch 1 Batch 1036 Train Loss 0.023454513400793076
Total Times. Batch: 1036, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012538433074951172, Forward: 0.08194828033447266, Backward: 0.10314130783081055, Optimizer: 0
Epoch 1 Batch 1037 Train Loss 0.02320895344018936
Total Times. Batch: 1037, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001188039779663086, Forward: 0.08159470558166504, Backward: 0.10315227508544922, Optimizer: 0
Epoch 1 Batch 1038 Train Loss 0.02367652766406536
Total Times. Batch: 1038, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013546943664550781, Forward: 0.08110952377319336, Backward: 0.1033623218536377, Optimizer: 0
Epoch 1 Batch 1039 Train Loss 0.02393624745309353
Total Times. Batch: 1039, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011370182037353516, Forward: 0.0814676284790039, Backward: 0.10336041450500488, Optimizer: 0.16706490516662598
Epoch 1 Batch 1040 Train Loss 0.023271555081009865
Total Times. Batch: 1040, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011348724365234375, Forward: 0.08119535446166992, Backward: 0.09417295455932617, Optimizer: 0
Epoch 1 Batch 1041 Train Loss 0.02283775620162487
Total Times. Batch: 1041, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011496543884277344, Forward: 0.08115172386169434, Backward: 0.10339808464050293, Optimizer: 0
Epoch 1 Batch 1042 Train Loss 0.023955630138516426
Total Times. Batch: 1042, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012693405151367188, Forward: 0.08151102066040039, Backward: 0.10369610786437988, Optimizer: 0
Epoch 1 Batch 1043 Train Loss 0.024444222450256348
Total Times. Batch: 1043, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001222848892211914, Forward: 0.08164715766906738, Backward: 0.10351133346557617, Optimizer: 0
Epoch 1 Batch 1044 Train Loss 0.024512462317943573
Total Times. Batch: 1044, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001163482666015625, Forward: 0.08107876777648926, Backward: 0.10335469245910645, Optimizer: 0.16709184646606445
Epoch 1 Batch 1045 Train Loss 0.023423749953508377
Total Times. Batch: 1045, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001142740249633789, Forward: 0.08123087882995605, Backward: 0.093963623046875, Optimizer: 0
Epoch 1 Batch 1046 Train Loss 0.022085994482040405
Total Times. Batch: 1046, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012047290802001953, Forward: 0.08143281936645508, Backward: 0.10330724716186523, Optimizer: 0
Epoch 1 Batch 1047 Train Loss 0.023169463500380516
Total Times. Batch: 1047, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.00115203857421875, Forward: 0.08128666877746582, Backward: 0.10377001762390137, Optimizer: 0
Epoch 1 Batch 1048 Train Loss 0.023681124672293663
Total Times. Batch: 1048, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011565685272216797, Forward: 0.08116292953491211, Backward: 0.1034383773803711, Optimizer: 0
Epoch 1 Batch 1049 Train Loss 0.023564761504530907
Total Times. Batch: 1049, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001332998275756836, Forward: 0.08188891410827637, Backward: 0.10339522361755371, Optimizer: 0.16696763038635254
Epoch 1 Batch 1050 Train Loss 0.023131266236305237
Total Times. Batch: 1050, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011763572692871094, Forward: 0.08179950714111328, Backward: 0.09382510185241699, Optimizer: 0
Epoch 1 Batch 1051 Train Loss 0.02227950282394886
Total Times. Batch: 1051, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013031959533691406, Forward: 0.08128070831298828, Backward: 0.10350346565246582, Optimizer: 0
Epoch 1 Batch 1052 Train Loss 0.021681146696209908
Total Times. Batch: 1052, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011746883392333984, Forward: 0.08107233047485352, Backward: 0.10326242446899414, Optimizer: 0
Epoch 1 Batch 1053 Train Loss 0.023336738348007202
Total Times. Batch: 1053, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011060237884521484, Forward: 0.0816493034362793, Backward: 0.1034705638885498, Optimizer: 0
Epoch 1 Batch 1054 Train Loss 0.02355603687465191
Total Times. Batch: 1054, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011518001556396484, Forward: 0.08186626434326172, Backward: 0.10322046279907227, Optimizer: 0.16655755043029785
Epoch 1 Batch 1055 Train Loss 0.023692552000284195
Total Times. Batch: 1055, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011196136474609375, Forward: 0.08171200752258301, Backward: 0.09370756149291992, Optimizer: 0
Epoch 1 Batch 1056 Train Loss 0.022630512714385986
Total Times. Batch: 1056, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001257181167602539, Forward: 0.08137178421020508, Backward: 0.103057861328125, Optimizer: 0
Epoch 1 Batch 1057 Train Loss 0.02313058450818062
Total Times. Batch: 1057, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011641979217529297, Forward: 0.08151841163635254, Backward: 0.1034390926361084, Optimizer: 0
Epoch 1 Batch 1058 Train Loss 0.022072002291679382
Total Times. Batch: 1058, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010917186737060547, Forward: 0.0812232494354248, Backward: 0.1032724380493164, Optimizer: 0
Epoch 1 Batch 1059 Train Loss 0.022929657250642776
Total Times. Batch: 1059, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011544227600097656, Forward: 0.08122014999389648, Backward: 0.10331463813781738, Optimizer: 0.16719794273376465
Epoch 1 Batch 1060 Train Loss 0.02229407988488674
Total Times. Batch: 1060, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011479854583740234, Forward: 0.08147192001342773, Backward: 0.09366083145141602, Optimizer: 0
Epoch 1 Batch 1061 Train Loss 0.021303463727235794
Total Times. Batch: 1061, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012969970703125, Forward: 0.08132266998291016, Backward: 0.1035010814666748, Optimizer: 0
Epoch 1 Batch 1062 Train Loss 0.022447291761636734
Total Times. Batch: 1062, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001191854476928711, Forward: 0.08112215995788574, Backward: 0.10360884666442871, Optimizer: 0
Epoch 1 Batch 1063 Train Loss 0.022910675033926964
Total Times. Batch: 1063, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012514591217041016, Forward: 0.08251571655273438, Backward: 0.10351300239562988, Optimizer: 0
Epoch 1 Batch 1064 Train Loss 0.023362746462225914
Total Times. Batch: 1064, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011789798736572266, Forward: 0.08211421966552734, Backward: 0.1032249927520752, Optimizer: 0.16683626174926758
Epoch 1 Batch 1065 Train Loss 0.02201451174914837
Total Times. Batch: 1065, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001178741455078125, Forward: 0.08168816566467285, Backward: 0.09397530555725098, Optimizer: 0
Epoch 1 Batch 1066 Train Loss 0.022588664665818214
Total Times. Batch: 1066, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011467933654785156, Forward: 0.08172059059143066, Backward: 0.10332822799682617, Optimizer: 0
Epoch 1 Batch 1067 Train Loss 0.02292048931121826
Total Times. Batch: 1067, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012314319610595703, Forward: 0.08161234855651855, Backward: 0.10322904586791992, Optimizer: 0
Epoch 1 Batch 1068 Train Loss 0.021324677392840385
Total Times. Batch: 1068, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001154184341430664, Forward: 0.08139348030090332, Backward: 0.10322809219360352, Optimizer: 0
Epoch 1 Batch 1069 Train Loss 0.023254092782735825
Total Times. Batch: 1069, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011141300201416016, Forward: 0.08110332489013672, Backward: 0.10363316535949707, Optimizer: 0.16747069358825684
Epoch 1 Batch 1070 Train Loss 0.022266102954745293
Total Times. Batch: 1070, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011174678802490234, Forward: 0.08105707168579102, Backward: 0.09380364418029785, Optimizer: 0
Epoch 1 Batch 1071 Train Loss 0.021903200075030327
Total Times. Batch: 1071, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011110305786132812, Forward: 0.08134269714355469, Backward: 0.10352230072021484, Optimizer: 0
Epoch 1 Batch 1072 Train Loss 0.021918734535574913
Total Times. Batch: 1072, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011484622955322266, Forward: 0.08202791213989258, Backward: 0.10338187217712402, Optimizer: 0
Epoch 1 Batch 1073 Train Loss 0.022279610857367516
Total Times. Batch: 1073, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012214183807373047, Forward: 0.08183717727661133, Backward: 0.10338664054870605, Optimizer: 0
Epoch 1 Batch 1074 Train Loss 0.021926138550043106
Total Times. Batch: 1074, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011715888977050781, Forward: 0.08128643035888672, Backward: 0.1033775806427002, Optimizer: 0.16704559326171875
Epoch 1 Batch 1075 Train Loss 0.0221771951764822
Total Times. Batch: 1075, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001149892807006836, Forward: 0.08089828491210938, Backward: 0.09431934356689453, Optimizer: 0
Epoch 1 Batch 1076 Train Loss 0.02233448438346386
Total Times. Batch: 1076, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012829303741455078, Forward: 0.08139228820800781, Backward: 0.10350298881530762, Optimizer: 0
Epoch 1 Batch 1077 Train Loss 0.023051319643855095
Total Times. Batch: 1077, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011630058288574219, Forward: 0.08133244514465332, Backward: 0.103912353515625, Optimizer: 0
Epoch 1 Batch 1078 Train Loss 0.022026419639587402
Total Times. Batch: 1078, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011568069458007812, Forward: 0.08143925666809082, Backward: 0.10342168807983398, Optimizer: 0
Epoch 1 Batch 1079 Train Loss 0.021507931873202324
Total Times. Batch: 1079, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012090206146240234, Forward: 0.08124637603759766, Backward: 0.1031484603881836, Optimizer: 0.16697144508361816
Epoch 1 Batch 1080 Train Loss 0.021457603201270103
Total Times. Batch: 1080, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011548995971679688, Forward: 0.08143997192382812, Backward: 0.09358787536621094, Optimizer: 0
Epoch 1 Batch 1081 Train Loss 0.021451598033308983
Total Times. Batch: 1081, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001241922378540039, Forward: 0.08143901824951172, Backward: 0.10343217849731445, Optimizer: 0
Epoch 1 Batch 1082 Train Loss 0.021408187225461006
Total Times. Batch: 1082, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013453960418701172, Forward: 0.0810995101928711, Backward: 0.10324501991271973, Optimizer: 0
Epoch 1 Batch 1083 Train Loss 0.021479515358805656
Total Times. Batch: 1083, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010943412780761719, Forward: 0.08203673362731934, Backward: 0.10329437255859375, Optimizer: 0
Epoch 1 Batch 1084 Train Loss 0.022377582266926765
Total Times. Batch: 1084, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011341571807861328, Forward: 0.08130502700805664, Backward: 0.10323095321655273, Optimizer: 0.16683387756347656
Epoch 1 Batch 1085 Train Loss 0.02181568555533886
Total Times. Batch: 1085, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012116432189941406, Forward: 0.08126950263977051, Backward: 0.09349846839904785, Optimizer: 0
Epoch 1 Batch 1086 Train Loss 0.02155260741710663
Total Times. Batch: 1086, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013954639434814453, Forward: 0.08115577697753906, Backward: 0.1035764217376709, Optimizer: 0
Epoch 1 Batch 1087 Train Loss 0.02325521968305111
Total Times. Batch: 1087, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001138448715209961, Forward: 0.08148717880249023, Backward: 0.10357046127319336, Optimizer: 0
Epoch 1 Batch 1088 Train Loss 0.021360892802476883
Total Times. Batch: 1088, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001094818115234375, Forward: 0.08143305778503418, Backward: 0.10324239730834961, Optimizer: 0
Epoch 1 Batch 1089 Train Loss 0.022188683971762657
Total Times. Batch: 1089, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011749267578125, Forward: 0.0811765193939209, Backward: 0.1031944751739502, Optimizer: 0.16678357124328613
Epoch 1 Batch 1090 Train Loss 0.021256564185023308
Total Times. Batch: 1090, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001163482666015625, Forward: 0.08139419555664062, Backward: 0.09393072128295898, Optimizer: 0
Epoch 1 Batch 1091 Train Loss 0.02161256968975067
Total Times. Batch: 1091, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0015316009521484375, Forward: 0.08144187927246094, Backward: 0.1032094955444336, Optimizer: 0
Epoch 1 Batch 1092 Train Loss 0.02116217091679573
Total Times. Batch: 1092, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011692047119140625, Forward: 0.08146476745605469, Backward: 0.10338997840881348, Optimizer: 0
Epoch 1 Batch 1093 Train Loss 0.02232237718999386
Total Times. Batch: 1093, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011477470397949219, Forward: 0.08196473121643066, Backward: 0.10377001762390137, Optimizer: 0
Epoch 1 Batch 1094 Train Loss 0.022380009293556213
Total Times. Batch: 1094, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011849403381347656, Forward: 0.08172416687011719, Backward: 0.10350918769836426, Optimizer: 0.16652774810791016
Epoch 1 Batch 1095 Train Loss 0.022426018491387367
Total Times. Batch: 1095, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001199960708618164, Forward: 0.08136153221130371, Backward: 0.0937507152557373, Optimizer: 0
Epoch 1 Batch 1096 Train Loss 0.021942032501101494
Total Times. Batch: 1096, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011382102966308594, Forward: 0.08139514923095703, Backward: 0.10328531265258789, Optimizer: 0
Epoch 1 Batch 1097 Train Loss 0.022305523976683617
Total Times. Batch: 1097, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011913776397705078, Forward: 0.08153533935546875, Backward: 0.10342788696289062, Optimizer: 0
Epoch 1 Batch 1098 Train Loss 0.02222408913075924
Total Times. Batch: 1098, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011553764343261719, Forward: 0.08145546913146973, Backward: 0.10316824913024902, Optimizer: 0
Epoch 1 Batch 1099 Train Loss 0.02217014692723751
Total Times. Batch: 1099, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011641979217529297, Forward: 0.0814354419708252, Backward: 0.10337686538696289, Optimizer: 0.1670839786529541
Epoch 1 Batch 1100 Train Loss 0.02072988450527191
Total Times. Batch: 1100, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011560916900634766, Forward: 0.0808556079864502, Backward: 0.09440231323242188, Optimizer: 0
Epoch 1 Batch 1101 Train Loss 0.02209635078907013
Total Times. Batch: 1101, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012402534484863281, Forward: 0.08212161064147949, Backward: 0.103515625, Optimizer: 0
Epoch 1 Batch 1102 Train Loss 0.022230910137295723
Total Times. Batch: 1102, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011372566223144531, Forward: 0.081573486328125, Backward: 0.10321927070617676, Optimizer: 0
Epoch 1 Batch 1103 Train Loss 0.022765779867768288
Total Times. Batch: 1103, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011911392211914062, Forward: 0.08191871643066406, Backward: 0.10322332382202148, Optimizer: 0
Epoch 1 Batch 1104 Train Loss 0.02171964757144451
Total Times. Batch: 1104, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011947154998779297, Forward: 0.08201813697814941, Backward: 0.10347771644592285, Optimizer: 0.16694378852844238
Epoch 1 Batch 1105 Train Loss 0.022423984482884407
Total Times. Batch: 1105, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011811256408691406, Forward: 0.08190488815307617, Backward: 0.09358596801757812, Optimizer: 0
Epoch 1 Batch 1106 Train Loss 0.019915437325835228
Total Times. Batch: 1106, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001291036605834961, Forward: 0.08171916007995605, Backward: 0.10333871841430664, Optimizer: 0
Epoch 1 Batch 1107 Train Loss 0.02029806189239025
Total Times. Batch: 1107, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011858940124511719, Forward: 0.08163261413574219, Backward: 0.10339951515197754, Optimizer: 0
Epoch 1 Batch 1108 Train Loss 0.022024743258953094
Total Times. Batch: 1108, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011599063873291016, Forward: 0.08142805099487305, Backward: 0.10328102111816406, Optimizer: 0
Epoch 1 Batch 1109 Train Loss 0.023017194122076035
Total Times. Batch: 1109, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012195110321044922, Forward: 0.08116745948791504, Backward: 0.1033930778503418, Optimizer: 0.16688132286071777
Epoch 1 Batch 1110 Train Loss 0.020282382145524025
Total Times. Batch: 1110, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011508464813232422, Forward: 0.08132100105285645, Backward: 0.09414792060852051, Optimizer: 0
Epoch 1 Batch 1111 Train Loss 0.021792052313685417
Total Times. Batch: 1111, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001245260238647461, Forward: 0.08140921592712402, Backward: 0.10351276397705078, Optimizer: 0
Epoch 1 Batch 1112 Train Loss 0.021797744557261467
Total Times. Batch: 1112, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011067390441894531, Forward: 0.08108067512512207, Backward: 0.10349631309509277, Optimizer: 0
Epoch 1 Batch 1113 Train Loss 0.020042112097144127
Total Times. Batch: 1113, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011751651763916016, Forward: 0.08132576942443848, Backward: 0.10322356224060059, Optimizer: 0
Epoch 1 Batch 1114 Train Loss 0.020973263308405876
Total Times. Batch: 1114, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011518001556396484, Forward: 0.0820460319519043, Backward: 0.10342001914978027, Optimizer: 0.16681456565856934
Epoch 1 Batch 1115 Train Loss 0.021224843338131905
Total Times. Batch: 1115, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011882781982421875, Forward: 0.08214664459228516, Backward: 0.09358739852905273, Optimizer: 0
Epoch 1 Batch 1116 Train Loss 0.020573914051055908
Total Times. Batch: 1116, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013360977172851562, Forward: 0.08197760581970215, Backward: 0.10324573516845703, Optimizer: 0
Epoch 1 Batch 1117 Train Loss 0.02221420407295227
Total Times. Batch: 1117, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012061595916748047, Forward: 0.08118677139282227, Backward: 0.10332965850830078, Optimizer: 0
Epoch 1 Batch 1118 Train Loss 0.02138056792318821
Total Times. Batch: 1118, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001154184341430664, Forward: 0.08103179931640625, Backward: 0.10323619842529297, Optimizer: 0
Epoch 1 Batch 1119 Train Loss 0.02109414152801037
Total Times. Batch: 1119, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011153221130371094, Forward: 0.08148384094238281, Backward: 0.10361647605895996, Optimizer: 0.16687798500061035
Epoch 1 Batch 1120 Train Loss 0.02244163304567337
Total Times. Batch: 1120, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011556148529052734, Forward: 0.08213496208190918, Backward: 0.09373736381530762, Optimizer: 0
Epoch 1 Batch 1121 Train Loss 0.021520113572478294
Total Times. Batch: 1121, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0014579296112060547, Forward: 0.08139181137084961, Backward: 0.10334277153015137, Optimizer: 0
Epoch 1 Batch 1122 Train Loss 0.021114205941557884
Total Times. Batch: 1122, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011928081512451172, Forward: 0.0813603401184082, Backward: 0.10338449478149414, Optimizer: 0
Epoch 1 Batch 1123 Train Loss 0.02147631347179413
Total Times. Batch: 1123, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012502670288085938, Forward: 0.08157157897949219, Backward: 0.10353398323059082, Optimizer: 0
Epoch 1 Batch 1124 Train Loss 0.021543357521295547
Total Times. Batch: 1124, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011682510375976562, Forward: 0.08110785484313965, Backward: 0.10328531265258789, Optimizer: 0.1670687198638916
Epoch 1 Batch 1125 Train Loss 0.021030403673648834
Total Times. Batch: 1125, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001375436782836914, Forward: 0.08191990852355957, Backward: 0.0934896469116211, Optimizer: 0
Epoch 1 Batch 1126 Train Loss 0.021087704226374626
Total Times. Batch: 1126, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011515617370605469, Forward: 0.08220243453979492, Backward: 0.10331845283508301, Optimizer: 0
Epoch 1 Batch 1127 Train Loss 0.021501285955309868
Total Times. Batch: 1127, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.00142669677734375, Forward: 0.08154416084289551, Backward: 0.1032857894897461, Optimizer: 0
Epoch 1 Batch 1128 Train Loss 0.020781762897968292
Total Times. Batch: 1128, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012242794036865234, Forward: 0.08133172988891602, Backward: 0.1034085750579834, Optimizer: 0
Epoch 1 Batch 1129 Train Loss 0.020742788910865784
Total Times. Batch: 1129, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012259483337402344, Forward: 0.08121919631958008, Backward: 0.10338139533996582, Optimizer: 0.16714787483215332
Epoch 1 Batch 1130 Train Loss 0.020805930718779564
Total Times. Batch: 1130, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001178741455078125, Forward: 0.08102798461914062, Backward: 0.0940709114074707, Optimizer: 0
Epoch 1 Batch 1131 Train Loss 0.020720763131976128
Total Times. Batch: 1131, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012500286102294922, Forward: 0.08125090599060059, Backward: 0.10357356071472168, Optimizer: 0
Epoch 1 Batch 1132 Train Loss 0.020568016916513443
Total Times. Batch: 1132, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001155853271484375, Forward: 0.08191895484924316, Backward: 0.10311651229858398, Optimizer: 0
Epoch 1 Batch 1133 Train Loss 0.019194623455405235
Total Times. Batch: 1133, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0014069080352783203, Forward: 0.08141422271728516, Backward: 0.10321259498596191, Optimizer: 0
Epoch 1 Batch 1134 Train Loss 0.020806333050131798
Total Times. Batch: 1134, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001195669174194336, Forward: 0.08165192604064941, Backward: 0.10293054580688477, Optimizer: 0.1670067310333252
Epoch 1 Batch 1135 Train Loss 0.02069271169602871
Total Times. Batch: 1135, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001173257827758789, Forward: 0.08144235610961914, Backward: 0.09411168098449707, Optimizer: 0
Epoch 1 Batch 1136 Train Loss 0.020236952230334282
Total Times. Batch: 1136, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013041496276855469, Forward: 0.08139896392822266, Backward: 0.1034092903137207, Optimizer: 0
Epoch 1 Batch 1137 Train Loss 0.020037440583109856
Total Times. Batch: 1137, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001210927963256836, Forward: 0.08130621910095215, Backward: 0.10351729393005371, Optimizer: 0
Epoch 1 Batch 1138 Train Loss 0.019929183647036552
Total Times. Batch: 1138, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012023448944091797, Forward: 0.0816044807434082, Backward: 0.10313558578491211, Optimizer: 0
Epoch 1 Batch 1139 Train Loss 0.020148148760199547
Total Times. Batch: 1139, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001234292984008789, Forward: 0.08159232139587402, Backward: 0.1031332015991211, Optimizer: 0.16684556007385254
Epoch 1 Batch 1140 Train Loss 0.021229712292551994
Total Times. Batch: 1140, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011820793151855469, Forward: 0.0813896656036377, Backward: 0.09380459785461426, Optimizer: 0
Epoch 1 Batch 1141 Train Loss 0.021117044612765312
Total Times. Batch: 1141, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001188039779663086, Forward: 0.08127450942993164, Backward: 0.10338187217712402, Optimizer: 0
Epoch 1 Batch 1142 Train Loss 0.020475979894399643
Total Times. Batch: 1142, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011670589447021484, Forward: 0.08129024505615234, Backward: 0.10327935218811035, Optimizer: 0
Epoch 1 Batch 1143 Train Loss 0.01958480291068554
Total Times. Batch: 1143, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011799335479736328, Forward: 0.08108949661254883, Backward: 0.10373330116271973, Optimizer: 0
Epoch 1 Batch 1144 Train Loss 0.020030317828059196
Total Times. Batch: 1144, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001163482666015625, Forward: 0.08142924308776855, Backward: 0.1035304069519043, Optimizer: 0.16646742820739746
Epoch 1 Batch 1145 Train Loss 0.020183874294161797
Total Times. Batch: 1145, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001123189926147461, Forward: 0.08173346519470215, Backward: 0.09363985061645508, Optimizer: 0
Epoch 1 Batch 1146 Train Loss 0.020545655861496925
Total Times. Batch: 1146, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013086795806884766, Forward: 0.08161211013793945, Backward: 0.10317635536193848, Optimizer: 0
Epoch 1 Batch 1147 Train Loss 0.01876164972782135
Total Times. Batch: 1147, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011835098266601562, Forward: 0.08118796348571777, Backward: 0.10362386703491211, Optimizer: 0
Epoch 1 Batch 1148 Train Loss 0.020710082724690437
Total Times. Batch: 1148, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011167526245117188, Forward: 0.0810236930847168, Backward: 0.10335612297058105, Optimizer: 0
Epoch 1 Batch 1149 Train Loss 0.01950269751250744
Total Times. Batch: 1149, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011005401611328125, Forward: 0.08131623268127441, Backward: 0.10359573364257812, Optimizer: 0.16674256324768066
Epoch 1 Batch 1150 Train Loss 0.02042490988969803
Total Times. Batch: 1150, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010921955108642578, Forward: 0.08168530464172363, Backward: 0.09397363662719727, Optimizer: 0
Epoch 1 Batch 1151 Train Loss 0.020466599613428116
Total Times. Batch: 1151, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013470649719238281, Forward: 0.08200860023498535, Backward: 0.1029963493347168, Optimizer: 0
Epoch 1 Batch 1152 Train Loss 0.020304441452026367
Total Times. Batch: 1152, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011966228485107422, Forward: 0.0815577507019043, Backward: 0.10344409942626953, Optimizer: 0
Epoch 1 Batch 1153 Train Loss 0.019708380103111267
Total Times. Batch: 1153, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011909008026123047, Forward: 0.08147406578063965, Backward: 0.10347795486450195, Optimizer: 0
Epoch 1 Batch 1154 Train Loss 0.020541921257972717
Total Times. Batch: 1154, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011074542999267578, Forward: 0.08125066757202148, Backward: 0.10352444648742676, Optimizer: 0.16703057289123535
Epoch 1 Batch 1155 Train Loss 0.019866259768605232
Total Times. Batch: 1155, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011456012725830078, Forward: 0.08092331886291504, Backward: 0.09404730796813965, Optimizer: 0
Epoch 1 Batch 1156 Train Loss 0.020414261147379875
Total Times. Batch: 1156, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012750625610351562, Forward: 0.08252072334289551, Backward: 0.1029820442199707, Optimizer: 0
Epoch 1 Batch 1157 Train Loss 0.02024865336716175
Total Times. Batch: 1157, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012543201446533203, Forward: 0.08169913291931152, Backward: 0.10309672355651855, Optimizer: 0
Epoch 1 Batch 1158 Train Loss 0.020420432090759277
Total Times. Batch: 1158, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011899471282958984, Forward: 0.08162593841552734, Backward: 0.10339021682739258, Optimizer: 0
Epoch 1 Batch 1159 Train Loss 0.019660508260130882
Total Times. Batch: 1159, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001196146011352539, Forward: 0.08134198188781738, Backward: 0.1032249927520752, Optimizer: 0.16727256774902344
Epoch 1 Batch 1160 Train Loss 0.02068515121936798
Total Times. Batch: 1160, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001186370849609375, Forward: 0.08126235008239746, Backward: 0.09405827522277832, Optimizer: 0
Epoch 1 Batch 1161 Train Loss 0.019704878330230713
Total Times. Batch: 1161, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012421607971191406, Forward: 0.08147382736206055, Backward: 0.10315275192260742, Optimizer: 0
Epoch 1 Batch 1162 Train Loss 0.019755734130740166
Total Times. Batch: 1162, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011487007141113281, Forward: 0.08159446716308594, Backward: 0.10318541526794434, Optimizer: 0
Epoch 1 Batch 1163 Train Loss 0.020745355635881424
Total Times. Batch: 1163, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012214183807373047, Forward: 0.08176493644714355, Backward: 0.10335588455200195, Optimizer: 0
Epoch 1 Batch 1164 Train Loss 0.019831998273730278
Total Times. Batch: 1164, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001188039779663086, Forward: 0.0817570686340332, Backward: 0.10348033905029297, Optimizer: 0.166900634765625
Epoch 1 Batch 1165 Train Loss 0.019971800968050957
Total Times. Batch: 1165, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011289119720458984, Forward: 0.08125019073486328, Backward: 0.09396648406982422, Optimizer: 0
Epoch 1 Batch 1166 Train Loss 0.01925850659608841
Total Times. Batch: 1166, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013642311096191406, Forward: 0.08132505416870117, Backward: 0.10351753234863281, Optimizer: 0
Epoch 1 Batch 1167 Train Loss 0.019933873787522316
Total Times. Batch: 1167, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011334419250488281, Forward: 0.08122754096984863, Backward: 0.10344648361206055, Optimizer: 0
Epoch 1 Batch 1168 Train Loss 0.020366599783301353
Total Times. Batch: 1168, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011451244354248047, Forward: 0.08178520202636719, Backward: 0.10342192649841309, Optimizer: 0
Epoch 1 Batch 1169 Train Loss 0.02019691839814186
Total Times. Batch: 1169, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013010501861572266, Forward: 0.08173346519470215, Backward: 0.10267066955566406, Optimizer: 0.167525053024292
Epoch 1 Batch 1170 Train Loss 0.020128412172198296
Total Times. Batch: 1170, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011584758758544922, Forward: 0.0816352367401123, Backward: 0.09392023086547852, Optimizer: 0
Epoch 1 Batch 1171 Train Loss 0.01982702687382698
Total Times. Batch: 1171, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011947154998779297, Forward: 0.08228611946105957, Backward: 0.10339760780334473, Optimizer: 0
Epoch 1 Batch 1172 Train Loss 0.02067667432129383
Total Times. Batch: 1172, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001184225082397461, Forward: 0.0810706615447998, Backward: 0.10344529151916504, Optimizer: 0
Epoch 1 Batch 1173 Train Loss 0.020733652636408806
Total Times. Batch: 1173, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011603832244873047, Forward: 0.08175969123840332, Backward: 0.1036522388458252, Optimizer: 0
Epoch 1 Batch 1174 Train Loss 0.020090805366635323
Total Times. Batch: 1174, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001142740249633789, Forward: 0.08132457733154297, Backward: 0.10357284545898438, Optimizer: 0.16694855690002441
Epoch 1 Batch 1175 Train Loss 0.021107150241732597
Total Times. Batch: 1175, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011701583862304688, Forward: 0.08159971237182617, Backward: 0.09328699111938477, Optimizer: 0
Epoch 1 Batch 1176 Train Loss 0.01957019604742527
Total Times. Batch: 1176, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011014938354492188, Forward: 0.08185768127441406, Backward: 0.10312366485595703, Optimizer: 0
Epoch 1 Batch 1177 Train Loss 0.02017996646463871
Total Times. Batch: 1177, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012040138244628906, Forward: 0.08199930191040039, Backward: 0.10354185104370117, Optimizer: 0
Epoch 1 Batch 1178 Train Loss 0.020667972043156624
Total Times. Batch: 1178, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001169443130493164, Forward: 0.0810554027557373, Backward: 0.10368704795837402, Optimizer: 0
Epoch 1 Batch 1179 Train Loss 0.0193785447627306
Total Times. Batch: 1179, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011591911315917969, Forward: 0.08190011978149414, Backward: 0.1031031608581543, Optimizer: 0.16682004928588867
Epoch 1 Batch 1180 Train Loss 0.02030283212661743
Total Times. Batch: 1180, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011417865753173828, Forward: 0.08181071281433105, Backward: 0.09389328956604004, Optimizer: 0
Epoch 1 Batch 1181 Train Loss 0.020114649087190628
Total Times. Batch: 1181, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001291036605834961, Forward: 0.08113741874694824, Backward: 0.10346102714538574, Optimizer: 0
Epoch 1 Batch 1182 Train Loss 0.021248329430818558
Total Times. Batch: 1182, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011010169982910156, Forward: 0.08127880096435547, Backward: 0.10311698913574219, Optimizer: 0
Epoch 1 Batch 1183 Train Loss 0.020813392475247383
Total Times. Batch: 1183, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012912750244140625, Forward: 0.08168840408325195, Backward: 0.1035459041595459, Optimizer: 0
Epoch 1 Batch 1184 Train Loss 0.020586533471941948
Total Times. Batch: 1184, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011134147644042969, Forward: 0.08100652694702148, Backward: 0.10347723960876465, Optimizer: 0.16740918159484863
Epoch 1 Batch 1185 Train Loss 0.019625255838036537
Total Times. Batch: 1185, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011608600616455078, Forward: 0.08111023902893066, Backward: 0.09394717216491699, Optimizer: 0
Epoch 1 Batch 1186 Train Loss 0.01956634223461151
Total Times. Batch: 1186, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012600421905517578, Forward: 0.08133268356323242, Backward: 0.10339045524597168, Optimizer: 0
Epoch 1 Batch 1187 Train Loss 0.01972569152712822
Total Times. Batch: 1187, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001249074935913086, Forward: 0.08237528800964355, Backward: 0.10326790809631348, Optimizer: 0
Epoch 1 Batch 1188 Train Loss 0.021386638283729553
Total Times. Batch: 1188, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012252330780029297, Forward: 0.08129358291625977, Backward: 0.10329580307006836, Optimizer: 0
Epoch 1 Batch 1189 Train Loss 0.020558830350637436
Total Times. Batch: 1189, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001192331314086914, Forward: 0.0814979076385498, Backward: 0.10342144966125488, Optimizer: 0.16757655143737793
Epoch 1 Batch 1190 Train Loss 0.019227785989642143
Total Times. Batch: 1190, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011954307556152344, Forward: 0.08120393753051758, Backward: 0.09413003921508789, Optimizer: 0
Epoch 1 Batch 1191 Train Loss 0.02052856609225273
Total Times. Batch: 1191, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011153221130371094, Forward: 0.08216023445129395, Backward: 0.10341739654541016, Optimizer: 0
Epoch 1 Batch 1192 Train Loss 0.019313057884573936
Total Times. Batch: 1192, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010955333709716797, Forward: 0.08248591423034668, Backward: 0.10312914848327637, Optimizer: 0
Epoch 1 Batch 1193 Train Loss 0.019394101575016975
Total Times. Batch: 1193, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011882781982421875, Forward: 0.0813291072845459, Backward: 0.1034090518951416, Optimizer: 0
Epoch 1 Batch 1194 Train Loss 0.019019005820155144
Total Times. Batch: 1194, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012736320495605469, Forward: 0.08152484893798828, Backward: 0.10322380065917969, Optimizer: 0.16747164726257324
Epoch 1 Batch 1195 Train Loss 0.019308745861053467
Total Times. Batch: 1195, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011632442474365234, Forward: 0.08188557624816895, Backward: 0.09416747093200684, Optimizer: 0
Epoch 1 Batch 1196 Train Loss 0.01908961497247219
Total Times. Batch: 1196, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001294851303100586, Forward: 0.08121776580810547, Backward: 0.10356497764587402, Optimizer: 0
Epoch 1 Batch 1197 Train Loss 0.019546156749129295
Total Times. Batch: 1197, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001161336898803711, Forward: 0.08109140396118164, Backward: 0.10323405265808105, Optimizer: 0
Epoch 1 Batch 1198 Train Loss 0.01966838352382183
Total Times. Batch: 1198, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011391639709472656, Forward: 0.08131718635559082, Backward: 0.10345578193664551, Optimizer: 0
Epoch 1 Batch 1199 Train Loss 0.02009667456150055
Total Times. Batch: 1199, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012967586517333984, Forward: 0.08176422119140625, Backward: 0.10328173637390137, Optimizer: 0.16705036163330078
Epoch 1 Batch 1200 Train Loss 0.020084775984287262
Total Times. Batch: 1200, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011708736419677734, Forward: 0.08149075508117676, Backward: 0.09375429153442383, Optimizer: 0
Epoch 1 Batch 1201 Train Loss 0.01929801143705845
Total Times. Batch: 1201, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001285552978515625, Forward: 0.08112621307373047, Backward: 0.10344958305358887, Optimizer: 0
Epoch 1 Batch 1202 Train Loss 0.019163763150572777
Total Times. Batch: 1202, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001094818115234375, Forward: 0.08212924003601074, Backward: 0.10352706909179688, Optimizer: 0
Epoch 1 Batch 1203 Train Loss 0.018717512488365173
Total Times. Batch: 1203, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011665821075439453, Forward: 0.0813286304473877, Backward: 0.10327339172363281, Optimizer: 0
Epoch 1 Batch 1204 Train Loss 0.019401153549551964
Total Times. Batch: 1204, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011594295501708984, Forward: 0.08224081993103027, Backward: 0.10284042358398438, Optimizer: 0.16748666763305664
Epoch 1 Batch 1205 Train Loss 0.01920701377093792
Total Times. Batch: 1205, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011920928955078125, Forward: 0.08168840408325195, Backward: 0.09372067451477051, Optimizer: 0
Epoch 1 Batch 1206 Train Loss 0.01941479556262493
Total Times. Batch: 1206, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012984275817871094, Forward: 0.08126568794250488, Backward: 0.10341548919677734, Optimizer: 0
Epoch 1 Batch 1207 Train Loss 0.018551472574472427
Total Times. Batch: 1207, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011789798736572266, Forward: 0.08142304420471191, Backward: 0.10330772399902344, Optimizer: 0
Epoch 1 Batch 1208 Train Loss 0.01996648870408535
Total Times. Batch: 1208, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011515617370605469, Forward: 0.08119344711303711, Backward: 0.10360026359558105, Optimizer: 0
Epoch 1 Batch 1209 Train Loss 0.019621893763542175
Total Times. Batch: 1209, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001161813735961914, Forward: 0.08200883865356445, Backward: 0.10307955741882324, Optimizer: 0.1668071746826172
Epoch 1 Batch 1210 Train Loss 0.020110221579670906
Total Times. Batch: 1210, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001142263412475586, Forward: 0.0816650390625, Backward: 0.0936741828918457, Optimizer: 0
Epoch 1 Batch 1211 Train Loss 0.01906995289027691
Total Times. Batch: 1211, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013608932495117188, Forward: 0.08153653144836426, Backward: 0.10318803787231445, Optimizer: 0
Epoch 1 Batch 1212 Train Loss 0.018328147009015083
Total Times. Batch: 1212, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011935234069824219, Forward: 0.08142614364624023, Backward: 0.10344147682189941, Optimizer: 0
Epoch 1 Batch 1213 Train Loss 0.018809709697961807
Total Times. Batch: 1213, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012235641479492188, Forward: 0.08116817474365234, Backward: 0.1036372184753418, Optimizer: 0
Epoch 1 Batch 1214 Train Loss 0.019208310171961784
Total Times. Batch: 1214, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011487007141113281, Forward: 0.0812382698059082, Backward: 0.10396027565002441, Optimizer: 0.1665802001953125
Epoch 1 Batch 1215 Train Loss 0.018078071996569633
Total Times. Batch: 1215, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011436939239501953, Forward: 0.0818643569946289, Backward: 0.09410572052001953, Optimizer: 0
Epoch 1 Batch 1216 Train Loss 0.01877225935459137
Total Times. Batch: 1216, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001257181167602539, Forward: 0.08125472068786621, Backward: 0.10291266441345215, Optimizer: 0
Epoch 1 Batch 1217 Train Loss 0.01902042329311371
Total Times. Batch: 1217, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013325214385986328, Forward: 0.08168625831604004, Backward: 0.10287785530090332, Optimizer: 0
Epoch 1 Batch 1218 Train Loss 0.018637603148818016
Total Times. Batch: 1218, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013117790222167969, Forward: 0.08184075355529785, Backward: 0.10351228713989258, Optimizer: 0
Epoch 1 Batch 1219 Train Loss 0.019850607961416245
Total Times. Batch: 1219, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011904239654541016, Forward: 0.08182096481323242, Backward: 0.1036231517791748, Optimizer: 0.1682271957397461
Epoch 1 Batch 1220 Train Loss 0.01855221576988697
Total Times. Batch: 1220, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011675357818603516, Forward: 0.0812082290649414, Backward: 0.09368658065795898, Optimizer: 0
Epoch 1 Batch 1221 Train Loss 0.01836424134671688
Total Times. Batch: 1221, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011539459228515625, Forward: 0.08145475387573242, Backward: 0.10336875915527344, Optimizer: 0
Epoch 1 Batch 1222 Train Loss 0.018111083656549454
Total Times. Batch: 1222, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011603832244873047, Forward: 0.08124732971191406, Backward: 0.10343384742736816, Optimizer: 0
Epoch 1 Batch 1223 Train Loss 0.018389515578746796
Total Times. Batch: 1223, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001210927963256836, Forward: 0.08226251602172852, Backward: 0.10314607620239258, Optimizer: 0
Epoch 1 Batch 1224 Train Loss 0.018856775015592575
Total Times. Batch: 1224, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011386871337890625, Forward: 0.08256125450134277, Backward: 0.10284924507141113, Optimizer: 0.16670870780944824
Epoch 1 Batch 1225 Train Loss 0.019474759697914124
Total Times. Batch: 1225, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011785030364990234, Forward: 0.08140826225280762, Backward: 0.09365391731262207, Optimizer: 0
Epoch 1 Batch 1226 Train Loss 0.01967763341963291
Total Times. Batch: 1226, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011096000671386719, Forward: 0.08109283447265625, Backward: 0.10369634628295898, Optimizer: 0
Epoch 1 Batch 1227 Train Loss 0.01898401789367199
Total Times. Batch: 1227, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011522769927978516, Forward: 0.08104777336120605, Backward: 0.10389161109924316, Optimizer: 0
Epoch 1 Batch 1228 Train Loss 0.01852836087346077
Total Times. Batch: 1228, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011518001556396484, Forward: 0.08176779747009277, Backward: 0.1030721664428711, Optimizer: 0
Epoch 1 Batch 1229 Train Loss 0.018696721643209457
Total Times. Batch: 1229, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012598037719726562, Forward: 0.08164715766906738, Backward: 0.1033172607421875, Optimizer: 0.16750860214233398
Epoch 1 Batch 1230 Train Loss 0.01849256455898285
Total Times. Batch: 1230, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011627674102783203, Forward: 0.08228564262390137, Backward: 0.09372520446777344, Optimizer: 0
Epoch 1 Batch 1231 Train Loss 0.018305785953998566
Total Times. Batch: 1231, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012938976287841797, Forward: 0.08114409446716309, Backward: 0.10362672805786133, Optimizer: 0
Epoch 1 Batch 1232 Train Loss 0.018926845863461494
Total Times. Batch: 1232, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010976791381835938, Forward: 0.08177542686462402, Backward: 0.10385823249816895, Optimizer: 0
Epoch 1 Batch 1233 Train Loss 0.0184981357306242
Total Times. Batch: 1233, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012121200561523438, Forward: 0.08114767074584961, Backward: 0.10362744331359863, Optimizer: 0
Epoch 1 Batch 1234 Train Loss 0.018458256497979164
Total Times. Batch: 1234, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011386871337890625, Forward: 0.08185958862304688, Backward: 0.10309576988220215, Optimizer: 0.1676158905029297
Epoch 1 Batch 1235 Train Loss 0.018398739397525787
Total Times. Batch: 1235, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011982917785644531, Forward: 0.08182406425476074, Backward: 0.09349226951599121, Optimizer: 0
Epoch 1 Batch 1236 Train Loss 0.018787657842040062
Total Times. Batch: 1236, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001298666000366211, Forward: 0.08152556419372559, Backward: 0.10337448120117188, Optimizer: 0
Epoch 1 Batch 1237 Train Loss 0.018580926582217216
Total Times. Batch: 1237, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013782978057861328, Forward: 0.08108782768249512, Backward: 0.10342955589294434, Optimizer: 0
Epoch 1 Batch 1238 Train Loss 0.01893875002861023
Total Times. Batch: 1238, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001178741455078125, Forward: 0.08235740661621094, Backward: 0.10343790054321289, Optimizer: 0
Epoch 1 Batch 1239 Train Loss 0.017914189025759697
Total Times. Batch: 1239, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001100301742553711, Forward: 0.0816493034362793, Backward: 0.10372710227966309, Optimizer: 0.16637468338012695
Epoch 1 Batch 1240 Train Loss 0.01908903382718563
Total Times. Batch: 1240, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001129150390625, Forward: 0.08224177360534668, Backward: 0.09394383430480957, Optimizer: 0
Epoch 1 Batch 1241 Train Loss 0.01858087256550789
Total Times. Batch: 1241, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001299142837524414, Forward: 0.08163619041442871, Backward: 0.10311508178710938, Optimizer: 0
Epoch 1 Batch 1242 Train Loss 0.01829957775771618
Total Times. Batch: 1242, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012063980102539062, Forward: 0.0812382698059082, Backward: 0.10343170166015625, Optimizer: 0
Epoch 1 Batch 1243 Train Loss 0.018421432003378868
Total Times. Batch: 1243, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011773109436035156, Forward: 0.08110475540161133, Backward: 0.10355591773986816, Optimizer: 0
Epoch 1 Batch 1244 Train Loss 0.018640270456671715
Total Times. Batch: 1244, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011670589447021484, Forward: 0.08182287216186523, Backward: 0.10337352752685547, Optimizer: 0.1670067310333252
Epoch 1 Batch 1245 Train Loss 0.01843697391450405
Total Times. Batch: 1245, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001157522201538086, Forward: 0.08112001419067383, Backward: 0.09368777275085449, Optimizer: 0
Epoch 1 Batch 1246 Train Loss 0.017578477039933205
Total Times. Batch: 1246, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012409687042236328, Forward: 0.08130025863647461, Backward: 0.10353827476501465, Optimizer: 0
Epoch 1 Batch 1247 Train Loss 0.018730301409959793
Total Times. Batch: 1247, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012094974517822266, Forward: 0.08137798309326172, Backward: 0.10352110862731934, Optimizer: 0
Epoch 1 Batch 1248 Train Loss 0.01844434253871441
Total Times. Batch: 1248, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011591911315917969, Forward: 0.08211755752563477, Backward: 0.10307836532592773, Optimizer: 0
Epoch 1 Batch 1249 Train Loss 0.017867939546704292
Total Times. Batch: 1249, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001470804214477539, Forward: 0.08167743682861328, Backward: 0.10307669639587402, Optimizer: 0.16708898544311523
Epoch 1 Batch 1250 Train Loss 0.01812777854502201
Total Times. Batch: 1250, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011746883392333984, Forward: 0.08114361763000488, Backward: 0.0941152572631836, Optimizer: 0
Epoch 1 Batch 1251 Train Loss 0.018721917644143105
Total Times. Batch: 1251, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012483596801757812, Forward: 0.08127546310424805, Backward: 0.10335063934326172, Optimizer: 0
Epoch 1 Batch 1252 Train Loss 0.01780218444764614
Total Times. Batch: 1252, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011374950408935547, Forward: 0.08129501342773438, Backward: 0.10329890251159668, Optimizer: 0
Epoch 1 Batch 1253 Train Loss 0.01878020539879799
Total Times. Batch: 1253, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011935234069824219, Forward: 0.08224034309387207, Backward: 0.10315752029418945, Optimizer: 0
Epoch 1 Batch 1254 Train Loss 0.018724750727415085
Total Times. Batch: 1254, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011951923370361328, Forward: 0.08158040046691895, Backward: 0.10342836380004883, Optimizer: 0.1681196689605713
Epoch 1 Batch 1255 Train Loss 0.018536265939474106
Total Times. Batch: 1255, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013675689697265625, Forward: 0.08124256134033203, Backward: 0.09409356117248535, Optimizer: 0
Epoch 1 Batch 1256 Train Loss 0.018467236310243607
Total Times. Batch: 1256, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013070106506347656, Forward: 0.0812833309173584, Backward: 0.1033470630645752, Optimizer: 0
Epoch 1 Batch 1257 Train Loss 0.017423804849386215
Total Times. Batch: 1257, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011050701141357422, Forward: 0.08170080184936523, Backward: 0.10323143005371094, Optimizer: 0
Epoch 1 Batch 1258 Train Loss 0.018473006784915924
Total Times. Batch: 1258, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011508464813232422, Forward: 0.08141160011291504, Backward: 0.10321831703186035, Optimizer: 0
Epoch 1 Batch 1259 Train Loss 0.01851608417928219
Total Times. Batch: 1259, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011267662048339844, Forward: 0.08173298835754395, Backward: 0.10314106941223145, Optimizer: 0.16701221466064453
Epoch 1 Batch 1260 Train Loss 0.018646225333213806
Total Times. Batch: 1260, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011632442474365234, Forward: 0.081573486328125, Backward: 0.09374332427978516, Optimizer: 0
Epoch 1 Batch 1261 Train Loss 0.017441295087337494
Total Times. Batch: 1261, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013010501861572266, Forward: 0.0818021297454834, Backward: 0.1033480167388916, Optimizer: 0
Epoch 1 Batch 1262 Train Loss 0.01778988540172577
Total Times. Batch: 1262, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.00116729736328125, Forward: 0.08101963996887207, Backward: 0.1032862663269043, Optimizer: 0
Epoch 1 Batch 1263 Train Loss 0.017975876107811928
Total Times. Batch: 1263, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010993480682373047, Forward: 0.08242535591125488, Backward: 0.10327816009521484, Optimizer: 0
Epoch 1 Batch 1264 Train Loss 0.018860606476664543
Total Times. Batch: 1264, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011572837829589844, Forward: 0.08134150505065918, Backward: 0.10337662696838379, Optimizer: 0.16713285446166992
Epoch 1 Batch 1265 Train Loss 0.017662668600678444
Total Times. Batch: 1265, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012164115905761719, Forward: 0.08144402503967285, Backward: 0.09391379356384277, Optimizer: 0
Epoch 1 Batch 1266 Train Loss 0.01795460283756256
Total Times. Batch: 1266, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013201236724853516, Forward: 0.08183860778808594, Backward: 0.10288214683532715, Optimizer: 0
Epoch 1 Batch 1267 Train Loss 0.01875573955476284
Total Times. Batch: 1267, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011868476867675781, Forward: 0.08126211166381836, Backward: 0.10336422920227051, Optimizer: 0
Epoch 1 Batch 1268 Train Loss 0.01786618120968342
Total Times. Batch: 1268, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011737346649169922, Forward: 0.0810997486114502, Backward: 0.10324645042419434, Optimizer: 0
Epoch 1 Batch 1269 Train Loss 0.018551483750343323
Total Times. Batch: 1269, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010976791381835938, Forward: 0.08139967918395996, Backward: 0.10336184501647949, Optimizer: 0.16718626022338867
Epoch 1 Batch 1270 Train Loss 0.018565846607089043
Total Times. Batch: 1270, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011677742004394531, Forward: 0.08156800270080566, Backward: 0.0936136245727539, Optimizer: 0
Epoch 1 Batch 1271 Train Loss 0.01788456365466118
Total Times. Batch: 1271, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011355876922607422, Forward: 0.08122587203979492, Backward: 0.10285568237304688, Optimizer: 0
Epoch 1 Batch 1272 Train Loss 0.017463132739067078
Total Times. Batch: 1272, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011620521545410156, Forward: 0.08174562454223633, Backward: 0.10319852828979492, Optimizer: 0
Epoch 1 Batch 1273 Train Loss 0.018141791224479675
Total Times. Batch: 1273, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011887550354003906, Forward: 0.08131551742553711, Backward: 0.10361528396606445, Optimizer: 0
Epoch 1 Batch 1274 Train Loss 0.01829046942293644
Total Times. Batch: 1274, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011661052703857422, Forward: 0.08122539520263672, Backward: 0.10367989540100098, Optimizer: 0.16692686080932617
Epoch 1 Batch 1275 Train Loss 0.018493620678782463
Total Times. Batch: 1275, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011701583862304688, Forward: 0.08095312118530273, Backward: 0.09395551681518555, Optimizer: 0
Epoch 1 Batch 1276 Train Loss 0.017887044697999954
Total Times. Batch: 1276, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011429786682128906, Forward: 0.08162140846252441, Backward: 0.1031351089477539, Optimizer: 0
Epoch 1 Batch 1277 Train Loss 0.01809626817703247
Total Times. Batch: 1277, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012047290802001953, Forward: 0.0822145938873291, Backward: 0.10289287567138672, Optimizer: 0
Epoch 1 Batch 1278 Train Loss 0.01874946989119053
Total Times. Batch: 1278, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012290477752685547, Forward: 0.08148407936096191, Backward: 0.10338425636291504, Optimizer: 0
Epoch 1 Batch 1279 Train Loss 0.01786426268517971
Total Times. Batch: 1279, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011935234069824219, Forward: 0.08167839050292969, Backward: 0.10310935974121094, Optimizer: 0.16671347618103027
Epoch 1 Batch 1280 Train Loss 0.018671885132789612
Total Times. Batch: 1280, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013287067413330078, Forward: 0.08092474937438965, Backward: 0.09436392784118652, Optimizer: 0
Epoch 1 Batch 1281 Train Loss 0.017983822152018547
Total Times. Batch: 1281, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001271963119506836, Forward: 0.08128809928894043, Backward: 0.10332846641540527, Optimizer: 0
Epoch 1 Batch 1282 Train Loss 0.018472755327820778
Total Times. Batch: 1282, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011773109436035156, Forward: 0.08128976821899414, Backward: 0.10340619087219238, Optimizer: 0
Epoch 1 Batch 1283 Train Loss 0.018195826560258865
Total Times. Batch: 1283, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011870861053466797, Forward: 0.0820915699005127, Backward: 0.10314607620239258, Optimizer: 0
Epoch 1 Batch 1284 Train Loss 0.018909776583313942
Total Times. Batch: 1284, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011484622955322266, Forward: 0.08186221122741699, Backward: 0.10328173637390137, Optimizer: 0.16716670989990234
Epoch 1 Batch 1285 Train Loss 0.017548521980643272
Total Times. Batch: 1285, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011858940124511719, Forward: 0.08232235908508301, Backward: 0.09334325790405273, Optimizer: 0
Epoch 1 Batch 1286 Train Loss 0.01836531236767769
Total Times. Batch: 1286, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012936592102050781, Forward: 0.08142328262329102, Backward: 0.10341238975524902, Optimizer: 0
Epoch 1 Batch 1287 Train Loss 0.017510680481791496
Total Times. Batch: 1287, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011627674102783203, Forward: 0.0812690258026123, Backward: 0.10345792770385742, Optimizer: 0
Epoch 1 Batch 1288 Train Loss 0.01798485592007637
Total Times. Batch: 1288, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011475086212158203, Forward: 0.08225417137145996, Backward: 0.10342526435852051, Optimizer: 0
Epoch 1 Batch 1289 Train Loss 0.019018307328224182
Total Times. Batch: 1289, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011746883392333984, Forward: 0.0823667049407959, Backward: 0.10344529151916504, Optimizer: 0.16764020919799805
Epoch 1 Batch 1290 Train Loss 0.0174524188041687
Total Times. Batch: 1290, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011661052703857422, Forward: 0.08172178268432617, Backward: 0.0941169261932373, Optimizer: 0
Epoch 1 Batch 1291 Train Loss 0.01832493208348751
Total Times. Batch: 1291, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011837482452392578, Forward: 0.0813133716583252, Backward: 0.10354399681091309, Optimizer: 0
Epoch 1 Batch 1292 Train Loss 0.018056506291031837
Total Times. Batch: 1292, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011548995971679688, Forward: 0.08123016357421875, Backward: 0.1034092903137207, Optimizer: 0
Epoch 1 Batch 1293 Train Loss 0.017406463623046875
Total Times. Batch: 1293, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001148223876953125, Forward: 0.08110833168029785, Backward: 0.10346364974975586, Optimizer: 0
Epoch 1 Batch 1294 Train Loss 0.01850980333983898
Total Times. Batch: 1294, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011458396911621094, Forward: 0.08123564720153809, Backward: 0.10343313217163086, Optimizer: 0.16726398468017578
Epoch 1 Batch 1295 Train Loss 0.017947478219866753
Total Times. Batch: 1295, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011744499206542969, Forward: 0.0821225643157959, Backward: 0.09374308586120605, Optimizer: 0
Epoch 1 Batch 1296 Train Loss 0.018313543871045113
Total Times. Batch: 1296, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013580322265625, Forward: 0.08157515525817871, Backward: 0.10354280471801758, Optimizer: 0
Epoch 1 Batch 1297 Train Loss 0.017467660829424858
Total Times. Batch: 1297, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012009143829345703, Forward: 0.08158183097839355, Backward: 0.10288476943969727, Optimizer: 0
Epoch 1 Batch 1298 Train Loss 0.018229974433779716
Total Times. Batch: 1298, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011756420135498047, Forward: 0.0812997817993164, Backward: 0.10341882705688477, Optimizer: 0
Epoch 1 Batch 1299 Train Loss 0.01736823283135891
Total Times. Batch: 1299, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011749267578125, Forward: 0.08135247230529785, Backward: 0.10331511497497559, Optimizer: 0.16731500625610352
Epoch 1 Batch 1300 Train Loss 0.017199715599417686
Total Times. Batch: 1300, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011653900146484375, Forward: 0.0821220874786377, Backward: 0.09404659271240234, Optimizer: 0
Epoch 1 Batch 1301 Train Loss 0.017638465389609337
Total Times. Batch: 1301, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001336812973022461, Forward: 0.08175921440124512, Backward: 0.10286307334899902, Optimizer: 0
Epoch 1 Batch 1302 Train Loss 0.01762864924967289
Total Times. Batch: 1302, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012688636779785156, Forward: 0.08153724670410156, Backward: 0.10339474678039551, Optimizer: 0
Epoch 1 Batch 1303 Train Loss 0.01802719011902809
Total Times. Batch: 1303, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011973381042480469, Forward: 0.08147978782653809, Backward: 0.1035006046295166, Optimizer: 0
Epoch 1 Batch 1304 Train Loss 0.01743597351014614
Total Times. Batch: 1304, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001359701156616211, Forward: 0.08101344108581543, Backward: 0.10364151000976562, Optimizer: 0.16696667671203613
Epoch 1 Batch 1305 Train Loss 0.017465095967054367
Total Times. Batch: 1305, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011761188507080078, Forward: 0.08111429214477539, Backward: 0.09431648254394531, Optimizer: 0
Epoch 1 Batch 1306 Train Loss 0.016991911455988884
Total Times. Batch: 1306, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001203775405883789, Forward: 0.08232331275939941, Backward: 0.10292816162109375, Optimizer: 0
Epoch 1 Batch 1307 Train Loss 0.017533963546156883
Total Times. Batch: 1307, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012621879577636719, Forward: 0.08235859870910645, Backward: 0.10306453704833984, Optimizer: 0
Epoch 1 Batch 1308 Train Loss 0.017919978126883507
Total Times. Batch: 1308, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011434555053710938, Forward: 0.08154797554016113, Backward: 0.10338711738586426, Optimizer: 0
Epoch 1 Batch 1309 Train Loss 0.017544535920023918
Total Times. Batch: 1309, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011882781982421875, Forward: 0.0811309814453125, Backward: 0.10339212417602539, Optimizer: 0.16704225540161133
Epoch 1 Batch 1310 Train Loss 0.017144305631518364
Total Times. Batch: 1310, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011644363403320312, Forward: 0.08104562759399414, Backward: 0.09404706954956055, Optimizer: 0
Epoch 1 Batch 1311 Train Loss 0.016928091645240784
Total Times. Batch: 1311, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012564659118652344, Forward: 0.08136391639709473, Backward: 0.10328030586242676, Optimizer: 0
Epoch 1 Batch 1312 Train Loss 0.01800081692636013
Total Times. Batch: 1312, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011508464813232422, Forward: 0.08164381980895996, Backward: 0.10311627388000488, Optimizer: 0
Epoch 1 Batch 1313 Train Loss 0.01800219900906086
Total Times. Batch: 1313, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001230001449584961, Forward: 0.08163952827453613, Backward: 0.10350275039672852, Optimizer: 0
Epoch 1 Batch 1314 Train Loss 0.017183054238557816
Total Times. Batch: 1314, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011799335479736328, Forward: 0.08141040802001953, Backward: 0.10356688499450684, Optimizer: 0.16747331619262695
Epoch 1 Batch 1315 Train Loss 0.018043728545308113
Total Times. Batch: 1315, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011873245239257812, Forward: 0.08175063133239746, Backward: 0.09387946128845215, Optimizer: 0
Epoch 1 Batch 1316 Train Loss 0.017428386956453323
Total Times. Batch: 1316, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001262664794921875, Forward: 0.08136105537414551, Backward: 0.1036367416381836, Optimizer: 0
Epoch 1 Batch 1317 Train Loss 0.01764063909649849
Total Times. Batch: 1317, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011582374572753906, Forward: 0.08108854293823242, Backward: 0.10331392288208008, Optimizer: 0
Epoch 1 Batch 1318 Train Loss 0.017574574798345566
Total Times. Batch: 1318, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011420249938964844, Forward: 0.08194780349731445, Backward: 0.10304951667785645, Optimizer: 0
Epoch 1 Batch 1319 Train Loss 0.018020648509263992
Total Times. Batch: 1319, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012309551239013672, Forward: 0.08177042007446289, Backward: 0.10340332984924316, Optimizer: 0.1671915054321289
Epoch 1 Batch 1320 Train Loss 0.017711898311972618
Total Times. Batch: 1320, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011827945709228516, Forward: 0.08118104934692383, Backward: 0.09376215934753418, Optimizer: 0
Epoch 1 Batch 1321 Train Loss 0.01730019599199295
Total Times. Batch: 1321, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012309551239013672, Forward: 0.08160901069641113, Backward: 0.10336041450500488, Optimizer: 0
Epoch 1 Batch 1322 Train Loss 0.017448846250772476
Total Times. Batch: 1322, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011763572692871094, Forward: 0.08125805854797363, Backward: 0.10375118255615234, Optimizer: 0
Epoch 1 Batch 1323 Train Loss 0.017941517755389214
Total Times. Batch: 1323, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011250972747802734, Forward: 0.08128476142883301, Backward: 0.10337042808532715, Optimizer: 0
Epoch 1 Batch 1324 Train Loss 0.016582442447543144
Total Times. Batch: 1324, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001135110855102539, Forward: 0.08195996284484863, Backward: 0.10306453704833984, Optimizer: 0.16742944717407227
Epoch 1 Batch 1325 Train Loss 0.017013344913721085
Total Times. Batch: 1325, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001201629638671875, Forward: 0.08134317398071289, Backward: 0.0937798023223877, Optimizer: 0
Epoch 1 Batch 1326 Train Loss 0.017728909850120544
Total Times. Batch: 1326, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011944770812988281, Forward: 0.08241677284240723, Backward: 0.10326766967773438, Optimizer: 0
Epoch 1 Batch 1327 Train Loss 0.017133722081780434
Total Times. Batch: 1327, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011835098266601562, Forward: 0.08131265640258789, Backward: 0.10366654396057129, Optimizer: 0
Epoch 1 Batch 1328 Train Loss 0.01746935211122036
Total Times. Batch: 1328, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.00118255615234375, Forward: 0.08104562759399414, Backward: 0.10362482070922852, Optimizer: 0
Epoch 1 Batch 1329 Train Loss 0.01719130575656891
Total Times. Batch: 1329, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011720657348632812, Forward: 0.08175539970397949, Backward: 0.1028909683227539, Optimizer: 0.167222261428833
Epoch 1 Batch 1330 Train Loss 0.01813250593841076
Total Times. Batch: 1330, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011301040649414062, Forward: 0.08165955543518066, Backward: 0.09401869773864746, Optimizer: 0
Epoch 1 Batch 1331 Train Loss 0.016811760142445564
Total Times. Batch: 1331, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012753009796142578, Forward: 0.08136367797851562, Backward: 0.10330986976623535, Optimizer: 0
Epoch 1 Batch 1332 Train Loss 0.017858058214187622
Total Times. Batch: 1332, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010998249053955078, Forward: 0.08163905143737793, Backward: 0.10310101509094238, Optimizer: 0
Epoch 1 Batch 1333 Train Loss 0.016175931319594383
Total Times. Batch: 1333, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010981559753417969, Forward: 0.0812370777130127, Backward: 0.10352253913879395, Optimizer: 0
Epoch 1 Batch 1334 Train Loss 0.017076561227440834
Total Times. Batch: 1334, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.00115203857421875, Forward: 0.08144354820251465, Backward: 0.1033625602722168, Optimizer: 0.1671595573425293
Epoch 1 Batch 1335 Train Loss 0.017474805936217308
Total Times. Batch: 1335, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010869503021240234, Forward: 0.08143997192382812, Backward: 0.09400749206542969, Optimizer: 0
Epoch 1 Batch 1336 Train Loss 0.017284967005252838
Total Times. Batch: 1336, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012137889862060547, Forward: 0.08196854591369629, Backward: 0.1031494140625, Optimizer: 0
Epoch 1 Batch 1337 Train Loss 0.017671294510364532
Total Times. Batch: 1337, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011632442474365234, Forward: 0.08171367645263672, Backward: 0.10322284698486328, Optimizer: 0
Epoch 1 Batch 1338 Train Loss 0.01719001494348049
Total Times. Batch: 1338, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012042522430419922, Forward: 0.08157682418823242, Backward: 0.10324215888977051, Optimizer: 0
Epoch 1 Batch 1339 Train Loss 0.017588665708899498
Total Times. Batch: 1339, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001194000244140625, Forward: 0.08106851577758789, Backward: 0.10359930992126465, Optimizer: 0.1672368049621582
Epoch 1 Batch 1340 Train Loss 0.017247816547751427
Total Times. Batch: 1340, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011365413665771484, Forward: 0.08126354217529297, Backward: 0.0937509536743164, Optimizer: 0
Epoch 1 Batch 1341 Train Loss 0.017939260229468346
Total Times. Batch: 1341, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011012554168701172, Forward: 0.08141398429870605, Backward: 0.10356855392456055, Optimizer: 0
Epoch 1 Batch 1342 Train Loss 0.017581794410943985
Total Times. Batch: 1342, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012328624725341797, Forward: 0.08178544044494629, Backward: 0.10342001914978027, Optimizer: 0
Epoch 1 Batch 1343 Train Loss 0.017339736223220825
Total Times. Batch: 1343, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.00122833251953125, Forward: 0.08167052268981934, Backward: 0.10334300994873047, Optimizer: 0
Epoch 1 Batch 1344 Train Loss 0.017261207103729248
Total Times. Batch: 1344, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012106895446777344, Forward: 0.08213067054748535, Backward: 0.10361742973327637, Optimizer: 0.16706180572509766
Epoch 1 Batch 1345 Train Loss 0.016931740567088127
Total Times. Batch: 1345, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011773109436035156, Forward: 0.08112311363220215, Backward: 0.09380722045898438, Optimizer: 0
Epoch 1 Batch 1346 Train Loss 0.018034929409623146
Total Times. Batch: 1346, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012631416320800781, Forward: 0.08133363723754883, Backward: 0.10312938690185547, Optimizer: 0
Epoch 1 Batch 1347 Train Loss 0.017646634951233864
Total Times. Batch: 1347, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011572837829589844, Forward: 0.08147144317626953, Backward: 0.10341763496398926, Optimizer: 0
Epoch 1 Batch 1348 Train Loss 0.01701856218278408
Total Times. Batch: 1348, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011410713195800781, Forward: 0.0818028450012207, Backward: 0.10327029228210449, Optimizer: 0
Epoch 1 Batch 1349 Train Loss 0.01763916015625
Total Times. Batch: 1349, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012357234954833984, Forward: 0.08162760734558105, Backward: 0.10310482978820801, Optimizer: 0.16764354705810547
Epoch 1 Batch 1350 Train Loss 0.016990328207612038
Total Times. Batch: 1350, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011680126190185547, Forward: 0.08133840560913086, Backward: 0.09413909912109375, Optimizer: 0
Epoch 1 Batch 1351 Train Loss 0.01719949021935463
Total Times. Batch: 1351, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001298666000366211, Forward: 0.08141732215881348, Backward: 0.10316276550292969, Optimizer: 0
Epoch 1 Batch 1352 Train Loss 0.01702609285712242
Total Times. Batch: 1352, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011851787567138672, Forward: 0.08126640319824219, Backward: 0.10345745086669922, Optimizer: 0
Epoch 1 Batch 1353 Train Loss 0.01782901957631111
Total Times. Batch: 1353, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011134147644042969, Forward: 0.0816500186920166, Backward: 0.10345005989074707, Optimizer: 0
Epoch 1 Batch 1354 Train Loss 0.017239956185221672
Total Times. Batch: 1354, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011501312255859375, Forward: 0.08146452903747559, Backward: 0.10352921485900879, Optimizer: 0.1668694019317627
Epoch 1 Batch 1355 Train Loss 0.01755344681441784
Total Times. Batch: 1355, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011756420135498047, Forward: 0.0816354751586914, Backward: 0.09341835975646973, Optimizer: 0
Epoch 1 Batch 1356 Train Loss 0.017326002940535545
Total Times. Batch: 1356, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013298988342285156, Forward: 0.08139371871948242, Backward: 0.1033165454864502, Optimizer: 0
Epoch 1 Batch 1357 Train Loss 0.01668057218194008
Total Times. Batch: 1357, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001186370849609375, Forward: 0.08110356330871582, Backward: 0.10356950759887695, Optimizer: 0
Epoch 1 Batch 1358 Train Loss 0.017668837681412697
Total Times. Batch: 1358, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011725425720214844, Forward: 0.08105993270874023, Backward: 0.10352325439453125, Optimizer: 0
Epoch 1 Batch 1359 Train Loss 0.016280701383948326
Total Times. Batch: 1359, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011677742004394531, Forward: 0.08129334449768066, Backward: 0.1030111312866211, Optimizer: 0.1671452522277832
Epoch 1 Batch 1360 Train Loss 0.017206519842147827
Total Times. Batch: 1360, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011589527130126953, Forward: 0.08168339729309082, Backward: 0.09376263618469238, Optimizer: 0
Epoch 1 Batch 1361 Train Loss 0.018306531012058258
Total Times. Batch: 1361, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.00135040283203125, Forward: 0.0816032886505127, Backward: 0.10316896438598633, Optimizer: 0
Epoch 1 Batch 1362 Train Loss 0.017632154747843742
Total Times. Batch: 1362, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012044906616210938, Forward: 0.08135128021240234, Backward: 0.10340213775634766, Optimizer: 0
Epoch 1 Batch 1363 Train Loss 0.017185263335704803
Total Times. Batch: 1363, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011906623840332031, Forward: 0.08135271072387695, Backward: 0.10344552993774414, Optimizer: 0
Epoch 1 Batch 1364 Train Loss 0.018203336745500565
Total Times. Batch: 1364, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011584758758544922, Forward: 0.08155083656311035, Backward: 0.10370922088623047, Optimizer: 0.16689705848693848
Epoch 1 Batch 1365 Train Loss 0.017284715548157692
Total Times. Batch: 1365, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011563301086425781, Forward: 0.0814049243927002, Backward: 0.09361124038696289, Optimizer: 0
Epoch 1 Batch 1366 Train Loss 0.016414055600762367
Total Times. Batch: 1366, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012595653533935547, Forward: 0.08153796195983887, Backward: 0.10321283340454102, Optimizer: 0
Epoch 1 Batch 1367 Train Loss 0.01734291948378086
Total Times. Batch: 1367, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001316070556640625, Forward: 0.0815877914428711, Backward: 0.10325431823730469, Optimizer: 0
Epoch 1 Batch 1368 Train Loss 0.017090745270252228
Total Times. Batch: 1368, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012047290802001953, Forward: 0.08203315734863281, Backward: 0.10338687896728516, Optimizer: 0
Epoch 1 Batch 1369 Train Loss 0.01723831333220005
Total Times. Batch: 1369, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011887550354003906, Forward: 0.08154487609863281, Backward: 0.10326361656188965, Optimizer: 0.1670377254486084
Epoch 1 Batch 1370 Train Loss 0.01815822720527649
Total Times. Batch: 1370, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011126995086669922, Forward: 0.08127617835998535, Backward: 0.09434866905212402, Optimizer: 0
Epoch 1 Batch 1371 Train Loss 0.016637708991765976
Total Times. Batch: 1371, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011649131774902344, Forward: 0.08120036125183105, Backward: 0.1036076545715332, Optimizer: 0
Epoch 1 Batch 1372 Train Loss 0.01790545880794525
Total Times. Batch: 1372, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011553764343261719, Forward: 0.08185076713562012, Backward: 0.10346317291259766, Optimizer: 0
Epoch 1 Batch 1373 Train Loss 0.01737126149237156
Total Times. Batch: 1373, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013530254364013672, Forward: 0.08179569244384766, Backward: 0.10314679145812988, Optimizer: 0
Epoch 1 Batch 1374 Train Loss 0.017344070598483086
Total Times. Batch: 1374, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012187957763671875, Forward: 0.08142709732055664, Backward: 0.10329747200012207, Optimizer: 0.1670224666595459
Epoch 1 Batch 1375 Train Loss 0.017859293147921562
Total Times. Batch: 1375, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011141300201416016, Forward: 0.08128881454467773, Backward: 0.09368133544921875, Optimizer: 0
Epoch 1 Batch 1376 Train Loss 0.01696036383509636
Total Times. Batch: 1376, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001173257827758789, Forward: 0.08195352554321289, Backward: 0.10347938537597656, Optimizer: 0
Epoch 1 Batch 1377 Train Loss 0.017120730131864548
Total Times. Batch: 1377, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011088848114013672, Forward: 0.08182978630065918, Backward: 0.10382509231567383, Optimizer: 0
Epoch 1 Batch 1378 Train Loss 0.017241189256310463
Total Times. Batch: 1378, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011584758758544922, Forward: 0.08186030387878418, Backward: 0.10305285453796387, Optimizer: 0
Epoch 1 Batch 1379 Train Loss 0.01709544099867344
Total Times. Batch: 1379, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012454986572265625, Forward: 0.08164286613464355, Backward: 0.10347223281860352, Optimizer: 0.1673586368560791
Epoch 1 Batch 1380 Train Loss 0.016481196507811546
Total Times. Batch: 1380, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011873245239257812, Forward: 0.08185386657714844, Backward: 0.09339094161987305, Optimizer: 0
Epoch 1 Batch 1381 Train Loss 0.01694035902619362
Total Times. Batch: 1381, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011870861053466797, Forward: 0.08135485649108887, Backward: 0.10339999198913574, Optimizer: 0
Epoch 1 Batch 1382 Train Loss 0.016366377472877502
Total Times. Batch: 1382, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010974407196044922, Forward: 0.08129096031188965, Backward: 0.10362577438354492, Optimizer: 0
Epoch 1 Batch 1383 Train Loss 0.01611592434346676
Total Times. Batch: 1383, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011467933654785156, Forward: 0.08127713203430176, Backward: 0.10338735580444336, Optimizer: 0
Epoch 1 Batch 1384 Train Loss 0.01658550836145878
Total Times. Batch: 1384, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011687278747558594, Forward: 0.08190202713012695, Backward: 0.10288047790527344, Optimizer: 0.16708087921142578
Epoch 1 Batch 1385 Train Loss 0.017005696892738342
Total Times. Batch: 1385, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011200904846191406, Forward: 0.0817103385925293, Backward: 0.09396696090698242, Optimizer: 0
Epoch 1 Batch 1386 Train Loss 0.016581818461418152
Total Times. Batch: 1386, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013103485107421875, Forward: 0.0816185474395752, Backward: 0.10362577438354492, Optimizer: 0
Epoch 1 Batch 1387 Train Loss 0.017927879467606544
Total Times. Batch: 1387, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011935234069824219, Forward: 0.08145904541015625, Backward: 0.10352873802185059, Optimizer: 0
Epoch 1 Batch 1388 Train Loss 0.016645992174744606
Total Times. Batch: 1388, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001173257827758789, Forward: 0.08144998550415039, Backward: 0.10399937629699707, Optimizer: 0
Epoch 1 Batch 1389 Train Loss 0.016640309244394302
Total Times. Batch: 1389, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013108253479003906, Forward: 0.08108162879943848, Backward: 0.10352158546447754, Optimizer: 0.1674041748046875
Epoch 1 Batch 1390 Train Loss 0.017440563067793846
Total Times. Batch: 1390, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011401176452636719, Forward: 0.08173012733459473, Backward: 0.09369254112243652, Optimizer: 0
Epoch 1 Batch 1391 Train Loss 0.016472741961479187
Total Times. Batch: 1391, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012142658233642578, Forward: 0.08228611946105957, Backward: 0.1030268669128418, Optimizer: 0
Epoch 1 Batch 1392 Train Loss 0.017480073496699333
Total Times. Batch: 1392, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012176036834716797, Forward: 0.0817101001739502, Backward: 0.10346627235412598, Optimizer: 0
Epoch 1 Batch 1393 Train Loss 0.01767774671316147
Total Times. Batch: 1393, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001196146011352539, Forward: 0.08123612403869629, Backward: 0.10345244407653809, Optimizer: 0
Epoch 1 Batch 1394 Train Loss 0.01670595444738865
Total Times. Batch: 1394, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010988712310791016, Forward: 0.0810546875, Backward: 0.10323095321655273, Optimizer: 0.16679096221923828
Epoch 1 Batch 1395 Train Loss 0.016353946179151535
Total Times. Batch: 1395, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011143684387207031, Forward: 0.08088541030883789, Backward: 0.09432482719421387, Optimizer: 0
Epoch 1 Batch 1396 Train Loss 0.01679832674562931
Total Times. Batch: 1396, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012555122375488281, Forward: 0.08232927322387695, Backward: 0.10306262969970703, Optimizer: 0
Epoch 1 Batch 1397 Train Loss 0.016696274280548096
Total Times. Batch: 1397, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012471675872802734, Forward: 0.08165216445922852, Backward: 0.10333752632141113, Optimizer: 0
Epoch 1 Batch 1398 Train Loss 0.017159655690193176
Total Times. Batch: 1398, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011870861053466797, Forward: 0.08135485649108887, Backward: 0.1033942699432373, Optimizer: 0
Epoch 1 Batch 1399 Train Loss 0.017086965963244438
Total Times. Batch: 1399, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011875629425048828, Forward: 0.08208990097045898, Backward: 0.10347723960876465, Optimizer: 0.16669464111328125
Epoch 1 Batch 1400 Train Loss 0.016964340582489967
Total Times. Batch: 1400, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001173257827758789, Forward: 0.08201313018798828, Backward: 0.09396481513977051, Optimizer: 0
Epoch 1 Batch 1401 Train Loss 0.01638045907020569
Total Times. Batch: 1401, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012631416320800781, Forward: 0.08173418045043945, Backward: 0.10372543334960938, Optimizer: 0
Epoch 1 Batch 1402 Train Loss 0.0166082214564085
Total Times. Batch: 1402, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011396408081054688, Forward: 0.08128571510314941, Backward: 0.10343098640441895, Optimizer: 0
Epoch 1 Batch 1403 Train Loss 0.016221826896071434
Total Times. Batch: 1403, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013132095336914062, Forward: 0.08191204071044922, Backward: 0.10316276550292969, Optimizer: 0
Epoch 1 Batch 1404 Train Loss 0.016886845231056213
Total Times. Batch: 1404, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013065338134765625, Forward: 0.0814516544342041, Backward: 0.10330915451049805, Optimizer: 0.16692328453063965
Epoch 1 Batch 1405 Train Loss 0.016849657520651817
Total Times. Batch: 1405, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011775493621826172, Forward: 0.08103561401367188, Backward: 0.09394645690917969, Optimizer: 0
Epoch 1 Batch 1406 Train Loss 0.016357002779841423
Total Times. Batch: 1406, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012083053588867188, Forward: 0.08113932609558105, Backward: 0.10369682312011719, Optimizer: 0
Epoch 1 Batch 1407 Train Loss 0.017039528116583824
Total Times. Batch: 1407, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011591911315917969, Forward: 0.08149433135986328, Backward: 0.10363364219665527, Optimizer: 0
Epoch 1 Batch 1408 Train Loss 0.01596817560493946
Total Times. Batch: 1408, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011417865753173828, Forward: 0.08141946792602539, Backward: 0.10315060615539551, Optimizer: 0
Epoch 1 Batch 1409 Train Loss 0.017099225893616676
Total Times. Batch: 1409, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.00122833251953125, Forward: 0.08172202110290527, Backward: 0.10315465927124023, Optimizer: 0.16743254661560059
Epoch 1 Batch 1410 Train Loss 0.017168262973427773
Total Times. Batch: 1410, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012068748474121094, Forward: 0.08117866516113281, Backward: 0.0941622257232666, Optimizer: 0
Epoch 1 Batch 1411 Train Loss 0.016376161947846413
Total Times. Batch: 1411, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012249946594238281, Forward: 0.08155393600463867, Backward: 0.1035003662109375, Optimizer: 0
Epoch 1 Batch 1412 Train Loss 0.016185954213142395
Total Times. Batch: 1412, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011050701141357422, Forward: 0.08111715316772461, Backward: 0.10303473472595215, Optimizer: 0
Epoch 1 Batch 1413 Train Loss 0.016199255362153053
Total Times. Batch: 1413, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010962486267089844, Forward: 0.08137130737304688, Backward: 0.10337114334106445, Optimizer: 0
Epoch 1 Batch 1414 Train Loss 0.016905752941966057
Total Times. Batch: 1414, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011091232299804688, Forward: 0.08183646202087402, Backward: 0.10285544395446777, Optimizer: 0.16712379455566406
Epoch 1 Batch 1415 Train Loss 0.016635913401842117
Total Times. Batch: 1415, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001196146011352539, Forward: 0.08138370513916016, Backward: 0.09380602836608887, Optimizer: 0
Epoch 1 Batch 1416 Train Loss 0.017069537192583084
Total Times. Batch: 1416, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011920928955078125, Forward: 0.0817108154296875, Backward: 0.10340523719787598, Optimizer: 0
Epoch 1 Batch 1417 Train Loss 0.016173116862773895
Total Times. Batch: 1417, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010960102081298828, Forward: 0.08141946792602539, Backward: 0.10366654396057129, Optimizer: 0
Epoch 1 Batch 1418 Train Loss 0.016407683491706848
Total Times. Batch: 1418, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010962486267089844, Forward: 0.08161163330078125, Backward: 0.10370850563049316, Optimizer: 0
Epoch 1 Batch 1419 Train Loss 0.01639663614332676
Total Times. Batch: 1419, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011835098266601562, Forward: 0.08137106895446777, Backward: 0.10343718528747559, Optimizer: 0.1677391529083252
Epoch 1 Batch 1420 Train Loss 0.01641525886952877
Total Times. Batch: 1420, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011332035064697266, Forward: 0.08154702186584473, Backward: 0.09386944770812988, Optimizer: 0
Epoch 1 Batch 1421 Train Loss 0.017489418387413025
Total Times. Batch: 1421, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011432170867919922, Forward: 0.08158683776855469, Backward: 0.10339498519897461, Optimizer: 0
Epoch 1 Batch 1422 Train Loss 0.0172526054084301
Total Times. Batch: 1422, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011513233184814453, Forward: 0.08128738403320312, Backward: 0.10306406021118164, Optimizer: 0
Epoch 1 Batch 1423 Train Loss 0.017104361206293106
Total Times. Batch: 1423, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001138448715209961, Forward: 0.08129644393920898, Backward: 0.10355186462402344, Optimizer: 0
Epoch 1 Batch 1424 Train Loss 0.01621655561029911
Total Times. Batch: 1424, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010941028594970703, Forward: 0.08155393600463867, Backward: 0.10325264930725098, Optimizer: 0.16698813438415527
Epoch 1 Batch 1425 Train Loss 0.01634093001484871
Total Times. Batch: 1425, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011663436889648438, Forward: 0.08141398429870605, Backward: 0.09401440620422363, Optimizer: 0
Epoch 1 Batch 1426 Train Loss 0.01766642928123474
Total Times. Batch: 1426, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011374950408935547, Forward: 0.08247756958007812, Backward: 0.10289311408996582, Optimizer: 0
Epoch 1 Batch 1427 Train Loss 0.016517654061317444
Total Times. Batch: 1427, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012402534484863281, Forward: 0.08236145973205566, Backward: 0.10324406623840332, Optimizer: 0
Epoch 1 Batch 1428 Train Loss 0.016330063343048096
Total Times. Batch: 1428, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012192726135253906, Forward: 0.0813591480255127, Backward: 0.10323858261108398, Optimizer: 0
Epoch 1 Batch 1429 Train Loss 0.016600532457232475
Total Times. Batch: 1429, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001190185546875, Forward: 0.08153128623962402, Backward: 0.10346555709838867, Optimizer: 0.16734051704406738
Epoch 1 Batch 1430 Train Loss 0.01647096686065197
Total Times. Batch: 1430, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011525154113769531, Forward: 0.08197259902954102, Backward: 0.09423828125, Optimizer: 0
Epoch 1 Batch 1431 Train Loss 0.01636357232928276
Total Times. Batch: 1431, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012598037719726562, Forward: 0.08199071884155273, Backward: 0.10337185859680176, Optimizer: 0
Epoch 1 Batch 1432 Train Loss 0.016647692769765854
Total Times. Batch: 1432, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011494159698486328, Forward: 0.08181095123291016, Backward: 0.1028585433959961, Optimizer: 0
Epoch 1 Batch 1433 Train Loss 0.015487906523048878
Total Times. Batch: 1433, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012233257293701172, Forward: 0.08153295516967773, Backward: 0.10338377952575684, Optimizer: 0
Epoch 1 Batch 1434 Train Loss 0.016176989302039146
Total Times. Batch: 1434, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011966228485107422, Forward: 0.08150720596313477, Backward: 0.10359811782836914, Optimizer: 0.16759109497070312
Epoch 1 Batch 1435 Train Loss 0.016430888324975967
Total Times. Batch: 1435, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001367330551147461, Forward: 0.08149194717407227, Backward: 0.09406232833862305, Optimizer: 0
Epoch 1 Batch 1436 Train Loss 0.016400033608078957
Total Times. Batch: 1436, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010960102081298828, Forward: 0.08121514320373535, Backward: 0.10350203514099121, Optimizer: 0
Epoch 1 Batch 1437 Train Loss 0.016884522512555122
Total Times. Batch: 1437, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.00115203857421875, Forward: 0.08138370513916016, Backward: 0.10352587699890137, Optimizer: 0
Epoch 1 Batch 1438 Train Loss 0.016499554738402367
Total Times. Batch: 1438, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001161813735961914, Forward: 0.08231210708618164, Backward: 0.10301423072814941, Optimizer: 0
Epoch 1 Batch 1439 Train Loss 0.01667746715247631
Total Times. Batch: 1439, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012259483337402344, Forward: 0.0818185806274414, Backward: 0.10299944877624512, Optimizer: 0.166703462600708
Epoch 1 Batch 1440 Train Loss 0.016185065731406212
Total Times. Batch: 1440, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011699199676513672, Forward: 0.08160853385925293, Backward: 0.09372377395629883, Optimizer: 0
Epoch 1 Batch 1441 Train Loss 0.016473637893795967
Total Times. Batch: 1441, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011920928955078125, Forward: 0.08114004135131836, Backward: 0.10361266136169434, Optimizer: 0
Epoch 1 Batch 1442 Train Loss 0.017216166481375694
Total Times. Batch: 1442, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011434555053710938, Forward: 0.08132600784301758, Backward: 0.10364794731140137, Optimizer: 0
Epoch 1 Batch 1443 Train Loss 0.017039218917489052
Total Times. Batch: 1443, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011565685272216797, Forward: 0.08106422424316406, Backward: 0.10343194007873535, Optimizer: 0
Epoch 1 Batch 1444 Train Loss 0.0167225431650877
Total Times. Batch: 1444, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001149892807006836, Forward: 0.0822603702545166, Backward: 0.10323381423950195, Optimizer: 0.16709136962890625
Epoch 1 Batch 1445 Train Loss 0.016544613987207413
Total Times. Batch: 1445, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011749267578125, Forward: 0.08167767524719238, Backward: 0.09375858306884766, Optimizer: 0
Epoch 1 Batch 1446 Train Loss 0.01629105769097805
Total Times. Batch: 1446, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012125968933105469, Forward: 0.0816659927368164, Backward: 0.10345649719238281, Optimizer: 0
Epoch 1 Batch 1447 Train Loss 0.015813997015357018
Total Times. Batch: 1447, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011894702911376953, Forward: 0.08203506469726562, Backward: 0.10333132743835449, Optimizer: 0
Epoch 1 Batch 1448 Train Loss 0.01619032584130764
Total Times. Batch: 1448, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011801719665527344, Forward: 0.08108401298522949, Backward: 0.10348153114318848, Optimizer: 0
Epoch 1 Batch 1449 Train Loss 0.016431616619229317
Total Times. Batch: 1449, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001104116439819336, Forward: 0.08210635185241699, Backward: 0.10321760177612305, Optimizer: 0.167921781539917
Epoch 1 Batch 1450 Train Loss 0.0166401918977499
Total Times. Batch: 1450, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011467933654785156, Forward: 0.08161759376525879, Backward: 0.09385418891906738, Optimizer: 0
Epoch 1 Batch 1451 Train Loss 0.01653243973851204
Total Times. Batch: 1451, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013470649719238281, Forward: 0.08155107498168945, Backward: 0.10311269760131836, Optimizer: 0
Epoch 1 Batch 1452 Train Loss 0.016833776608109474
Total Times. Batch: 1452, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012247562408447266, Forward: 0.08192706108093262, Backward: 0.10325932502746582, Optimizer: 0
Epoch 1 Batch 1453 Train Loss 0.01685836724936962
Total Times. Batch: 1453, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012004375457763672, Forward: 0.08127498626708984, Backward: 0.10331416130065918, Optimizer: 0
Epoch 1 Batch 1454 Train Loss 0.01715075969696045
Total Times. Batch: 1454, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011105537414550781, Forward: 0.08124518394470215, Backward: 0.10348272323608398, Optimizer: 0.1669607162475586
Epoch 1 Batch 1455 Train Loss 0.015702852979302406
Total Times. Batch: 1455, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011587142944335938, Forward: 0.08109068870544434, Backward: 0.09414315223693848, Optimizer: 0
Epoch 1 Batch 1456 Train Loss 0.016433224081993103
Total Times. Batch: 1456, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012598037719726562, Forward: 0.08152890205383301, Backward: 0.10332107543945312, Optimizer: 0
Epoch 1 Batch 1457 Train Loss 0.016801049932837486
Total Times. Batch: 1457, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012402534484863281, Forward: 0.08164429664611816, Backward: 0.10324263572692871, Optimizer: 0
Epoch 1 Batch 1458 Train Loss 0.015704823657870293
Total Times. Batch: 1458, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012083053588867188, Forward: 0.08140850067138672, Backward: 0.10322022438049316, Optimizer: 0
Epoch 1 Batch 1459 Train Loss 0.015578890219330788
Total Times. Batch: 1459, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011894702911376953, Forward: 0.08107376098632812, Backward: 0.10332989692687988, Optimizer: 0.16771626472473145
Epoch 1 Batch 1460 Train Loss 0.01602128893136978
Total Times. Batch: 1460, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011065006256103516, Forward: 0.08191847801208496, Backward: 0.09411358833312988, Optimizer: 0
Epoch 1 Batch 1461 Train Loss 0.01652561128139496
Total Times. Batch: 1461, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012934207916259766, Forward: 0.0814981460571289, Backward: 0.10326576232910156, Optimizer: 0
Epoch 1 Batch 1462 Train Loss 0.016575505957007408
Total Times. Batch: 1462, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011484622955322266, Forward: 0.08184599876403809, Backward: 0.10308218002319336, Optimizer: 0
Epoch 1 Batch 1463 Train Loss 0.01634235866367817
Total Times. Batch: 1463, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012471675872802734, Forward: 0.08241701126098633, Backward: 0.10299968719482422, Optimizer: 0
Epoch 1 Batch 1464 Train Loss 0.016516104340553284
Total Times. Batch: 1464, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012366771697998047, Forward: 0.08127284049987793, Backward: 0.10335421562194824, Optimizer: 0.16701769828796387
Epoch 1 Batch 1465 Train Loss 0.016096433624625206
Total Times. Batch: 1465, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011134147644042969, Forward: 0.08110213279724121, Backward: 0.09388279914855957, Optimizer: 0
Epoch 1 Batch 1466 Train Loss 0.01652122661471367
Total Times. Batch: 1466, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012774467468261719, Forward: 0.08126211166381836, Backward: 0.10366487503051758, Optimizer: 0
Epoch 1 Batch 1467 Train Loss 0.01675654947757721
Total Times. Batch: 1467, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012192726135253906, Forward: 0.08139276504516602, Backward: 0.10378527641296387, Optimizer: 0
Epoch 1 Batch 1468 Train Loss 0.015802579000592232
Total Times. Batch: 1468, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011403560638427734, Forward: 0.08179855346679688, Backward: 0.10309243202209473, Optimizer: 0
Epoch 1 Batch 1469 Train Loss 0.016483286395668983
Total Times. Batch: 1469, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012278556823730469, Forward: 0.08168482780456543, Backward: 0.10302495956420898, Optimizer: 0.16736650466918945
Epoch 1 Batch 1470 Train Loss 0.01603311114013195
Total Times. Batch: 1470, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011916160583496094, Forward: 0.08190298080444336, Backward: 0.09381484985351562, Optimizer: 0
Epoch 1 Batch 1471 Train Loss 0.016442643478512764
Total Times. Batch: 1471, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011911392211914062, Forward: 0.08127474784851074, Backward: 0.10352849960327148, Optimizer: 0
Epoch 1 Batch 1472 Train Loss 0.015852991491556168
Total Times. Batch: 1472, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011868476867675781, Forward: 0.08142685890197754, Backward: 0.10351324081420898, Optimizer: 0
Epoch 1 Batch 1473 Train Loss 0.016227301210165024
Total Times. Batch: 1473, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011820793151855469, Forward: 0.0810861587524414, Backward: 0.10364937782287598, Optimizer: 0
Epoch 1 Batch 1474 Train Loss 0.015278169885277748
Total Times. Batch: 1474, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011565685272216797, Forward: 0.0820770263671875, Backward: 0.10309481620788574, Optimizer: 0.16699719429016113
Epoch 1 Batch 1475 Train Loss 0.016606774181127548
Total Times. Batch: 1475, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012023448944091797, Forward: 0.08121418952941895, Backward: 0.09389472007751465, Optimizer: 0
Epoch 1 Batch 1476 Train Loss 0.016165781766176224
Total Times. Batch: 1476, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011870861053466797, Forward: 0.08140802383422852, Backward: 0.10348987579345703, Optimizer: 0
Epoch 1 Batch 1477 Train Loss 0.016121279448270798
Total Times. Batch: 1477, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001131296157836914, Forward: 0.08116769790649414, Backward: 0.10338950157165527, Optimizer: 0
Epoch 1 Batch 1478 Train Loss 0.016773736104369164
Total Times. Batch: 1478, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011496543884277344, Forward: 0.0811312198638916, Backward: 0.10388827323913574, Optimizer: 0
Epoch 1 Batch 1479 Train Loss 0.0162249356508255
Total Times. Batch: 1479, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011768341064453125, Forward: 0.08107328414916992, Backward: 0.10372114181518555, Optimizer: 0.1673293113708496
Epoch 1 Batch 1480 Train Loss 0.016124306246638298
Total Times. Batch: 1480, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011713504791259766, Forward: 0.08137106895446777, Backward: 0.09408450126647949, Optimizer: 0
Epoch 1 Batch 1481 Train Loss 0.017261013388633728
Total Times. Batch: 1481, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001422882080078125, Forward: 0.08237147331237793, Backward: 0.10320115089416504, Optimizer: 0
Epoch 1 Batch 1482 Train Loss 0.01719050481915474
Total Times. Batch: 1482, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001203298568725586, Forward: 0.08130788803100586, Backward: 0.10334062576293945, Optimizer: 0
Epoch 1 Batch 1483 Train Loss 0.015916330739855766
Total Times. Batch: 1483, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011932849884033203, Forward: 0.08122467994689941, Backward: 0.10358428955078125, Optimizer: 0
Epoch 1 Batch 1484 Train Loss 0.016808737069368362
Total Times. Batch: 1484, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011699199676513672, Forward: 0.08168864250183105, Backward: 0.10335040092468262, Optimizer: 0.16753411293029785
Epoch 1 Batch 1485 Train Loss 0.017120277509093285
Total Times. Batch: 1485, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011620521545410156, Forward: 0.08123302459716797, Backward: 0.0943443775177002, Optimizer: 0
Epoch 1 Batch 1486 Train Loss 0.017284177243709564
Total Times. Batch: 1486, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012199878692626953, Forward: 0.0818631649017334, Backward: 0.10291194915771484, Optimizer: 0
Epoch 1 Batch 1487 Train Loss 0.016587361693382263
Total Times. Batch: 1487, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011703968048095703, Forward: 0.08185386657714844, Backward: 0.10315108299255371, Optimizer: 0
Epoch 1 Batch 1488 Train Loss 0.016195345669984818
Total Times. Batch: 1488, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011930465698242188, Forward: 0.08131861686706543, Backward: 0.10329151153564453, Optimizer: 0
Epoch 1 Batch 1489 Train Loss 0.01602073200047016
Total Times. Batch: 1489, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001188516616821289, Forward: 0.08158230781555176, Backward: 0.10295414924621582, Optimizer: 0.16754555702209473
Epoch 1 Batch 1490 Train Loss 0.01607327349483967
Total Times. Batch: 1490, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011146068572998047, Forward: 0.08156132698059082, Backward: 0.09397482872009277, Optimizer: 0
Epoch 1 Batch 1491 Train Loss 0.01735820434987545
Total Times. Batch: 1491, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011322498321533203, Forward: 0.0814354419708252, Backward: 0.10336995124816895, Optimizer: 0
Epoch 1 Batch 1492 Train Loss 0.016649026423692703
Total Times. Batch: 1492, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001148223876953125, Forward: 0.0812070369720459, Backward: 0.10334515571594238, Optimizer: 0
Epoch 1 Batch 1493 Train Loss 0.016958707943558693
Total Times. Batch: 1493, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012001991271972656, Forward: 0.0814657211303711, Backward: 0.10317134857177734, Optimizer: 0
Epoch 1 Batch 1494 Train Loss 0.016943473368883133
Total Times. Batch: 1494, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010972023010253906, Forward: 0.08170914649963379, Backward: 0.10278105735778809, Optimizer: 0.1669158935546875
Epoch 1 Batch 1495 Train Loss 0.016817355528473854
Total Times. Batch: 1495, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001142263412475586, Forward: 0.08187484741210938, Backward: 0.0942842960357666, Optimizer: 0
Epoch 1 Batch 1496 Train Loss 0.016025355085730553
Total Times. Batch: 1496, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011529922485351562, Forward: 0.08112835884094238, Backward: 0.10331273078918457, Optimizer: 0
Epoch 1 Batch 1497 Train Loss 0.017295807600021362
Total Times. Batch: 1497, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001247406005859375, Forward: 0.08109927177429199, Backward: 0.10352611541748047, Optimizer: 0
Epoch 1 Batch 1498 Train Loss 0.016205493360757828
Total Times. Batch: 1498, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011518001556396484, Forward: 0.0813295841217041, Backward: 0.10346031188964844, Optimizer: 0
Epoch 1 Batch 1499 Train Loss 0.016403356567025185
Total Times. Batch: 1499, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001186370849609375, Forward: 0.08139538764953613, Backward: 0.10310935974121094, Optimizer: 0.16703224182128906
Epoch 1 Batch 1500 Train Loss 0.01584397628903389
Total Times. Batch: 1500, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011610984802246094, Forward: 0.08132147789001465, Backward: 0.09395480155944824, Optimizer: 0
Epoch 1 Batch 1501 Train Loss 0.017031176015734673
Total Times. Batch: 1501, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001220703125, Forward: 0.08215212821960449, Backward: 0.10360956192016602, Optimizer: 0
Epoch 1 Batch 1502 Train Loss 0.016961172223091125
Total Times. Batch: 1502, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011448860168457031, Forward: 0.0812673568725586, Backward: 0.1036233901977539, Optimizer: 0
Epoch 1 Batch 1503 Train Loss 0.01639285683631897
Total Times. Batch: 1503, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011074542999267578, Forward: 0.08138227462768555, Backward: 0.10300230979919434, Optimizer: 0
ng: 2 lr: 0.9449582947151273 dlr: 0.00437684715381069 d_hat: -0.01329217576202308, d: 0.004631788702516401. sksq_weighted=2.5e-08 sk_l1=1.9e-04 gsq_weighted=5.0e-06
Epoch 1 Batch 1504 Train Loss 0.016483131796121597
Total Times. Batch: 1504, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011489391326904297, Forward: 0.08241701126098633, Backward: 0.10336160659790039, Optimizer: 0.16662883758544922
Epoch 1 Batch 1505 Train Loss 0.016414081677794456
Total Times. Batch: 1505, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011470317840576172, Forward: 0.08124017715454102, Backward: 0.09406685829162598, Optimizer: 0
Epoch 1 Batch 1506 Train Loss 0.01665441505610943
Total Times. Batch: 1506, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011534690856933594, Forward: 0.08149170875549316, Backward: 0.10273551940917969, Optimizer: 0
Epoch 1 Batch 1507 Train Loss 0.017401674762368202
Total Times. Batch: 1507, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011944770812988281, Forward: 0.08212065696716309, Backward: 0.1034235954284668, Optimizer: 0
Epoch 1 Batch 1508 Train Loss 0.016704609617590904
Total Times. Batch: 1508, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011014938354492188, Forward: 0.08190679550170898, Backward: 0.10344815254211426, Optimizer: 0
Epoch 1 Batch 1509 Train Loss 0.016174698248505592
Total Times. Batch: 1509, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001104593276977539, Forward: 0.08143925666809082, Backward: 0.10358500480651855, Optimizer: 0.1673743724822998
Epoch 1 Batch 1510 Train Loss 0.017293866723775864
Total Times. Batch: 1510, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011374950408935547, Forward: 0.0820763111114502, Backward: 0.09369874000549316, Optimizer: 0
Epoch 1 Batch 1511 Train Loss 0.01628211699426174
Total Times. Batch: 1511, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013582706451416016, Forward: 0.08169150352478027, Backward: 0.10341715812683105, Optimizer: 0
Epoch 1 Batch 1512 Train Loss 0.01648865081369877
Total Times. Batch: 1512, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011947154998779297, Forward: 0.08162999153137207, Backward: 0.10312032699584961, Optimizer: 0
Epoch 1 Batch 1513 Train Loss 0.01552655454725027
Total Times. Batch: 1513, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012142658233642578, Forward: 0.0815427303314209, Backward: 0.1032555103302002, Optimizer: 0
Epoch 1 Batch 1514 Train Loss 0.015586349181830883
Total Times. Batch: 1514, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011529922485351562, Forward: 0.08100724220275879, Backward: 0.10380673408508301, Optimizer: 0.16676759719848633
Epoch 1 Batch 1515 Train Loss 0.01611674204468727
Total Times. Batch: 1515, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011019706726074219, Forward: 0.08109521865844727, Backward: 0.0941474437713623, Optimizer: 0
Epoch 1 Batch 1516 Train Loss 0.017717786133289337
Total Times. Batch: 1516, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012769699096679688, Forward: 0.08167195320129395, Backward: 0.1029205322265625, Optimizer: 0
Epoch 1 Batch 1517 Train Loss 0.01644696481525898
Total Times. Batch: 1517, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012392997741699219, Forward: 0.0816493034362793, Backward: 0.10321331024169922, Optimizer: 0
Epoch 1 Batch 1518 Train Loss 0.016438430175185204
Total Times. Batch: 1518, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001155853271484375, Forward: 0.08162879943847656, Backward: 0.1031029224395752, Optimizer: 0
Epoch 1 Batch 1519 Train Loss 0.016351165249943733
Total Times. Batch: 1519, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012373924255371094, Forward: 0.08169078826904297, Backward: 0.10337615013122559, Optimizer: 0.1674511432647705
Epoch 1 Batch 1520 Train Loss 0.016521889716386795
Total Times. Batch: 1520, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011267662048339844, Forward: 0.0808722972869873, Backward: 0.09413266181945801, Optimizer: 0
Epoch 1 Batch 1521 Train Loss 0.015781937167048454
Total Times. Batch: 1521, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011756420135498047, Forward: 0.08150815963745117, Backward: 0.10265111923217773, Optimizer: 0
Epoch 1 Batch 1522 Train Loss 0.015879064798355103
Total Times. Batch: 1522, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011444091796875, Forward: 0.08202505111694336, Backward: 0.10299468040466309, Optimizer: 0
Epoch 1 Batch 1523 Train Loss 0.016597053036093712
Total Times. Batch: 1523, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012214183807373047, Forward: 0.08138012886047363, Backward: 0.10337257385253906, Optimizer: 0
Epoch 1 Batch 1524 Train Loss 0.016319643706083298
Total Times. Batch: 1524, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011200904846191406, Forward: 0.08207917213439941, Backward: 0.10344123840332031, Optimizer: 0.16651153564453125
Epoch 1 Batch 1525 Train Loss 0.01697777770459652
Total Times. Batch: 1525, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001165628433227539, Forward: 0.08126115798950195, Backward: 0.09396147727966309, Optimizer: 0
Epoch 1 Batch 1526 Train Loss 0.016252674162387848
Total Times. Batch: 1526, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011088848114013672, Forward: 0.08148956298828125, Backward: 0.10324883460998535, Optimizer: 0
Epoch 1 Batch 1527 Train Loss 0.0164236631244421
Total Times. Batch: 1527, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011076927185058594, Forward: 0.0813436508178711, Backward: 0.10359454154968262, Optimizer: 0
Epoch 1 Batch 1528 Train Loss 0.015212456695735455
Total Times. Batch: 1528, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011680126190185547, Forward: 0.08170104026794434, Backward: 0.10339188575744629, Optimizer: 0
Epoch 1 Batch 1529 Train Loss 0.015915265306830406
Total Times. Batch: 1529, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012667179107666016, Forward: 0.08112978935241699, Backward: 0.10341835021972656, Optimizer: 0.1681201457977295
Epoch 1 Batch 1530 Train Loss 0.01595071516931057
Total Times. Batch: 1530, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011925697326660156, Forward: 0.08150696754455566, Backward: 0.09398818016052246, Optimizer: 0
Epoch 1 Batch 1531 Train Loss 0.01668362319469452
Total Times. Batch: 1531, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013015270233154297, Forward: 0.08104801177978516, Backward: 0.10329294204711914, Optimizer: 0
Epoch 1 Batch 1532 Train Loss 0.015078479424118996
Total Times. Batch: 1532, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011413097381591797, Forward: 0.08106660842895508, Backward: 0.10344743728637695, Optimizer: 0
Epoch 1 Batch 1533 Train Loss 0.01659567654132843
Total Times. Batch: 1533, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011570453643798828, Forward: 0.08139967918395996, Backward: 0.10337257385253906, Optimizer: 0
Epoch 1 Batch 1534 Train Loss 0.01602139137685299
Total Times. Batch: 1534, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011553764343261719, Forward: 0.08165550231933594, Backward: 0.10320138931274414, Optimizer: 0.16711139678955078
Epoch 1 Batch 1535 Train Loss 0.016426051035523415
Total Times. Batch: 1535, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012097358703613281, Forward: 0.08141493797302246, Backward: 0.09339666366577148, Optimizer: 0
Epoch 1 Batch 1536 Train Loss 0.015314607881009579
Total Times. Batch: 1536, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011894702911376953, Forward: 0.08158421516418457, Backward: 0.10334300994873047, Optimizer: 0
Epoch 1 Batch 1537 Train Loss 0.015217864885926247
Total Times. Batch: 1537, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011968612670898438, Forward: 0.08121514320373535, Backward: 0.1035301685333252, Optimizer: 0
Epoch 1 Batch 1538 Train Loss 0.015968896448612213
Total Times. Batch: 1538, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.00115966796875, Forward: 0.08110594749450684, Backward: 0.10351753234863281, Optimizer: 0
Epoch 1 Batch 1539 Train Loss 0.016264308243989944
Total Times. Batch: 1539, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011625289916992188, Forward: 0.08121395111083984, Backward: 0.10364079475402832, Optimizer: 0.16718196868896484
Epoch 1 Batch 1540 Train Loss 0.015556714497506618
Total Times. Batch: 1540, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011565685272216797, Forward: 0.08186626434326172, Backward: 0.0936286449432373, Optimizer: 0
Epoch 1 Batch 1541 Train Loss 0.015548874624073505
Total Times. Batch: 1541, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011608600616455078, Forward: 0.08128976821899414, Backward: 0.10331177711486816, Optimizer: 0
Epoch 1 Batch 1542 Train Loss 0.016103623434901237
Total Times. Batch: 1542, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011887550354003906, Forward: 0.08156299591064453, Backward: 0.1031334400177002, Optimizer: 0
Epoch 1 Batch 1543 Train Loss 0.01614345796406269
Total Times. Batch: 1543, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011849403381347656, Forward: 0.08139753341674805, Backward: 0.10339188575744629, Optimizer: 0
Epoch 1 Batch 1544 Train Loss 0.015163798816502094
Total Times. Batch: 1544, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013475418090820312, Forward: 0.08123040199279785, Backward: 0.10337042808532715, Optimizer: 0.16701745986938477
Epoch 1 Batch 1545 Train Loss 0.015864655375480652
Total Times. Batch: 1545, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011658668518066406, Forward: 0.08119559288024902, Backward: 0.09403347969055176, Optimizer: 0
Epoch 1 Batch 1546 Train Loss 0.015419289469718933
Total Times. Batch: 1546, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012764930725097656, Forward: 0.08126163482666016, Backward: 0.10345697402954102, Optimizer: 0
Epoch 1 Batch 1547 Train Loss 0.015314591117203236
Total Times. Batch: 1547, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013146400451660156, Forward: 0.08141803741455078, Backward: 0.10328006744384766, Optimizer: 0
Epoch 1 Batch 1548 Train Loss 0.01577179506421089
Total Times. Batch: 1548, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012102127075195312, Forward: 0.08150649070739746, Backward: 0.1031949520111084, Optimizer: 0
Epoch 1 Batch 1549 Train Loss 0.015646198764443398
Total Times. Batch: 1549, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011713504791259766, Forward: 0.08128905296325684, Backward: 0.10325121879577637, Optimizer: 0.16780638694763184
Epoch 1 Batch 1550 Train Loss 0.016623906791210175
Total Times. Batch: 1550, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011506080627441406, Forward: 0.08129143714904785, Backward: 0.09416747093200684, Optimizer: 0
Epoch 1 Batch 1551 Train Loss 0.015990564599633217
Total Times. Batch: 1551, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011913776397705078, Forward: 0.08125162124633789, Backward: 0.10303354263305664, Optimizer: 0
Epoch 1 Batch 1552 Train Loss 0.01616879366338253
Total Times. Batch: 1552, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011506080627441406, Forward: 0.08153200149536133, Backward: 0.10318803787231445, Optimizer: 0
Epoch 1 Batch 1553 Train Loss 0.015487547032535076
Total Times. Batch: 1553, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001220703125, Forward: 0.0813896656036377, Backward: 0.10304546356201172, Optimizer: 0
Epoch 1 Batch 1554 Train Loss 0.015100008808076382
Total Times. Batch: 1554, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011887550354003906, Forward: 0.08121371269226074, Backward: 0.10340094566345215, Optimizer: 0.16688203811645508
Epoch 1 Batch 1555 Train Loss 0.015915317460894585
Total Times. Batch: 1555, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011420249938964844, Forward: 0.08124637603759766, Backward: 0.0938558578491211, Optimizer: 0
Epoch 1 Batch 1556 Train Loss 0.016247902065515518
Total Times. Batch: 1556, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012545585632324219, Forward: 0.08120465278625488, Backward: 0.10330772399902344, Optimizer: 0
Epoch 1 Batch 1557 Train Loss 0.015719544142484665
Total Times. Batch: 1557, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011792182922363281, Forward: 0.08134341239929199, Backward: 0.10302066802978516, Optimizer: 0
Epoch 1 Batch 1558 Train Loss 0.015834813937544823
Total Times. Batch: 1558, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011484622955322266, Forward: 0.08110857009887695, Backward: 0.1033930778503418, Optimizer: 0
Epoch 1 Batch 1559 Train Loss 0.015538337640464306
Total Times. Batch: 1559, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013589859008789062, Forward: 0.08181357383728027, Backward: 0.10297942161560059, Optimizer: 0.1670997142791748
Epoch 1 Batch 1560 Train Loss 0.016107922419905663
Total Times. Batch: 1560, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001171112060546875, Forward: 0.08123373985290527, Backward: 0.09391570091247559, Optimizer: 0
Epoch 1 Batch 1561 Train Loss 0.015415668487548828
Total Times. Batch: 1561, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001277923583984375, Forward: 0.0812368392944336, Backward: 0.10367298126220703, Optimizer: 0
Epoch 1 Batch 1562 Train Loss 0.015321931801736355
Total Times. Batch: 1562, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011012554168701172, Forward: 0.08122563362121582, Backward: 0.1031332015991211, Optimizer: 0
Epoch 1 Batch 1563 Train Loss 0.015720278024673462
Total Times. Batch: 1563, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011680126190185547, Forward: 0.08141160011291504, Backward: 0.10319328308105469, Optimizer: 0
Epoch 1 Batch 1564 Train Loss 0.016206881031394005
Total Times. Batch: 1564, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011627674102783203, Forward: 0.08167195320129395, Backward: 0.10308194160461426, Optimizer: 0.1669313907623291
Epoch 1 Batch 1565 Train Loss 0.01616550050675869
Total Times. Batch: 1565, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011982917785644531, Forward: 0.0811605453491211, Backward: 0.0940549373626709, Optimizer: 0
Epoch 1 Batch 1566 Train Loss 0.015454589389264584
Total Times. Batch: 1566, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012602806091308594, Forward: 0.08131933212280273, Backward: 0.10311484336853027, Optimizer: 0
Epoch 1 Batch 1567 Train Loss 0.01531333476305008
Total Times. Batch: 1567, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012540817260742188, Forward: 0.08139729499816895, Backward: 0.1035757064819336, Optimizer: 0
Epoch 1 Batch 1568 Train Loss 0.015681123360991478
Total Times. Batch: 1568, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011897087097167969, Forward: 0.08116984367370605, Backward: 0.10335230827331543, Optimizer: 0
Epoch 1 Batch 1569 Train Loss 0.015254161320626736
Total Times. Batch: 1569, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011744499206542969, Forward: 0.08161687850952148, Backward: 0.1031796932220459, Optimizer: 0.16716217994689941
Epoch 1 Batch 1570 Train Loss 0.016405990347266197
Total Times. Batch: 1570, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001089334487915039, Forward: 0.08123278617858887, Backward: 0.09391570091247559, Optimizer: 0
Epoch 1 Batch 1571 Train Loss 0.01603008806705475
Total Times. Batch: 1571, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0016438961029052734, Forward: 0.08188319206237793, Backward: 0.1032094955444336, Optimizer: 0
Epoch 1 Batch 1572 Train Loss 0.01506652683019638
Total Times. Batch: 1572, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011980533599853516, Forward: 0.08144402503967285, Backward: 0.10336470603942871, Optimizer: 0
Epoch 1 Batch 1573 Train Loss 0.014963408000767231
Total Times. Batch: 1573, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012183189392089844, Forward: 0.08152079582214355, Backward: 0.10315752029418945, Optimizer: 0
Epoch 1 Batch 1574 Train Loss 0.015748346224427223
Total Times. Batch: 1574, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001142740249633789, Forward: 0.08129715919494629, Backward: 0.10355472564697266, Optimizer: 0.1683199405670166
Epoch 1 Batch 1575 Train Loss 0.01551548670977354
Total Times. Batch: 1575, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011556148529052734, Forward: 0.08108806610107422, Backward: 0.09426712989807129, Optimizer: 0
Epoch 1 Batch 1576 Train Loss 0.01609806716442108
Total Times. Batch: 1576, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011532306671142578, Forward: 0.08197164535522461, Backward: 0.103179931640625, Optimizer: 0
Epoch 1 Batch 1577 Train Loss 0.015508368611335754
Total Times. Batch: 1577, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012407302856445312, Forward: 0.08145833015441895, Backward: 0.10302329063415527, Optimizer: 0
Epoch 1 Batch 1578 Train Loss 0.01603357307612896
Total Times. Batch: 1578, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011551380157470703, Forward: 0.08127117156982422, Backward: 0.10317707061767578, Optimizer: 0
Epoch 1 Batch 1579 Train Loss 0.01570097729563713
Total Times. Batch: 1579, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011663436889648438, Forward: 0.08194947242736816, Backward: 0.10310053825378418, Optimizer: 0.16719317436218262
Epoch 1 Batch 1580 Train Loss 0.014803486876189709
Total Times. Batch: 1580, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001157999038696289, Forward: 0.08101058006286621, Backward: 0.09413337707519531, Optimizer: 0
Epoch 1 Batch 1581 Train Loss 0.015650613233447075
Total Times. Batch: 1581, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012860298156738281, Forward: 0.08195328712463379, Backward: 0.10391664505004883, Optimizer: 0
Epoch 1 Batch 1582 Train Loss 0.016303448006510735
Total Times. Batch: 1582, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011441707611083984, Forward: 0.08112883567810059, Backward: 0.10325145721435547, Optimizer: 0
Epoch 1 Batch 1583 Train Loss 0.015541739761829376
Total Times. Batch: 1583, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013318061828613281, Forward: 0.08176279067993164, Backward: 0.10311293601989746, Optimizer: 0
Epoch 1 Batch 1584 Train Loss 0.015363337472081184
Total Times. Batch: 1584, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012047290802001953, Forward: 0.08149838447570801, Backward: 0.10336661338806152, Optimizer: 0.16679143905639648
Epoch 1 Batch 1585 Train Loss 0.015400871634483337
Total Times. Batch: 1585, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001377105712890625, Forward: 0.08124613761901855, Backward: 0.09371566772460938, Optimizer: 0
Epoch 1 Batch 1586 Train Loss 0.01541968435049057
Total Times. Batch: 1586, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012545585632324219, Forward: 0.0811922550201416, Backward: 0.10340166091918945, Optimizer: 0
Epoch 1 Batch 1587 Train Loss 0.01590060628950596
Total Times. Batch: 1587, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010921955108642578, Forward: 0.08136320114135742, Backward: 0.10362887382507324, Optimizer: 0
Epoch 1 Batch 1588 Train Loss 0.015894535928964615
Total Times. Batch: 1588, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011513233184814453, Forward: 0.08168554306030273, Backward: 0.10325312614440918, Optimizer: 0
Epoch 1 Batch 1589 Train Loss 0.014656269922852516
Total Times. Batch: 1589, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001260995864868164, Forward: 0.08212757110595703, Backward: 0.10306501388549805, Optimizer: 0.167283296585083
Epoch 1 Batch 1590 Train Loss 0.015116176567971706
Total Times. Batch: 1590, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0014107227325439453, Forward: 0.08155012130737305, Backward: 0.0936439037322998, Optimizer: 0
Epoch 1 Batch 1591 Train Loss 0.015075111761689186
Total Times. Batch: 1591, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011920928955078125, Forward: 0.0821990966796875, Backward: 0.10329270362854004, Optimizer: 0
Epoch 1 Batch 1592 Train Loss 0.01496795006096363
Total Times. Batch: 1592, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013530254364013672, Forward: 0.08147573471069336, Backward: 0.10334229469299316, Optimizer: 0
Epoch 1 Batch 1593 Train Loss 0.014841744676232338
Total Times. Batch: 1593, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011725425720214844, Forward: 0.08138203620910645, Backward: 0.1034696102142334, Optimizer: 0
Epoch 1 Batch 1594 Train Loss 0.015512213110923767
Total Times. Batch: 1594, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001131296157836914, Forward: 0.08168315887451172, Backward: 0.10337281227111816, Optimizer: 0.16720175743103027
Epoch 1 Batch 1595 Train Loss 0.015428724698722363
Total Times. Batch: 1595, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011875629425048828, Forward: 0.08124089241027832, Backward: 0.09393429756164551, Optimizer: 0
Epoch 1 Batch 1596 Train Loss 0.015290058217942715
Total Times. Batch: 1596, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013585090637207031, Forward: 0.08165979385375977, Backward: 0.10340142250061035, Optimizer: 0
Epoch 1 Batch 1597 Train Loss 0.015769850462675095
Total Times. Batch: 1597, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001226663589477539, Forward: 0.08252429962158203, Backward: 0.10306739807128906, Optimizer: 0
Epoch 1 Batch 1598 Train Loss 0.015642056241631508
Total Times. Batch: 1598, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011544227600097656, Forward: 0.08129239082336426, Backward: 0.10359954833984375, Optimizer: 0
Epoch 1 Batch 1599 Train Loss 0.01571659743785858
Total Times. Batch: 1599, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011889934539794922, Forward: 0.08150434494018555, Backward: 0.10338425636291504, Optimizer: 0.1673109531402588
Epoch 1 Batch 1600 Train Loss 0.015241885557770729
Total Times. Batch: 1600, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011529922485351562, Forward: 0.08141326904296875, Backward: 0.09363722801208496, Optimizer: 0
Epoch 1 Batch 1601 Train Loss 0.015239821746945381
Total Times. Batch: 1601, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001470804214477539, Forward: 0.08161020278930664, Backward: 0.10319185256958008, Optimizer: 0
Epoch 1 Batch 1602 Train Loss 0.015131792984902859
Total Times. Batch: 1602, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012078285217285156, Forward: 0.0811910629272461, Backward: 0.10318875312805176, Optimizer: 0
Epoch 1 Batch 1603 Train Loss 0.014983671717345715
Total Times. Batch: 1603, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011789798736572266, Forward: 0.08116030693054199, Backward: 0.10338807106018066, Optimizer: 0
Epoch 1 Batch 1604 Train Loss 0.015385383740067482
Total Times. Batch: 1604, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011928081512451172, Forward: 0.08095335960388184, Backward: 0.10344839096069336, Optimizer: 0.1668546199798584
Epoch 1 Batch 1605 Train Loss 0.015078050084412098
Total Times. Batch: 1605, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001138925552368164, Forward: 0.08144998550415039, Backward: 0.09376931190490723, Optimizer: 0
Epoch 1 Batch 1606 Train Loss 0.01505174022167921
Total Times. Batch: 1606, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012471675872802734, Forward: 0.08223986625671387, Backward: 0.10320305824279785, Optimizer: 0
Epoch 1 Batch 1607 Train Loss 0.014770216308534145
Total Times. Batch: 1607, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001203298568725586, Forward: 0.08205819129943848, Backward: 0.10339570045471191, Optimizer: 0
Epoch 1 Batch 1608 Train Loss 0.01544065773487091
Total Times. Batch: 1608, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011587142944335938, Forward: 0.08121633529663086, Backward: 0.10332107543945312, Optimizer: 0
Epoch 1 Batch 1609 Train Loss 0.01543367188423872
Total Times. Batch: 1609, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010895729064941406, Forward: 0.08156967163085938, Backward: 0.10264444351196289, Optimizer: 0.16744160652160645
Epoch 1 Batch 1610 Train Loss 0.014810937456786633
Total Times. Batch: 1610, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011565685272216797, Forward: 0.08110451698303223, Backward: 0.09414434432983398, Optimizer: 0
Epoch 1 Batch 1611 Train Loss 0.015905339270830154
Total Times. Batch: 1611, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011646747589111328, Forward: 0.08120536804199219, Backward: 0.10354924201965332, Optimizer: 0
Epoch 1 Batch 1612 Train Loss 0.015512471087276936
Total Times. Batch: 1612, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011785030364990234, Forward: 0.08111047744750977, Backward: 0.10300517082214355, Optimizer: 0
Epoch 1 Batch 1613 Train Loss 0.014575262553989887
Total Times. Batch: 1613, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012209415435791016, Forward: 0.08145260810852051, Backward: 0.10309553146362305, Optimizer: 0
Epoch 1 Batch 1614 Train Loss 0.015011163428425789
Total Times. Batch: 1614, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001171112060546875, Forward: 0.08184266090393066, Backward: 0.1030421257019043, Optimizer: 0.16718387603759766
Epoch 1 Batch 1615 Train Loss 0.015426411293447018
Total Times. Batch: 1615, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011789798736572266, Forward: 0.08119988441467285, Backward: 0.09389734268188477, Optimizer: 0
Epoch 1 Batch 1616 Train Loss 0.015383362770080566
Total Times. Batch: 1616, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012733936309814453, Forward: 0.0814211368560791, Backward: 0.10331988334655762, Optimizer: 0
Epoch 1 Batch 1617 Train Loss 0.015852106735110283
Total Times. Batch: 1617, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011854171752929688, Forward: 0.08126354217529297, Backward: 0.10332727432250977, Optimizer: 0
Epoch 1 Batch 1618 Train Loss 0.01583992876112461
Total Times. Batch: 1618, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.00116729736328125, Forward: 0.08159160614013672, Backward: 0.10340619087219238, Optimizer: 0
Epoch 1 Batch 1619 Train Loss 0.015855086967349052
Total Times. Batch: 1619, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012066364288330078, Forward: 0.08187079429626465, Backward: 0.1033940315246582, Optimizer: 0.1676924228668213
Epoch 1 Batch 1620 Train Loss 0.014834338799118996
Total Times. Batch: 1620, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011713504791259766, Forward: 0.08106446266174316, Backward: 0.09393501281738281, Optimizer: 0
Epoch 1 Batch 1621 Train Loss 0.01581691764295101
Total Times. Batch: 1621, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001542806625366211, Forward: 0.0812525749206543, Backward: 0.10321331024169922, Optimizer: 0
Epoch 1 Batch 1622 Train Loss 0.015620735473930836
Total Times. Batch: 1622, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011377334594726562, Forward: 0.08122134208679199, Backward: 0.10366106033325195, Optimizer: 0
Epoch 1 Batch 1623 Train Loss 0.014794877730309963
Total Times. Batch: 1623, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012059211730957031, Forward: 0.08107542991638184, Backward: 0.10349416732788086, Optimizer: 0
Epoch 1 Batch 1624 Train Loss 0.015599462203681469
Total Times. Batch: 1624, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011491775512695312, Forward: 0.08130264282226562, Backward: 0.10347628593444824, Optimizer: 0.16715312004089355
Epoch 1 Batch 1625 Train Loss 0.015645135194063187
Total Times. Batch: 1625, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012149810791015625, Forward: 0.08149528503417969, Backward: 0.0939333438873291, Optimizer: 0
Epoch 1 Batch 1626 Train Loss 0.015369332395493984
Total Times. Batch: 1626, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012164115905761719, Forward: 0.08149480819702148, Backward: 0.10332441329956055, Optimizer: 0
Epoch 1 Batch 1627 Train Loss 0.01622302643954754
Total Times. Batch: 1627, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011489391326904297, Forward: 0.08137941360473633, Backward: 0.10339760780334473, Optimizer: 0
Epoch 1 Batch 1628 Train Loss 0.015489828772842884
Total Times. Batch: 1628, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011675357818603516, Forward: 0.08101582527160645, Backward: 0.10349869728088379, Optimizer: 0
Epoch 1 Batch 1629 Train Loss 0.015655996277928352
Total Times. Batch: 1629, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011646747589111328, Forward: 0.08165144920349121, Backward: 0.10294890403747559, Optimizer: 0.167280912399292
Epoch 1 Batch 1630 Train Loss 0.015541804023087025
Total Times. Batch: 1630, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011341571807861328, Forward: 0.0814816951751709, Backward: 0.09392952919006348, Optimizer: 0
Epoch 1 Batch 1631 Train Loss 0.015361669473350048
Total Times. Batch: 1631, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013053417205810547, Forward: 0.0813896656036377, Backward: 0.10344529151916504, Optimizer: 0
Epoch 1 Batch 1632 Train Loss 0.015296203084290028
Total Times. Batch: 1632, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011055469512939453, Forward: 0.08117222785949707, Backward: 0.10357928276062012, Optimizer: 0
Epoch 1 Batch 1633 Train Loss 0.015996098518371582
Total Times. Batch: 1633, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011494159698486328, Forward: 0.08122825622558594, Backward: 0.10344147682189941, Optimizer: 0
Epoch 1 Batch 1634 Train Loss 0.014622646383941174
Total Times. Batch: 1634, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011126995086669922, Forward: 0.08155393600463867, Backward: 0.10335683822631836, Optimizer: 0.16711211204528809
Epoch 1 Batch 1635 Train Loss 0.014747953973710537
Total Times. Batch: 1635, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011534690856933594, Forward: 0.08113718032836914, Backward: 0.09390473365783691, Optimizer: 0
Epoch 1 Batch 1636 Train Loss 0.016115708276629448
Total Times. Batch: 1636, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011501312255859375, Forward: 0.08150625228881836, Backward: 0.1030280590057373, Optimizer: 0
Epoch 1 Batch 1637 Train Loss 0.015540502965450287
Total Times. Batch: 1637, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012307167053222656, Forward: 0.08159995079040527, Backward: 0.10318732261657715, Optimizer: 0
Epoch 1 Batch 1638 Train Loss 0.015156412497162819
Total Times. Batch: 1638, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001226186752319336, Forward: 0.08144545555114746, Backward: 0.10341215133666992, Optimizer: 0
Epoch 1 Batch 1639 Train Loss 0.015021780505776405
Total Times. Batch: 1639, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011670589447021484, Forward: 0.08125925064086914, Backward: 0.10374927520751953, Optimizer: 0.16702699661254883
Epoch 1 Batch 1640 Train Loss 0.014710286632180214
Total Times. Batch: 1640, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011725425720214844, Forward: 0.08154630661010742, Backward: 0.0939633846282959, Optimizer: 0
Epoch 1 Batch 1641 Train Loss 0.014695507474243641
Total Times. Batch: 1641, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012044906616210938, Forward: 0.08175492286682129, Backward: 0.10372352600097656, Optimizer: 0
Epoch 1 Batch 1642 Train Loss 0.014556596986949444
Total Times. Batch: 1642, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001146554946899414, Forward: 0.08193349838256836, Backward: 0.10316681861877441, Optimizer: 0
Epoch 1 Batch 1643 Train Loss 0.01496821641921997
Total Times. Batch: 1643, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012125968933105469, Forward: 0.08131289482116699, Backward: 0.10342669486999512, Optimizer: 0
Epoch 1 Batch 1644 Train Loss 0.015268653631210327
Total Times. Batch: 1644, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011525154113769531, Forward: 0.08127593994140625, Backward: 0.10319280624389648, Optimizer: 0.1670396327972412
Epoch 1 Batch 1645 Train Loss 0.015667006373405457
Total Times. Batch: 1645, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011646747589111328, Forward: 0.08128690719604492, Backward: 0.09403657913208008, Optimizer: 0
Epoch 1 Batch 1646 Train Loss 0.015486120246350765
Total Times. Batch: 1646, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013103485107421875, Forward: 0.08129143714904785, Backward: 0.10347652435302734, Optimizer: 0
Epoch 1 Batch 1647 Train Loss 0.015171492472290993
Total Times. Batch: 1647, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011677742004394531, Forward: 0.08103036880493164, Backward: 0.10364460945129395, Optimizer: 0
Epoch 1 Batch 1648 Train Loss 0.01541938167065382
Total Times. Batch: 1648, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011408329010009766, Forward: 0.08104825019836426, Backward: 0.1034097671508789, Optimizer: 0
Epoch 1 Batch 1649 Train Loss 0.015274653211236
Total Times. Batch: 1649, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011684894561767578, Forward: 0.08123326301574707, Backward: 0.10338234901428223, Optimizer: 0.16849732398986816
Epoch 1 Batch 1650 Train Loss 0.01599504053592682
Total Times. Batch: 1650, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012271404266357422, Forward: 0.08147811889648438, Backward: 0.09352302551269531, Optimizer: 0
Epoch 1 Batch 1651 Train Loss 0.014819437637925148
Total Times. Batch: 1651, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012049674987792969, Forward: 0.08182668685913086, Backward: 0.1037757396697998, Optimizer: 0
Epoch 1 Batch 1652 Train Loss 0.016046475619077682
Total Times. Batch: 1652, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011796951293945312, Forward: 0.08093667030334473, Backward: 0.10356998443603516, Optimizer: 0
Epoch 1 Batch 1653 Train Loss 0.014533430337905884
Total Times. Batch: 1653, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011582374572753906, Forward: 0.08136177062988281, Backward: 0.10342836380004883, Optimizer: 0
Epoch 1 Batch 1654 Train Loss 0.015133315697312355
Total Times. Batch: 1654, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011391639709472656, Forward: 0.08137130737304688, Backward: 0.10325217247009277, Optimizer: 0.1668076515197754
Epoch 1 Batch 1655 Train Loss 0.014689612202346325
Total Times. Batch: 1655, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011868476867675781, Forward: 0.08114194869995117, Backward: 0.09366106986999512, Optimizer: 0
Epoch 1 Batch 1656 Train Loss 0.015334251336753368
Total Times. Batch: 1656, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013132095336914062, Forward: 0.08146071434020996, Backward: 0.10331153869628906, Optimizer: 0
Epoch 1 Batch 1657 Train Loss 0.015086526051163673
Total Times. Batch: 1657, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011785030364990234, Forward: 0.0812685489654541, Backward: 0.10341119766235352, Optimizer: 0
Epoch 1 Batch 1658 Train Loss 0.015572953037917614
Total Times. Batch: 1658, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011641979217529297, Forward: 0.08132410049438477, Backward: 0.10363149642944336, Optimizer: 0
Epoch 1 Batch 1659 Train Loss 0.016174810007214546
Total Times. Batch: 1659, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001168966293334961, Forward: 0.08130121231079102, Backward: 0.1035614013671875, Optimizer: 0.16698765754699707
Epoch 1 Batch 1660 Train Loss 0.015632465481758118
Total Times. Batch: 1660, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011506080627441406, Forward: 0.08175992965698242, Backward: 0.09389805793762207, Optimizer: 0
Epoch 1 Batch 1661 Train Loss 0.01480933465063572
Total Times. Batch: 1661, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013360977172851562, Forward: 0.08141398429870605, Backward: 0.10326981544494629, Optimizer: 0
Epoch 1 Batch 1662 Train Loss 0.015145785175263882
Total Times. Batch: 1662, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011990070343017578, Forward: 0.0813148021697998, Backward: 0.10344386100769043, Optimizer: 0
Epoch 1 Batch 1663 Train Loss 0.01616688445210457
Total Times. Batch: 1663, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011265277862548828, Forward: 0.08225369453430176, Backward: 0.10326480865478516, Optimizer: 0
Epoch 1 Batch 1664 Train Loss 0.014835118316113949
Total Times. Batch: 1664, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012035369873046875, Forward: 0.08143472671508789, Backward: 0.10349154472351074, Optimizer: 0.16750192642211914
Epoch 1 Batch 1665 Train Loss 0.01495875883847475
Total Times. Batch: 1665, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011584758758544922, Forward: 0.08111214637756348, Backward: 0.09435033798217773, Optimizer: 0
Epoch 1 Batch 1666 Train Loss 0.01587051711976528
Total Times. Batch: 1666, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012669563293457031, Forward: 0.08113265037536621, Backward: 0.10336899757385254, Optimizer: 0
Epoch 1 Batch 1667 Train Loss 0.01491275429725647
Total Times. Batch: 1667, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012497901916503906, Forward: 0.0816030502319336, Backward: 0.10338878631591797, Optimizer: 0
Epoch 1 Batch 1668 Train Loss 0.014630553312599659
Total Times. Batch: 1668, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012025833129882812, Forward: 0.08146476745605469, Backward: 0.10315632820129395, Optimizer: 0
Epoch 1 Batch 1669 Train Loss 0.014164048247039318
Total Times. Batch: 1669, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011780261993408203, Forward: 0.08122587203979492, Backward: 0.10357022285461426, Optimizer: 0.1676020622253418
Epoch 1 Batch 1670 Train Loss 0.014745245687663555
Total Times. Batch: 1670, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011377334594726562, Forward: 0.08095312118530273, Backward: 0.0940866470336914, Optimizer: 0
Epoch 1 Batch 1671 Train Loss 0.01495176088064909
Total Times. Batch: 1671, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011637210845947266, Forward: 0.08137369155883789, Backward: 0.10337448120117188, Optimizer: 0
Epoch 1 Batch 1672 Train Loss 0.014523948542773724
Total Times. Batch: 1672, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011508464813232422, Forward: 0.08171916007995605, Backward: 0.1032865047454834, Optimizer: 0
Epoch 1 Batch 1673 Train Loss 0.014949011616408825
Total Times. Batch: 1673, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012333393096923828, Forward: 0.0813910961151123, Backward: 0.10316944122314453, Optimizer: 0
Epoch 1 Batch 1674 Train Loss 0.01501790713518858
Total Times. Batch: 1674, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011470317840576172, Forward: 0.08118176460266113, Backward: 0.10317802429199219, Optimizer: 0.16760897636413574
Epoch 1 Batch 1675 Train Loss 0.014694047160446644
Total Times. Batch: 1675, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010895729064941406, Forward: 0.0813300609588623, Backward: 0.09412145614624023, Optimizer: 0
Epoch 1 Batch 1676 Train Loss 0.015190541744232178
Total Times. Batch: 1676, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011572837829589844, Forward: 0.08141732215881348, Backward: 0.10351943969726562, Optimizer: 0
Epoch 1 Batch 1677 Train Loss 0.015023782849311829
Total Times. Batch: 1677, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011563301086425781, Forward: 0.0813910961151123, Backward: 0.10314655303955078, Optimizer: 0
Epoch 1 Batch 1678 Train Loss 0.014948762021958828
Total Times. Batch: 1678, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011432170867919922, Forward: 0.08188366889953613, Backward: 0.10305166244506836, Optimizer: 0
Epoch 1 Batch 1679 Train Loss 0.014767825603485107
Total Times. Batch: 1679, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012390613555908203, Forward: 0.08122467994689941, Backward: 0.10339117050170898, Optimizer: 0.1677241325378418
Epoch 1 Batch 1680 Train Loss 0.01518334448337555
Total Times. Batch: 1680, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011920928955078125, Forward: 0.08156991004943848, Backward: 0.09359312057495117, Optimizer: 0
Epoch 1 Batch 1681 Train Loss 0.015213802456855774
Total Times. Batch: 1681, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012884140014648438, Forward: 0.08136749267578125, Backward: 0.1034078598022461, Optimizer: 0
Epoch 1 Batch 1682 Train Loss 0.014239753596484661
Total Times. Batch: 1682, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011627674102783203, Forward: 0.08189582824707031, Backward: 0.10338211059570312, Optimizer: 0
Epoch 1 Batch 1683 Train Loss 0.014991635456681252
Total Times. Batch: 1683, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013265609741210938, Forward: 0.0812833309173584, Backward: 0.1035149097442627, Optimizer: 0
Epoch 1 Batch 1684 Train Loss 0.015093043446540833
Total Times. Batch: 1684, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011336803436279297, Forward: 0.08162617683410645, Backward: 0.1032717227935791, Optimizer: 0.16678524017333984
Epoch 1 Batch 1685 Train Loss 0.014990532770752907
Total Times. Batch: 1685, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011301040649414062, Forward: 0.08113932609558105, Backward: 0.09414315223693848, Optimizer: 0
Epoch 1 Batch 1686 Train Loss 0.015218344517052174
Total Times. Batch: 1686, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013217926025390625, Forward: 0.0816335678100586, Backward: 0.10314083099365234, Optimizer: 0
Epoch 1 Batch 1687 Train Loss 0.014184655621647835
Total Times. Batch: 1687, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011823177337646484, Forward: 0.08112215995788574, Backward: 0.10361909866333008, Optimizer: 0
Epoch 1 Batch 1688 Train Loss 0.01558843906968832
Total Times. Batch: 1688, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011682510375976562, Forward: 0.08121967315673828, Backward: 0.10370755195617676, Optimizer: 0
Epoch 1 Batch 1689 Train Loss 0.015612251125276089
Total Times. Batch: 1689, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011725425720214844, Forward: 0.08119440078735352, Backward: 0.10379409790039062, Optimizer: 0.1674938201904297
Epoch 1 Batch 1690 Train Loss 0.015160620212554932
Total Times. Batch: 1690, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011641979217529297, Forward: 0.08205223083496094, Backward: 0.0933847427368164, Optimizer: 0
Epoch 1 Batch 1691 Train Loss 0.015215489082038403
Total Times. Batch: 1691, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011615753173828125, Forward: 0.08137798309326172, Backward: 0.10343074798583984, Optimizer: 0
Epoch 1 Batch 1692 Train Loss 0.015380660071969032
Total Times. Batch: 1692, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011870861053466797, Forward: 0.0816659927368164, Backward: 0.10329294204711914, Optimizer: 0
Epoch 1 Batch 1693 Train Loss 0.015284623019397259
Total Times. Batch: 1693, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011796951293945312, Forward: 0.0815575122833252, Backward: 0.10344767570495605, Optimizer: 0
Epoch 1 Batch 1694 Train Loss 0.014756782911717892
Total Times. Batch: 1694, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001108407974243164, Forward: 0.08115077018737793, Backward: 0.10364270210266113, Optimizer: 0.16750359535217285
Epoch 1 Batch 1695 Train Loss 0.014476309530436993
Total Times. Batch: 1695, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011057853698730469, Forward: 0.08117985725402832, Backward: 0.09387707710266113, Optimizer: 0
Epoch 1 Batch 1696 Train Loss 0.015288302674889565
Total Times. Batch: 1696, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012619495391845703, Forward: 0.08127617835998535, Backward: 0.10326623916625977, Optimizer: 0
Epoch 1 Batch 1697 Train Loss 0.015195188112556934
Total Times. Batch: 1697, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010979175567626953, Forward: 0.08139967918395996, Backward: 0.10341310501098633, Optimizer: 0
Epoch 1 Batch 1698 Train Loss 0.014858096837997437
Total Times. Batch: 1698, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011737346649169922, Forward: 0.08139610290527344, Backward: 0.10339212417602539, Optimizer: 0
Epoch 1 Batch 1699 Train Loss 0.015062788501381874
Total Times. Batch: 1699, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011496543884277344, Forward: 0.08132696151733398, Backward: 0.10298418998718262, Optimizer: 0.1669149398803711
Epoch 1 Batch 1700 Train Loss 0.014735037460923195
Total Times. Batch: 1700, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011510848999023438, Forward: 0.08116388320922852, Backward: 0.0940701961517334, Optimizer: 0
Epoch 1 Batch 1701 Train Loss 0.015269546769559383
Total Times. Batch: 1701, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012171268463134766, Forward: 0.08167815208435059, Backward: 0.1034855842590332, Optimizer: 0
Epoch 1 Batch 1702 Train Loss 0.014828860759735107
Total Times. Batch: 1702, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011453628540039062, Forward: 0.08132266998291016, Backward: 0.10318350791931152, Optimizer: 0
Epoch 1 Batch 1703 Train Loss 0.014734971337020397
Total Times. Batch: 1703, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012559890747070312, Forward: 0.082183837890625, Backward: 0.10319399833679199, Optimizer: 0
Epoch 1 Batch 1704 Train Loss 0.014850600622594357
Total Times. Batch: 1704, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012230873107910156, Forward: 0.08119702339172363, Backward: 0.10346293449401855, Optimizer: 0.16704201698303223
Epoch 1 Batch 1705 Train Loss 0.014572739601135254
Total Times. Batch: 1705, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011715888977050781, Forward: 0.08145928382873535, Backward: 0.09377527236938477, Optimizer: 0
Epoch 1 Batch 1706 Train Loss 0.014435377903282642
Total Times. Batch: 1706, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012531280517578125, Forward: 0.0813133716583252, Backward: 0.10347342491149902, Optimizer: 0
Epoch 1 Batch 1707 Train Loss 0.0143783213570714
Total Times. Batch: 1707, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011501312255859375, Forward: 0.08107542991638184, Backward: 0.10325932502746582, Optimizer: 0
Epoch 1 Batch 1708 Train Loss 0.015305973589420319
Total Times. Batch: 1708, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001150369644165039, Forward: 0.08125019073486328, Backward: 0.1036386489868164, Optimizer: 0
Epoch 1 Batch 1709 Train Loss 0.015101495198905468
Total Times. Batch: 1709, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013010501861572266, Forward: 0.08177614212036133, Backward: 0.10322093963623047, Optimizer: 0.1668848991394043
Epoch 1 Batch 1710 Train Loss 0.014701339416205883
Total Times. Batch: 1710, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011603832244873047, Forward: 0.08181047439575195, Backward: 0.09381318092346191, Optimizer: 0
Epoch 1 Batch 1711 Train Loss 0.014450617134571075
Total Times. Batch: 1711, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012936592102050781, Forward: 0.08109760284423828, Backward: 0.10367131233215332, Optimizer: 0
Epoch 1 Batch 1712 Train Loss 0.014064769260585308
Total Times. Batch: 1712, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010924339294433594, Forward: 0.0813448429107666, Backward: 0.10308218002319336, Optimizer: 0
Epoch 1 Batch 1713 Train Loss 0.015095206908881664
Total Times. Batch: 1713, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011532306671142578, Forward: 0.08149409294128418, Backward: 0.10329461097717285, Optimizer: 0
Epoch 1 Batch 1714 Train Loss 0.014576195739209652
Total Times. Batch: 1714, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001176595687866211, Forward: 0.0811004638671875, Backward: 0.10361933708190918, Optimizer: 0.16772007942199707
Epoch 1 Batch 1715 Train Loss 0.014422989450395107
Total Times. Batch: 1715, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012073516845703125, Forward: 0.08152389526367188, Backward: 0.09424281120300293, Optimizer: 0
Epoch 1 Batch 1716 Train Loss 0.014393939636647701
Total Times. Batch: 1716, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001499176025390625, Forward: 0.08136606216430664, Backward: 0.10318779945373535, Optimizer: 0
Epoch 1 Batch 1717 Train Loss 0.014663608744740486
Total Times. Batch: 1717, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011856555938720703, Forward: 0.08117008209228516, Backward: 0.10333895683288574, Optimizer: 0
Epoch 1 Batch 1718 Train Loss 0.015094535425305367
Total Times. Batch: 1718, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011444091796875, Forward: 0.08129763603210449, Backward: 0.1034688949584961, Optimizer: 0
Epoch 1 Batch 1719 Train Loss 0.013978429138660431
Total Times. Batch: 1719, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011670589447021484, Forward: 0.08106160163879395, Backward: 0.10344457626342773, Optimizer: 0.16725945472717285
Epoch 1 Batch 1720 Train Loss 0.014390346594154835
Total Times. Batch: 1720, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011451244354248047, Forward: 0.08158421516418457, Backward: 0.09387779235839844, Optimizer: 0
Epoch 1 Batch 1721 Train Loss 0.014438708312809467
Total Times. Batch: 1721, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0014529228210449219, Forward: 0.08156871795654297, Backward: 0.10342597961425781, Optimizer: 0
Epoch 1 Batch 1722 Train Loss 0.014853402972221375
Total Times. Batch: 1722, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012288093566894531, Forward: 0.08112478256225586, Backward: 0.1034092903137207, Optimizer: 0
Epoch 1 Batch 1723 Train Loss 0.014601442031562328
Total Times. Batch: 1723, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010869503021240234, Forward: 0.08118581771850586, Backward: 0.10326743125915527, Optimizer: 0
Epoch 1 Batch 1724 Train Loss 0.014443640597164631
Total Times. Batch: 1724, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012087821960449219, Forward: 0.08144307136535645, Backward: 0.10323572158813477, Optimizer: 0.16678738594055176
Epoch 1 Batch 1725 Train Loss 0.014224961400032043
Total Times. Batch: 1725, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001190185546875, Forward: 0.08104276657104492, Backward: 0.09417939186096191, Optimizer: 0
Epoch 1 Batch 1726 Train Loss 0.014639409258961678
Total Times. Batch: 1726, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011415481567382812, Forward: 0.08177351951599121, Backward: 0.10323905944824219, Optimizer: 0
Epoch 1 Batch 1727 Train Loss 0.014822485856711864
Total Times. Batch: 1727, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011756420135498047, Forward: 0.08190107345581055, Backward: 0.10315203666687012, Optimizer: 0
Epoch 1 Batch 1728 Train Loss 0.014852690510451794
Total Times. Batch: 1728, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011358261108398438, Forward: 0.08154129981994629, Backward: 0.10337948799133301, Optimizer: 0
Epoch 1 Batch 1729 Train Loss 0.015052678994834423
Total Times. Batch: 1729, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011992454528808594, Forward: 0.08118939399719238, Backward: 0.10321402549743652, Optimizer: 0.16718125343322754
Epoch 1 Batch 1730 Train Loss 0.014982578344643116
Total Times. Batch: 1730, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001142740249633789, Forward: 0.0816645622253418, Backward: 0.09418416023254395, Optimizer: 0
Epoch 1 Batch 1731 Train Loss 0.01486230082809925
Total Times. Batch: 1731, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011444091796875, Forward: 0.08182930946350098, Backward: 0.10347151756286621, Optimizer: 0
Epoch 1 Batch 1732 Train Loss 0.015226147137582302
Total Times. Batch: 1732, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011403560638427734, Forward: 0.08186054229736328, Backward: 0.10310649871826172, Optimizer: 0
Epoch 1 Batch 1733 Train Loss 0.014543158002197742
Total Times. Batch: 1733, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0014216899871826172, Forward: 0.08146500587463379, Backward: 0.10326814651489258, Optimizer: 0
Epoch 1 Batch 1734 Train Loss 0.014894872903823853
Total Times. Batch: 1734, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012030601501464844, Forward: 0.08162975311279297, Backward: 0.10343766212463379, Optimizer: 0.1672830581665039
Epoch 1 Batch 1735 Train Loss 0.014844452030956745
Total Times. Batch: 1735, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011532306671142578, Forward: 0.08138585090637207, Backward: 0.0941007137298584, Optimizer: 0
Epoch 1 Batch 1736 Train Loss 0.014415288344025612
Total Times. Batch: 1736, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012309551239013672, Forward: 0.08126068115234375, Backward: 0.1033930778503418, Optimizer: 0
Epoch 1 Batch 1737 Train Loss 0.01453984435647726
Total Times. Batch: 1737, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010924339294433594, Forward: 0.08142852783203125, Backward: 0.10336995124816895, Optimizer: 0
Epoch 1 Batch 1738 Train Loss 0.01460717897862196
Total Times. Batch: 1738, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011415481567382812, Forward: 0.08139896392822266, Backward: 0.10343337059020996, Optimizer: 0
Epoch 1 Batch 1739 Train Loss 0.014655831269919872
Total Times. Batch: 1739, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012292861938476562, Forward: 0.08149528503417969, Backward: 0.103118896484375, Optimizer: 0.16745901107788086
Epoch 1 Batch 1740 Train Loss 0.014309363439679146
Total Times. Batch: 1740, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011296272277832031, Forward: 0.08136105537414551, Backward: 0.09408330917358398, Optimizer: 0
Epoch 1 Batch 1741 Train Loss 0.014903060160577297
Total Times. Batch: 1741, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001176595687866211, Forward: 0.08145546913146973, Backward: 0.10323381423950195, Optimizer: 0
Epoch 1 Batch 1742 Train Loss 0.014840955846011639
Total Times. Batch: 1742, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011138916015625, Forward: 0.08157825469970703, Backward: 0.10305929183959961, Optimizer: 0
Epoch 1 Batch 1743 Train Loss 0.014242686331272125
Total Times. Batch: 1743, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011060237884521484, Forward: 0.08170771598815918, Backward: 0.10349893569946289, Optimizer: 0
Epoch 1 Batch 1744 Train Loss 0.014733646996319294
Total Times. Batch: 1744, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011429786682128906, Forward: 0.08129024505615234, Backward: 0.10333871841430664, Optimizer: 0.1670985221862793
Epoch 1 Batch 1745 Train Loss 0.015232257544994354
Total Times. Batch: 1745, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001405954360961914, Forward: 0.08166217803955078, Backward: 0.09389042854309082, Optimizer: 0
Epoch 1 Batch 1746 Train Loss 0.014482631348073483
Total Times. Batch: 1746, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013427734375, Forward: 0.0810859203338623, Backward: 0.10323739051818848, Optimizer: 0
Epoch 1 Batch 1747 Train Loss 0.014184745959937572
Total Times. Batch: 1747, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012459754943847656, Forward: 0.0815892219543457, Backward: 0.10328507423400879, Optimizer: 0
Epoch 1 Batch 1748 Train Loss 0.013776428066194057
Total Times. Batch: 1748, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011684894561767578, Forward: 0.08155393600463867, Backward: 0.1035010814666748, Optimizer: 0
Epoch 1 Batch 1749 Train Loss 0.014778479933738708
Total Times. Batch: 1749, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012354850769042969, Forward: 0.08130311965942383, Backward: 0.10369348526000977, Optimizer: 0.1679544448852539
Epoch 1 Batch 1750 Train Loss 0.014363981783390045
Total Times. Batch: 1750, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011975765228271484, Forward: 0.08185625076293945, Backward: 0.09374189376831055, Optimizer: 0
Epoch 1 Batch 1751 Train Loss 0.014719036407768726
Total Times. Batch: 1751, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001344442367553711, Forward: 0.0815892219543457, Backward: 0.10287594795227051, Optimizer: 0
Epoch 1 Batch 1752 Train Loss 0.014258190989494324
Total Times. Batch: 1752, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012233257293701172, Forward: 0.08165907859802246, Backward: 0.10358071327209473, Optimizer: 0
Epoch 1 Batch 1753 Train Loss 0.014133251272141933
Total Times. Batch: 1753, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011522769927978516, Forward: 0.08173274993896484, Backward: 0.10390233993530273, Optimizer: 0
Epoch 1 Batch 1754 Train Loss 0.014466422609984875
Total Times. Batch: 1754, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011103153228759766, Forward: 0.08102178573608398, Backward: 0.10356569290161133, Optimizer: 0.16734743118286133
Epoch 1 Batch 1755 Train Loss 0.014201723039150238
Total Times. Batch: 1755, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001102447509765625, Forward: 0.0814046859741211, Backward: 0.09369897842407227, Optimizer: 0
Epoch 1 Batch 1756 Train Loss 0.014650747179985046
Total Times. Batch: 1756, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012557506561279297, Forward: 0.08132791519165039, Backward: 0.10344624519348145, Optimizer: 0
Epoch 1 Batch 1757 Train Loss 0.015304085798561573
Total Times. Batch: 1757, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012226104736328125, Forward: 0.08131814002990723, Backward: 0.1033320426940918, Optimizer: 0
Epoch 1 Batch 1758 Train Loss 0.014845723286271095
Total Times. Batch: 1758, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001168966293334961, Forward: 0.08139204978942871, Backward: 0.10335445404052734, Optimizer: 0
Epoch 1 Batch 1759 Train Loss 0.014390951953828335
Total Times. Batch: 1759, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001089334487915039, Forward: 0.08128643035888672, Backward: 0.10312986373901367, Optimizer: 0.1669445037841797
Epoch 1 Batch 1760 Train Loss 0.014772209338843822
Total Times. Batch: 1760, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011572837829589844, Forward: 0.08122634887695312, Backward: 0.09404611587524414, Optimizer: 0
Epoch 1 Batch 1761 Train Loss 0.01431390643119812
Total Times. Batch: 1761, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012531280517578125, Forward: 0.08159375190734863, Backward: 0.10297727584838867, Optimizer: 0
Epoch 1 Batch 1762 Train Loss 0.013896122574806213
Total Times. Batch: 1762, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001142740249633789, Forward: 0.08133101463317871, Backward: 0.10329389572143555, Optimizer: 0
Epoch 1 Batch 1763 Train Loss 0.015526602044701576
Total Times. Batch: 1763, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013148784637451172, Forward: 0.08174633979797363, Backward: 0.10305428504943848, Optimizer: 0
Epoch 1 Batch 1764 Train Loss 0.014829710125923157
Total Times. Batch: 1764, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012402534484863281, Forward: 0.08123016357421875, Backward: 0.10324239730834961, Optimizer: 0.16703176498413086
Epoch 1 Batch 1765 Train Loss 0.014494198374450207
Total Times. Batch: 1765, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011742115020751953, Forward: 0.08137321472167969, Backward: 0.09353256225585938, Optimizer: 0
Epoch 1 Batch 1766 Train Loss 0.015044594183564186
Total Times. Batch: 1766, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012392997741699219, Forward: 0.08149981498718262, Backward: 0.10379338264465332, Optimizer: 0
Epoch 1 Batch 1767 Train Loss 0.014035873115062714
Total Times. Batch: 1767, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001104116439819336, Forward: 0.08159136772155762, Backward: 0.10367012023925781, Optimizer: 0
Epoch 1 Batch 1768 Train Loss 0.014820115640759468
Total Times. Batch: 1768, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001148223876953125, Forward: 0.08166956901550293, Backward: 0.10331010818481445, Optimizer: 0
Epoch 1 Batch 1769 Train Loss 0.014776899479329586
Total Times. Batch: 1769, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012280941009521484, Forward: 0.08170747756958008, Backward: 0.10316705703735352, Optimizer: 0.1673598289489746
Epoch 1 Batch 1770 Train Loss 0.015301363542675972
Total Times. Batch: 1770, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011479854583740234, Forward: 0.08135342597961426, Backward: 0.09359288215637207, Optimizer: 0
Epoch 1 Batch 1771 Train Loss 0.014434218406677246
Total Times. Batch: 1771, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011813640594482422, Forward: 0.08233141899108887, Backward: 0.1035308837890625, Optimizer: 0
Epoch 1 Batch 1772 Train Loss 0.014248989522457123
Total Times. Batch: 1772, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001203775405883789, Forward: 0.08104419708251953, Backward: 0.1034233570098877, Optimizer: 0
Epoch 1 Batch 1773 Train Loss 0.013719749636948109
Total Times. Batch: 1773, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011610984802246094, Forward: 0.08130955696105957, Backward: 0.10328030586242676, Optimizer: 0
Epoch 1 Batch 1774 Train Loss 0.013911467976868153
Total Times. Batch: 1774, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011572837829589844, Forward: 0.08135557174682617, Backward: 0.10352969169616699, Optimizer: 0.16733908653259277
Epoch 1 Batch 1775 Train Loss 0.014243140816688538
Total Times. Batch: 1775, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001142740249633789, Forward: 0.08133864402770996, Backward: 0.09365701675415039, Optimizer: 0
Epoch 1 Batch 1776 Train Loss 0.013471667654812336
Total Times. Batch: 1776, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012547969818115234, Forward: 0.08137226104736328, Backward: 0.10333681106567383, Optimizer: 0
Epoch 1 Batch 1777 Train Loss 0.014726715162396431
Total Times. Batch: 1777, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012068748474121094, Forward: 0.08159875869750977, Backward: 0.1036679744720459, Optimizer: 0
Epoch 1 Batch 1778 Train Loss 0.014403148554265499
Total Times. Batch: 1778, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011701583862304688, Forward: 0.08143186569213867, Backward: 0.10357141494750977, Optimizer: 0
Epoch 1 Batch 1779 Train Loss 0.014345712028443813
Total Times. Batch: 1779, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001111745834350586, Forward: 0.08111572265625, Backward: 0.10338711738586426, Optimizer: 0.16713929176330566
Epoch 1 Batch 1780 Train Loss 0.014155647717416286
Total Times. Batch: 1780, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001165628433227539, Forward: 0.08192276954650879, Backward: 0.09326910972595215, Optimizer: 0
Epoch 1 Batch 1781 Train Loss 0.01463756151497364
Total Times. Batch: 1781, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012576580047607422, Forward: 0.08162403106689453, Backward: 0.10327267646789551, Optimizer: 0
Epoch 1 Batch 1782 Train Loss 0.014877678826451302
Total Times. Batch: 1782, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001207113265991211, Forward: 0.08118605613708496, Backward: 0.10353899002075195, Optimizer: 0
Epoch 1 Batch 1783 Train Loss 0.014510325156152248
Total Times. Batch: 1783, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011868476867675781, Forward: 0.081390380859375, Backward: 0.10324978828430176, Optimizer: 0
Epoch 1 Batch 1784 Train Loss 0.014802900142967701
Total Times. Batch: 1784, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011417865753173828, Forward: 0.08206057548522949, Backward: 0.10307526588439941, Optimizer: 0.16669631004333496
Epoch 1 Batch 1785 Train Loss 0.01507992297410965
Total Times. Batch: 1785, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011479854583740234, Forward: 0.08092188835144043, Backward: 0.09409308433532715, Optimizer: 0
Epoch 1 Batch 1786 Train Loss 0.015250694938004017
Total Times. Batch: 1786, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011982917785644531, Forward: 0.08120298385620117, Backward: 0.10329127311706543, Optimizer: 0
Epoch 1 Batch 1787 Train Loss 0.014616687782108784
Total Times. Batch: 1787, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011730194091796875, Forward: 0.08231496810913086, Backward: 0.1031806468963623, Optimizer: 0
Epoch 1 Batch 1788 Train Loss 0.01533827930688858
Total Times. Batch: 1788, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012176036834716797, Forward: 0.08154153823852539, Backward: 0.10312628746032715, Optimizer: 0
Epoch 1 Batch 1789 Train Loss 0.01425964292138815
Total Times. Batch: 1789, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012004375457763672, Forward: 0.08129453659057617, Backward: 0.10344743728637695, Optimizer: 0.16830182075500488
Epoch 1 Batch 1790 Train Loss 0.014912761747837067
Total Times. Batch: 1790, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011470317840576172, Forward: 0.08139753341674805, Backward: 0.09402132034301758, Optimizer: 0
Epoch 1 Batch 1791 Train Loss 0.01486963964998722
Total Times. Batch: 1791, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011909008026123047, Forward: 0.08127951622009277, Backward: 0.1034233570098877, Optimizer: 0
Epoch 1 Batch 1792 Train Loss 0.015461498871445656
Total Times. Batch: 1792, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011756420135498047, Forward: 0.08179855346679688, Backward: 0.10330748558044434, Optimizer: 0
Epoch 1 Batch 1793 Train Loss 0.014094251208007336
Total Times. Batch: 1793, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012254714965820312, Forward: 0.08156013488769531, Backward: 0.10346698760986328, Optimizer: 0
Epoch 1 Batch 1794 Train Loss 0.014916717074811459
Total Times. Batch: 1794, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011904239654541016, Forward: 0.08158516883850098, Backward: 0.10355615615844727, Optimizer: 0.16669249534606934
Epoch 1 Batch 1795 Train Loss 0.014898953028023243
Total Times. Batch: 1795, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.00135040283203125, Forward: 0.08094549179077148, Backward: 0.09406375885009766, Optimizer: 0
Epoch 1 Batch 1796 Train Loss 0.015126973390579224
Total Times. Batch: 1796, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013096332550048828, Forward: 0.08127617835998535, Backward: 0.10334467887878418, Optimizer: 0
Epoch 1 Batch 1797 Train Loss 0.015154588036239147
Total Times. Batch: 1797, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012061595916748047, Forward: 0.08166146278381348, Backward: 0.10324907302856445, Optimizer: 0
Epoch 1 Batch 1798 Train Loss 0.014593943953514099
Total Times. Batch: 1798, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011646747589111328, Forward: 0.08139610290527344, Backward: 0.10341429710388184, Optimizer: 0
Epoch 1 Batch 1799 Train Loss 0.014380494132637978
Total Times. Batch: 1799, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012240409851074219, Forward: 0.08132171630859375, Backward: 0.10294651985168457, Optimizer: 0.16726922988891602
Epoch 1 Batch 1800 Train Loss 0.014828503131866455
Total Times. Batch: 1800, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011341571807861328, Forward: 0.08148479461669922, Backward: 0.09391331672668457, Optimizer: 0
Epoch 1 Batch 1801 Train Loss 0.01453548576682806
Total Times. Batch: 1801, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012836456298828125, Forward: 0.08118486404418945, Backward: 0.10344982147216797, Optimizer: 0
Epoch 1 Batch 1802 Train Loss 0.01451157033443451
Total Times. Batch: 1802, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011777877807617188, Forward: 0.08106207847595215, Backward: 0.1031196117401123, Optimizer: 0
Epoch 1 Batch 1803 Train Loss 0.01553085446357727
Total Times. Batch: 1803, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011920928955078125, Forward: 0.0812528133392334, Backward: 0.10341286659240723, Optimizer: 0
Epoch 1 Batch 1804 Train Loss 0.015632951632142067
Total Times. Batch: 1804, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011849403381347656, Forward: 0.08189845085144043, Backward: 0.10311436653137207, Optimizer: 0.16706109046936035
Epoch 1 Batch 1805 Train Loss 0.014619268476963043
Total Times. Batch: 1805, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011904239654541016, Forward: 0.08126401901245117, Backward: 0.09360051155090332, Optimizer: 0
Epoch 1 Batch 1806 Train Loss 0.014933529309928417
Total Times. Batch: 1806, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013015270233154297, Forward: 0.08165812492370605, Backward: 0.10314011573791504, Optimizer: 0
Epoch 1 Batch 1807 Train Loss 0.014787743799388409
Total Times. Batch: 1807, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012476444244384766, Forward: 0.08123135566711426, Backward: 0.10347390174865723, Optimizer: 0
Epoch 1 Batch 1808 Train Loss 0.015247697941958904
Total Times. Batch: 1808, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011620521545410156, Forward: 0.08156728744506836, Backward: 0.10335230827331543, Optimizer: 0
Epoch 1 Batch 1809 Train Loss 0.015649978071451187
Total Times. Batch: 1809, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010914802551269531, Forward: 0.08135056495666504, Backward: 0.10343503952026367, Optimizer: 0.16689300537109375
Epoch 1 Batch 1810 Train Loss 0.014569050632417202
Total Times. Batch: 1810, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011453628540039062, Forward: 0.08108377456665039, Backward: 0.09387803077697754, Optimizer: 0
Epoch 1 Batch 1811 Train Loss 0.01457552146166563
Total Times. Batch: 1811, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0017309188842773438, Forward: 0.08192133903503418, Backward: 0.1031191349029541, Optimizer: 0
Epoch 1 Batch 1812 Train Loss 0.014831364154815674
Total Times. Batch: 1812, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011870861053466797, Forward: 0.0815129280090332, Backward: 0.10338973999023438, Optimizer: 0
Epoch 1 Batch 1813 Train Loss 0.014489402063190937
Total Times. Batch: 1813, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011911392211914062, Forward: 0.08159518241882324, Backward: 0.1032416820526123, Optimizer: 0
Epoch 1 Batch 1814 Train Loss 0.015151928178966045
Total Times. Batch: 1814, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011162757873535156, Forward: 0.08149480819702148, Backward: 0.10297703742980957, Optimizer: 0.16723203659057617
Epoch 1 Batch 1815 Train Loss 0.014893871732056141
Total Times. Batch: 1815, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011453628540039062, Forward: 0.08132576942443848, Backward: 0.0935661792755127, Optimizer: 0
Epoch 1 Batch 1816 Train Loss 0.014410358853638172
Total Times. Batch: 1816, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.00124359130859375, Forward: 0.08115506172180176, Backward: 0.10330796241760254, Optimizer: 0
Epoch 1 Batch 1817 Train Loss 0.013835780322551727
Total Times. Batch: 1817, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001306772232055664, Forward: 0.08196663856506348, Backward: 0.10324573516845703, Optimizer: 0
Epoch 1 Batch 1818 Train Loss 0.013976046815514565
Total Times. Batch: 1818, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001207590103149414, Forward: 0.08151483535766602, Backward: 0.10337519645690918, Optimizer: 0
Epoch 1 Batch 1819 Train Loss 0.014831657521426678
Total Times. Batch: 1819, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011856555938720703, Forward: 0.08122587203979492, Backward: 0.10357880592346191, Optimizer: 0.16730976104736328
Epoch 1 Batch 1820 Train Loss 0.013852976262569427
Total Times. Batch: 1820, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011081695556640625, Forward: 0.08104395866394043, Backward: 0.09362196922302246, Optimizer: 0
Epoch 1 Batch 1821 Train Loss 0.014196353033185005
Total Times. Batch: 1821, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011053085327148438, Forward: 0.08132624626159668, Backward: 0.1034536361694336, Optimizer: 0
Epoch 1 Batch 1822 Train Loss 0.014524412341415882
Total Times. Batch: 1822, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011510848999023438, Forward: 0.08108854293823242, Backward: 0.10341215133666992, Optimizer: 0
Epoch 1 Batch 1823 Train Loss 0.014645877294242382
Total Times. Batch: 1823, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013132095336914062, Forward: 0.08195686340332031, Backward: 0.10305523872375488, Optimizer: 0
Epoch 1 Batch 1824 Train Loss 0.013861934654414654
Total Times. Batch: 1824, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001171112060546875, Forward: 0.08117938041687012, Backward: 0.1034994125366211, Optimizer: 0.16708660125732422
Epoch 1 Batch 1825 Train Loss 0.01472010463476181
Total Times. Batch: 1825, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011086463928222656, Forward: 0.08130598068237305, Backward: 0.09410500526428223, Optimizer: 0
Epoch 1 Batch 1826 Train Loss 0.014837813563644886
Total Times. Batch: 1826, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011973381042480469, Forward: 0.08115172386169434, Backward: 0.10373783111572266, Optimizer: 0
Epoch 1 Batch 1827 Train Loss 0.014421171508729458
Total Times. Batch: 1827, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011746883392333984, Forward: 0.08125162124633789, Backward: 0.10335230827331543, Optimizer: 0
Epoch 1 Batch 1828 Train Loss 0.014226189814507961
Total Times. Batch: 1828, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011570453643798828, Forward: 0.08110976219177246, Backward: 0.10325169563293457, Optimizer: 0
Epoch 1 Batch 1829 Train Loss 0.014503884129226208
Total Times. Batch: 1829, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013229846954345703, Forward: 0.08161091804504395, Backward: 0.1032717227935791, Optimizer: 0.16731715202331543
Epoch 1 Batch 1830 Train Loss 0.014471790753304958
Total Times. Batch: 1830, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011639595031738281, Forward: 0.08156323432922363, Backward: 0.09343338012695312, Optimizer: 0
Epoch 1 Batch 1831 Train Loss 0.01390190701931715
Total Times. Batch: 1831, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012629032135009766, Forward: 0.08124351501464844, Backward: 0.10342812538146973, Optimizer: 0
Epoch 1 Batch 1832 Train Loss 0.01460642833262682
Total Times. Batch: 1832, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012025833129882812, Forward: 0.08156180381774902, Backward: 0.10348939895629883, Optimizer: 0
Epoch 1 Batch 1833 Train Loss 0.014069839380681515
Total Times. Batch: 1833, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001180887222290039, Forward: 0.0811605453491211, Backward: 0.10329246520996094, Optimizer: 0
Epoch 1 Batch 1834 Train Loss 0.014148915186524391
Total Times. Batch: 1834, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001171112060546875, Forward: 0.08134651184082031, Backward: 0.10327649116516113, Optimizer: 0.16745781898498535
Epoch 1 Batch 1835 Train Loss 0.014367634430527687
Total Times. Batch: 1835, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012426376342773438, Forward: 0.08131694793701172, Backward: 0.0938870906829834, Optimizer: 0
Epoch 1 Batch 1836 Train Loss 0.013841805048286915
Total Times. Batch: 1836, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011856555938720703, Forward: 0.08150768280029297, Backward: 0.1031951904296875, Optimizer: 0
Epoch 1 Batch 1837 Train Loss 0.013959959149360657
Total Times. Batch: 1837, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001135110855102539, Forward: 0.08135342597961426, Backward: 0.10331892967224121, Optimizer: 0
Epoch 1 Batch 1838 Train Loss 0.014517205767333508
Total Times. Batch: 1838, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012493133544921875, Forward: 0.08108401298522949, Backward: 0.10368585586547852, Optimizer: 0
Epoch 1 Batch 1839 Train Loss 0.013934902846813202
Total Times. Batch: 1839, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011138916015625, Forward: 0.08144283294677734, Backward: 0.10288596153259277, Optimizer: 0.16699624061584473
Epoch 1 Batch 1840 Train Loss 0.013789883814752102
Total Times. Batch: 1840, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011496543884277344, Forward: 0.08095288276672363, Backward: 0.09431648254394531, Optimizer: 0
Epoch 1 Batch 1841 Train Loss 0.014020413160324097
Total Times. Batch: 1841, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011730194091796875, Forward: 0.08208513259887695, Backward: 0.1031801700592041, Optimizer: 0
Epoch 1 Batch 1842 Train Loss 0.014217957854270935
Total Times. Batch: 1842, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012180805206298828, Forward: 0.08155202865600586, Backward: 0.10340213775634766, Optimizer: 0
Epoch 1 Batch 1843 Train Loss 0.013906051404774189
Total Times. Batch: 1843, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012118816375732422, Forward: 0.0813896656036377, Backward: 0.10326552391052246, Optimizer: 0
Epoch 1 Batch 1844 Train Loss 0.014122365973889828
Total Times. Batch: 1844, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001184225082397461, Forward: 0.08148360252380371, Backward: 0.10329675674438477, Optimizer: 0.16698193550109863
Epoch 1 Batch 1845 Train Loss 0.014797307550907135
Total Times. Batch: 1845, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011444091796875, Forward: 0.08101105690002441, Backward: 0.09418964385986328, Optimizer: 0
Epoch 1 Batch 1846 Train Loss 0.01337465364485979
Total Times. Batch: 1846, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012557506561279297, Forward: 0.08133625984191895, Backward: 0.10322165489196777, Optimizer: 0
Epoch 1 Batch 1847 Train Loss 0.014056280255317688
Total Times. Batch: 1847, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011990070343017578, Forward: 0.08122038841247559, Backward: 0.10363435745239258, Optimizer: 0
Epoch 1 Batch 1848 Train Loss 0.014122599735856056
Total Times. Batch: 1848, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011005401611328125, Forward: 0.08142518997192383, Backward: 0.10343170166015625, Optimizer: 0
Epoch 1 Batch 1849 Train Loss 0.014898908324539661
Total Times. Batch: 1849, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010986328125, Forward: 0.08121633529663086, Backward: 0.10321283340454102, Optimizer: 0.16677522659301758
Epoch 1 Batch 1850 Train Loss 0.014252351596951485
Total Times. Batch: 1850, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013446807861328125, Forward: 0.08108067512512207, Backward: 0.09389090538024902, Optimizer: 0
Epoch 1 Batch 1851 Train Loss 0.014299175702035427
Total Times. Batch: 1851, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012536048889160156, Forward: 0.08130192756652832, Backward: 0.1032109260559082, Optimizer: 0
Epoch 1 Batch 1852 Train Loss 0.013580165803432465
Total Times. Batch: 1852, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011589527130126953, Forward: 0.0817406177520752, Backward: 0.10333847999572754, Optimizer: 0
Epoch 1 Batch 1853 Train Loss 0.014542058110237122
Total Times. Batch: 1853, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012257099151611328, Forward: 0.08110713958740234, Backward: 0.10332989692687988, Optimizer: 0
Epoch 1 Batch 1854 Train Loss 0.014228485524654388
Total Times. Batch: 1854, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012242794036865234, Forward: 0.0814964771270752, Backward: 0.1033773422241211, Optimizer: 0.16686463356018066
Epoch 1 Batch 1855 Train Loss 0.01441489439457655
Total Times. Batch: 1855, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011718273162841797, Forward: 0.0811605453491211, Backward: 0.09418320655822754, Optimizer: 0
Epoch 1 Batch 1856 Train Loss 0.014217713847756386
Total Times. Batch: 1856, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001313924789428711, Forward: 0.0810859203338623, Backward: 0.10323071479797363, Optimizer: 0
Epoch 1 Batch 1857 Train Loss 0.014285278506577015
Total Times. Batch: 1857, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011699199676513672, Forward: 0.08126330375671387, Backward: 0.1032552719116211, Optimizer: 0
Epoch 1 Batch 1858 Train Loss 0.013928472995758057
Total Times. Batch: 1858, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011088848114013672, Forward: 0.08140921592712402, Backward: 0.10345339775085449, Optimizer: 0
Epoch 1 Batch 1859 Train Loss 0.014438820071518421
Total Times. Batch: 1859, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013399124145507812, Forward: 0.0817251205444336, Backward: 0.10324883460998535, Optimizer: 0.16804766654968262
Epoch 1 Batch 1860 Train Loss 0.014153093099594116
Total Times. Batch: 1860, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011801719665527344, Forward: 0.0810079574584961, Backward: 0.09403514862060547, Optimizer: 0
Epoch 1 Batch 1861 Train Loss 0.013965657912194729
Total Times. Batch: 1861, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012383460998535156, Forward: 0.0820016860961914, Backward: 0.10324573516845703, Optimizer: 0
Epoch 1 Batch 1862 Train Loss 0.013948634266853333
Total Times. Batch: 1862, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001108407974243164, Forward: 0.08148574829101562, Backward: 0.10315966606140137, Optimizer: 0
Epoch 1 Batch 1863 Train Loss 0.013449435122311115
Total Times. Batch: 1863, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011570453643798828, Forward: 0.08178186416625977, Backward: 0.10361981391906738, Optimizer: 0
Epoch 1 Batch 1864 Train Loss 0.014007951132953167
Total Times. Batch: 1864, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011374950408935547, Forward: 0.08134007453918457, Backward: 0.10340499877929688, Optimizer: 0.16695284843444824
Epoch 1 Batch 1865 Train Loss 0.014549297280609608
Total Times. Batch: 1865, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012116432189941406, Forward: 0.08135795593261719, Backward: 0.09383797645568848, Optimizer: 0
Epoch 1 Batch 1866 Train Loss 0.014744122512638569
Total Times. Batch: 1866, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001215219497680664, Forward: 0.08128881454467773, Backward: 0.1031186580657959, Optimizer: 0
Epoch 1 Batch 1867 Train Loss 0.013992716558277607
Total Times. Batch: 1867, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010929107666015625, Forward: 0.08104062080383301, Backward: 0.10374188423156738, Optimizer: 0
Epoch 1 Batch 1868 Train Loss 0.01404763013124466
Total Times. Batch: 1868, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011472702026367188, Forward: 0.08128190040588379, Backward: 0.1031944751739502, Optimizer: 0
Epoch 1 Batch 1869 Train Loss 0.01384290773421526
Total Times. Batch: 1869, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001089334487915039, Forward: 0.08127784729003906, Backward: 0.10382342338562012, Optimizer: 0.16637587547302246
Epoch 1 Batch 1870 Train Loss 0.013766387477517128
Total Times. Batch: 1870, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011413097381591797, Forward: 0.0814814567565918, Backward: 0.0934450626373291, Optimizer: 0
Epoch 1 Batch 1871 Train Loss 0.01327428501099348
Total Times. Batch: 1871, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012204647064208984, Forward: 0.08119893074035645, Backward: 0.10332036018371582, Optimizer: 0
Epoch 1 Batch 1872 Train Loss 0.014596951194107533
Total Times. Batch: 1872, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011610984802246094, Forward: 0.08124208450317383, Backward: 0.10329699516296387, Optimizer: 0
Epoch 1 Batch 1873 Train Loss 0.01329014915972948
Total Times. Batch: 1873, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011322498321533203, Forward: 0.08144211769104004, Backward: 0.10324883460998535, Optimizer: 0
Epoch 1 Batch 1874 Train Loss 0.014365717768669128
Total Times. Batch: 1874, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011451244354248047, Forward: 0.08138585090637207, Backward: 0.10323119163513184, Optimizer: 0.1665782928466797
Epoch 1 Batch 1875 Train Loss 0.014239804819226265
Total Times. Batch: 1875, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011532306671142578, Forward: 0.08128786087036133, Backward: 0.09389781951904297, Optimizer: 0
Epoch 1 Batch 1876 Train Loss 0.01396422740072012
Total Times. Batch: 1876, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011439323425292969, Forward: 0.08156275749206543, Backward: 0.10315632820129395, Optimizer: 0
Epoch 1 Batch 1877 Train Loss 0.013980196788907051
Total Times. Batch: 1877, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011692047119140625, Forward: 0.08213257789611816, Backward: 0.10320520401000977, Optimizer: 0
Epoch 1 Batch 1878 Train Loss 0.014857659116387367
Total Times. Batch: 1878, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012125968933105469, Forward: 0.08108925819396973, Backward: 0.10335612297058105, Optimizer: 0
Epoch 1 Batch 1879 Train Loss 0.014035540632903576
Total Times. Batch: 1879, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001207590103149414, Forward: 0.08256769180297852, Backward: 0.10352158546447754, Optimizer: 0.16741538047790527
Epoch 1 Batch 1880 Train Loss 0.013490268960595131
Total Times. Batch: 1880, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011761188507080078, Forward: 0.08134293556213379, Backward: 0.09420442581176758, Optimizer: 0
Epoch 1 Batch 1881 Train Loss 0.013928267173469067
Total Times. Batch: 1881, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011889934539794922, Forward: 0.08141112327575684, Backward: 0.10366296768188477, Optimizer: 0
Epoch 1 Batch 1882 Train Loss 0.01349563803523779
Total Times. Batch: 1882, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011456012725830078, Forward: 0.08156347274780273, Backward: 0.10324263572692871, Optimizer: 0
Epoch 1 Batch 1883 Train Loss 0.014208877459168434
Total Times. Batch: 1883, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011916160583496094, Forward: 0.0812225341796875, Backward: 0.10327553749084473, Optimizer: 0
Epoch 1 Batch 1884 Train Loss 0.013991436921060085
Total Times. Batch: 1884, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011508464813232422, Forward: 0.08184576034545898, Backward: 0.1033625602722168, Optimizer: 0.16713190078735352
Epoch 1 Batch 1885 Train Loss 0.014725184999406338
Total Times. Batch: 1885, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011439323425292969, Forward: 0.08121013641357422, Backward: 0.09401392936706543, Optimizer: 0
Epoch 1 Batch 1886 Train Loss 0.013950258493423462
Total Times. Batch: 1886, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001222372055053711, Forward: 0.08103156089782715, Backward: 0.10345101356506348, Optimizer: 0
Epoch 1 Batch 1887 Train Loss 0.014161154627799988
Total Times. Batch: 1887, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011556148529052734, Forward: 0.08125472068786621, Backward: 0.10345745086669922, Optimizer: 0
Epoch 1 Batch 1888 Train Loss 0.014340383000671864
Total Times. Batch: 1888, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011701583862304688, Forward: 0.08144783973693848, Backward: 0.10320663452148438, Optimizer: 0
Epoch 1 Batch 1889 Train Loss 0.014152352698147297
Total Times. Batch: 1889, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013103485107421875, Forward: 0.08156013488769531, Backward: 0.10324954986572266, Optimizer: 0.16767120361328125
Epoch 1 Batch 1890 Train Loss 0.013945757411420345
Total Times. Batch: 1890, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011777877807617188, Forward: 0.08147215843200684, Backward: 0.09380459785461426, Optimizer: 0
Epoch 1 Batch 1891 Train Loss 0.014114337973296642
Total Times. Batch: 1891, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011801719665527344, Forward: 0.08156609535217285, Backward: 0.1035146713256836, Optimizer: 0
Epoch 1 Batch 1892 Train Loss 0.013945162296295166
Total Times. Batch: 1892, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010933876037597656, Forward: 0.08127474784851074, Backward: 0.10326719284057617, Optimizer: 0
Epoch 1 Batch 1893 Train Loss 0.014015174470841885
Total Times. Batch: 1893, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011391639709472656, Forward: 0.08142471313476562, Backward: 0.10332727432250977, Optimizer: 0
Epoch 1 Batch 1894 Train Loss 0.013441269285976887
Total Times. Batch: 1894, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011594295501708984, Forward: 0.08120059967041016, Backward: 0.10356545448303223, Optimizer: 0.1670677661895752
Epoch 1 Batch 1895 Train Loss 0.014931002631783485
Total Times. Batch: 1895, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001184701919555664, Forward: 0.08113431930541992, Backward: 0.0937497615814209, Optimizer: 0
Epoch 1 Batch 1896 Train Loss 0.014850951731204987
Total Times. Batch: 1896, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012416839599609375, Forward: 0.08117151260375977, Backward: 0.1032099723815918, Optimizer: 0
Epoch 1 Batch 1897 Train Loss 0.013836977072060108
Total Times. Batch: 1897, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011348724365234375, Forward: 0.0813298225402832, Backward: 0.10321617126464844, Optimizer: 0
Epoch 1 Batch 1898 Train Loss 0.013748431578278542
Total Times. Batch: 1898, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010993480682373047, Forward: 0.08108758926391602, Backward: 0.10393333435058594, Optimizer: 0
Epoch 1 Batch 1899 Train Loss 0.013898256234824657
Total Times. Batch: 1899, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011470317840576172, Forward: 0.0815117359161377, Backward: 0.10281229019165039, Optimizer: 0.16697192192077637
Epoch 1 Batch 1900 Train Loss 0.014292868785560131
Total Times. Batch: 1900, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013599395751953125, Forward: 0.08107137680053711, Backward: 0.09373712539672852, Optimizer: 0
Epoch 1 Batch 1901 Train Loss 0.013782349415123463
Total Times. Batch: 1901, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0015730857849121094, Forward: 0.0816659927368164, Backward: 0.1033332347869873, Optimizer: 0
Epoch 1 Batch 1902 Train Loss 0.014156860299408436
Total Times. Batch: 1902, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011768341064453125, Forward: 0.08145928382873535, Backward: 0.10353755950927734, Optimizer: 0
Epoch 1 Batch 1903 Train Loss 0.014489364810287952
Total Times. Batch: 1903, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010881423950195312, Forward: 0.08105945587158203, Backward: 0.1034398078918457, Optimizer: 0
Epoch 1 Batch 1904 Train Loss 0.01414614636451006
Total Times. Batch: 1904, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011749267578125, Forward: 0.0814213752746582, Backward: 0.10332226753234863, Optimizer: 0.1673140525817871
Epoch 1 Batch 1905 Train Loss 0.01437696348875761
Total Times. Batch: 1905, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011067390441894531, Forward: 0.08138918876647949, Backward: 0.09372949600219727, Optimizer: 0
Epoch 1 Batch 1906 Train Loss 0.013751799240708351
Total Times. Batch: 1906, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012516975402832031, Forward: 0.08108139038085938, Backward: 0.10306334495544434, Optimizer: 0
Epoch 1 Batch 1907 Train Loss 0.014164782129228115
Total Times. Batch: 1907, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013124942779541016, Forward: 0.08188343048095703, Backward: 0.10327458381652832, Optimizer: 0
Epoch 1 Batch 1908 Train Loss 0.013692679814994335
Total Times. Batch: 1908, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012161731719970703, Forward: 0.08176827430725098, Backward: 0.10320568084716797, Optimizer: 0
Epoch 1 Batch 1909 Train Loss 0.013469770550727844
Total Times. Batch: 1909, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011792182922363281, Forward: 0.08155679702758789, Backward: 0.10313630104064941, Optimizer: 0.16790437698364258
Epoch 1 Batch 1910 Train Loss 0.014376563020050526
Total Times. Batch: 1910, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011515617370605469, Forward: 0.08134746551513672, Backward: 0.09400486946105957, Optimizer: 0
Epoch 1 Batch 1911 Train Loss 0.013347434811294079
Total Times. Batch: 1911, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013611316680908203, Forward: 0.08111953735351562, Backward: 0.10345292091369629, Optimizer: 0
Epoch 1 Batch 1912 Train Loss 0.014490350149571896
Total Times. Batch: 1912, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011432170867919922, Forward: 0.0816640853881836, Backward: 0.1029055118560791, Optimizer: 0
Epoch 1 Batch 1913 Train Loss 0.014348584227263927
Total Times. Batch: 1913, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012235641479492188, Forward: 0.08141684532165527, Backward: 0.10317015647888184, Optimizer: 0
Epoch 1 Batch 1914 Train Loss 0.013953250832855701
Total Times. Batch: 1914, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013513565063476562, Forward: 0.08163070678710938, Backward: 0.10353326797485352, Optimizer: 0.16711688041687012
Epoch 1 Batch 1915 Train Loss 0.014231676235795021
Total Times. Batch: 1915, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001168966293334961, Forward: 0.08182477951049805, Backward: 0.09372758865356445, Optimizer: 0
Epoch 1 Batch 1916 Train Loss 0.013732892461121082
Total Times. Batch: 1916, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012443065643310547, Forward: 0.08123898506164551, Backward: 0.10345935821533203, Optimizer: 0
Epoch 1 Batch 1917 Train Loss 0.014607161283493042
Total Times. Batch: 1917, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011644363403320312, Forward: 0.08142304420471191, Backward: 0.1034853458404541, Optimizer: 0
Epoch 1 Batch 1918 Train Loss 0.014981749467551708
Total Times. Batch: 1918, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011534690856933594, Forward: 0.08155274391174316, Backward: 0.10334396362304688, Optimizer: 0
Epoch 1 Batch 1919 Train Loss 0.014400980435311794
Total Times. Batch: 1919, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011851787567138672, Forward: 0.08126163482666016, Backward: 0.10330033302307129, Optimizer: 0.16762185096740723
Epoch 1 Batch 1920 Train Loss 0.013486462645232677
Total Times. Batch: 1920, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011949539184570312, Forward: 0.08107829093933105, Backward: 0.09404659271240234, Optimizer: 0
Epoch 1 Batch 1921 Train Loss 0.013737812638282776
Total Times. Batch: 1921, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011489391326904297, Forward: 0.0811624526977539, Backward: 0.10339713096618652, Optimizer: 0
Epoch 1 Batch 1922 Train Loss 0.013853427954018116
Total Times. Batch: 1922, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011472702026367188, Forward: 0.08134794235229492, Backward: 0.10313940048217773, Optimizer: 0
Epoch 1 Batch 1923 Train Loss 0.014181670732796192
Total Times. Batch: 1923, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001150369644165039, Forward: 0.08135199546813965, Backward: 0.1034548282623291, Optimizer: 0
Epoch 1 Batch 1924 Train Loss 0.013841524720191956
Total Times. Batch: 1924, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011248588562011719, Forward: 0.08139705657958984, Backward: 0.10350203514099121, Optimizer: 0.1672368049621582
Epoch 1 Batch 1925 Train Loss 0.014088990166783333
Total Times. Batch: 1925, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011150836944580078, Forward: 0.08124923706054688, Backward: 0.09383726119995117, Optimizer: 0
Epoch 1 Batch 1926 Train Loss 0.014165393076837063
Total Times. Batch: 1926, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011527538299560547, Forward: 0.08141970634460449, Backward: 0.10334300994873047, Optimizer: 0
Epoch 1 Batch 1927 Train Loss 0.01417194027453661
Total Times. Batch: 1927, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.00115203857421875, Forward: 0.08107781410217285, Backward: 0.10360217094421387, Optimizer: 0
Epoch 1 Batch 1928 Train Loss 0.014519574120640755
Total Times. Batch: 1928, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011432170867919922, Forward: 0.08138632774353027, Backward: 0.10349082946777344, Optimizer: 0
Epoch 1 Batch 1929 Train Loss 0.01385092455893755
Total Times. Batch: 1929, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011374950408935547, Forward: 0.0812520980834961, Backward: 0.10340714454650879, Optimizer: 0.1667766571044922
Epoch 1 Batch 1930 Train Loss 0.013732385821640491
Total Times. Batch: 1930, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001134634017944336, Forward: 0.08122444152832031, Backward: 0.09366703033447266, Optimizer: 0
Epoch 1 Batch 1931 Train Loss 0.01375630497932434
Total Times. Batch: 1931, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013391971588134766, Forward: 0.08182430267333984, Backward: 0.10322928428649902, Optimizer: 0
Epoch 1 Batch 1932 Train Loss 0.013712096028029919
Total Times. Batch: 1932, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011801719665527344, Forward: 0.08139824867248535, Backward: 0.10312390327453613, Optimizer: 0
Epoch 1 Batch 1933 Train Loss 0.01347072422504425
Total Times. Batch: 1933, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011823177337646484, Forward: 0.08133769035339355, Backward: 0.10334491729736328, Optimizer: 0
Epoch 1 Batch 1934 Train Loss 0.013508557341992855
Total Times. Batch: 1934, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011365413665771484, Forward: 0.08123493194580078, Backward: 0.10317850112915039, Optimizer: 0.16681313514709473
Epoch 1 Batch 1935 Train Loss 0.014131398871541023
Total Times. Batch: 1935, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011544227600097656, Forward: 0.08093619346618652, Backward: 0.09404706954956055, Optimizer: 0
Epoch 1 Batch 1936 Train Loss 0.013757801614701748
Total Times. Batch: 1936, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001256704330444336, Forward: 0.08135509490966797, Backward: 0.1031801700592041, Optimizer: 0
Epoch 1 Batch 1937 Train Loss 0.013214903883635998
Total Times. Batch: 1937, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011837482452392578, Forward: 0.08174276351928711, Backward: 0.10329818725585938, Optimizer: 0
Epoch 1 Batch 1938 Train Loss 0.01461978442966938
Total Times. Batch: 1938, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001186370849609375, Forward: 0.08132243156433105, Backward: 0.10324239730834961, Optimizer: 0
Epoch 1 Batch 1939 Train Loss 0.013828510418534279
Total Times. Batch: 1939, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011448860168457031, Forward: 0.08129739761352539, Backward: 0.10345077514648438, Optimizer: 0.16717123985290527
Epoch 1 Batch 1940 Train Loss 0.014365345239639282
Total Times. Batch: 1940, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011684894561767578, Forward: 0.08102750778198242, Backward: 0.09406471252441406, Optimizer: 0
Epoch 1 Batch 1941 Train Loss 0.01406219881027937
Total Times. Batch: 1941, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011589527130126953, Forward: 0.08190584182739258, Backward: 0.1032404899597168, Optimizer: 0
Epoch 1 Batch 1942 Train Loss 0.013829379342496395
Total Times. Batch: 1942, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012063980102539062, Forward: 0.08148550987243652, Backward: 0.10342764854431152, Optimizer: 0
Epoch 1 Batch 1943 Train Loss 0.013166993856430054
Total Times. Batch: 1943, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001188039779663086, Forward: 0.0814208984375, Backward: 0.10340428352355957, Optimizer: 0
Epoch 1 Batch 1944 Train Loss 0.013659586198627949
Total Times. Batch: 1944, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001096963882446289, Forward: 0.08103823661804199, Backward: 0.10322976112365723, Optimizer: 0.16791701316833496
Epoch 1 Batch 1945 Train Loss 0.014243724755942822
Total Times. Batch: 1945, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011658668518066406, Forward: 0.0810999870300293, Backward: 0.09404516220092773, Optimizer: 0
Epoch 1 Batch 1946 Train Loss 0.013536974787712097
Total Times. Batch: 1946, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012106895446777344, Forward: 0.08130383491516113, Backward: 0.10354065895080566, Optimizer: 0
Epoch 1 Batch 1947 Train Loss 0.01378631591796875
Total Times. Batch: 1947, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011436939239501953, Forward: 0.08107900619506836, Backward: 0.10338234901428223, Optimizer: 0
Epoch 1 Batch 1948 Train Loss 0.013010861352086067
Total Times. Batch: 1948, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011365413665771484, Forward: 0.08151769638061523, Backward: 0.1034383773803711, Optimizer: 0
Epoch 1 Batch 1949 Train Loss 0.01356066670268774
Total Times. Batch: 1949, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011856555938720703, Forward: 0.08203125, Backward: 0.1030879020690918, Optimizer: 0.16660118103027344
Epoch 1 Batch 1950 Train Loss 0.013782049529254436
Total Times. Batch: 1950, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001177072525024414, Forward: 0.08139324188232422, Backward: 0.0939486026763916, Optimizer: 0
Epoch 1 Batch 1951 Train Loss 0.013476616702973843
Total Times. Batch: 1951, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013089179992675781, Forward: 0.08115410804748535, Backward: 0.10338211059570312, Optimizer: 0
Epoch 1 Batch 1952 Train Loss 0.013567904010415077
Total Times. Batch: 1952, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011515617370605469, Forward: 0.08125185966491699, Backward: 0.10321664810180664, Optimizer: 0
Epoch 1 Batch 1953 Train Loss 0.013943612575531006
Total Times. Batch: 1953, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.00113677978515625, Forward: 0.08134579658508301, Backward: 0.1036219596862793, Optimizer: 0
Epoch 1 Batch 1954 Train Loss 0.01406987477093935
Total Times. Batch: 1954, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011506080627441406, Forward: 0.08159494400024414, Backward: 0.10283899307250977, Optimizer: 0.16738390922546387
Epoch 1 Batch 1955 Train Loss 0.013682986609637737
Total Times. Batch: 1955, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012180805206298828, Forward: 0.08186101913452148, Backward: 0.09388422966003418, Optimizer: 0
Epoch 1 Batch 1956 Train Loss 0.01329029817134142
Total Times. Batch: 1956, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013077259063720703, Forward: 0.0812845230102539, Backward: 0.1031639575958252, Optimizer: 0
Epoch 1 Batch 1957 Train Loss 0.013239756226539612
Total Times. Batch: 1957, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001180887222290039, Forward: 0.0819556713104248, Backward: 0.10346412658691406, Optimizer: 0
Epoch 1 Batch 1958 Train Loss 0.01371732261031866
Total Times. Batch: 1958, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011017322540283203, Forward: 0.08141255378723145, Backward: 0.10344266891479492, Optimizer: 0
Epoch 1 Batch 1959 Train Loss 0.01416989415884018
Total Times. Batch: 1959, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012145042419433594, Forward: 0.08145380020141602, Backward: 0.10355734825134277, Optimizer: 0.16722750663757324
Epoch 1 Batch 1960 Train Loss 0.013938868418335915
Total Times. Batch: 1960, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011560916900634766, Forward: 0.08198046684265137, Backward: 0.09382891654968262, Optimizer: 0
Epoch 1 Batch 1961 Train Loss 0.014185790903866291
Total Times. Batch: 1961, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001341104507446289, Forward: 0.08173036575317383, Backward: 0.10326719284057617, Optimizer: 0
Epoch 1 Batch 1962 Train Loss 0.01435172837227583
Total Times. Batch: 1962, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001245737075805664, Forward: 0.0811002254486084, Backward: 0.10321855545043945, Optimizer: 0
Epoch 1 Batch 1963 Train Loss 0.013743710704147816
Total Times. Batch: 1963, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012102127075195312, Forward: 0.08135199546813965, Backward: 0.10361123085021973, Optimizer: 0
Epoch 1 Batch 1964 Train Loss 0.014106707647442818
Total Times. Batch: 1964, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011639595031738281, Forward: 0.08124184608459473, Backward: 0.10353493690490723, Optimizer: 0.16693711280822754
Epoch 1 Batch 1965 Train Loss 0.014070897363126278
Total Times. Batch: 1965, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011458396911621094, Forward: 0.08131003379821777, Backward: 0.0939333438873291, Optimizer: 0
Epoch 1 Batch 1966 Train Loss 0.013547594659030437
Total Times. Batch: 1966, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011599063873291016, Forward: 0.08193230628967285, Backward: 0.10309386253356934, Optimizer: 0
Epoch 1 Batch 1967 Train Loss 0.013600820675492287
Total Times. Batch: 1967, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013604164123535156, Forward: 0.08151769638061523, Backward: 0.10312557220458984, Optimizer: 0
Epoch 1 Batch 1968 Train Loss 0.013425031676888466
Total Times. Batch: 1968, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011055469512939453, Forward: 0.08157634735107422, Backward: 0.10372805595397949, Optimizer: 0
Epoch 1 Batch 1969 Train Loss 0.01362580992281437
Total Times. Batch: 1969, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001161813735961914, Forward: 0.08153581619262695, Backward: 0.10364151000976562, Optimizer: 0.16717195510864258
Epoch 1 Batch 1970 Train Loss 0.013276695273816586
Total Times. Batch: 1970, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011963844299316406, Forward: 0.08147740364074707, Backward: 0.0937952995300293, Optimizer: 0
Epoch 1 Batch 1971 Train Loss 0.013924689963459969
Total Times. Batch: 1971, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010907649993896484, Forward: 0.08114933967590332, Backward: 0.10380887985229492, Optimizer: 0
Epoch 1 Batch 1972 Train Loss 0.013893111608922482
Total Times. Batch: 1972, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011591911315917969, Forward: 0.08129549026489258, Backward: 0.10348796844482422, Optimizer: 0
Epoch 1 Batch 1973 Train Loss 0.01373195368796587
Total Times. Batch: 1973, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012295246124267578, Forward: 0.0817720890045166, Backward: 0.10340189933776855, Optimizer: 0
Epoch 1 Batch 1974 Train Loss 0.013689259067177773
Total Times. Batch: 1974, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.00125885009765625, Forward: 0.08142209053039551, Backward: 0.10347294807434082, Optimizer: 0.16680026054382324
Epoch 1 Batch 1975 Train Loss 0.0139467166736722
Total Times. Batch: 1975, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0012028217315673828, Forward: 0.08119082450866699, Backward: 0.09401845932006836, Optimizer: 0
Epoch 1 Batch 1976 Train Loss 0.013573735021054745
Total Times. Batch: 1976, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011713504791259766, Forward: 0.08116292953491211, Backward: 0.1033942699432373, Optimizer: 0
Epoch 1 Batch 1977 Train Loss 0.01391642726957798
Total Times. Batch: 1977, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011701583862304688, Forward: 0.08154129981994629, Backward: 0.10306930541992188, Optimizer: 0
Epoch 1 Batch 1978 Train Loss 0.013453574851155281
Total Times. Batch: 1978, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011529922485351562, Forward: 0.08128619194030762, Backward: 0.10360217094421387, Optimizer: 0
Epoch 1 Batch 1979 Train Loss 0.013569468632340431
Total Times. Batch: 1979, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013146400451660156, Forward: 0.08195757865905762, Backward: 0.10336732864379883, Optimizer: 0.16704440116882324
Epoch 1 Batch 1980 Train Loss 0.013600519858300686
Total Times. Batch: 1980, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011801719665527344, Forward: 0.08132743835449219, Backward: 0.09348106384277344, Optimizer: 0
Epoch 1 Batch 1981 Train Loss 0.013491451740264893
Total Times. Batch: 1981, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001291513442993164, Forward: 0.08109879493713379, Backward: 0.10351848602294922, Optimizer: 0
Epoch 1 Batch 1982 Train Loss 0.013630437664687634
Total Times. Batch: 1982, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001163482666015625, Forward: 0.08141112327575684, Backward: 0.1034395694732666, Optimizer: 0
Epoch 1 Batch 1983 Train Loss 0.014163970947265625
Total Times. Batch: 1983, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001148223876953125, Forward: 0.0812070369720459, Backward: 0.10344123840332031, Optimizer: 0
Epoch 1 Batch 1984 Train Loss 0.013858288526535034
Total Times. Batch: 1984, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001140594482421875, Forward: 0.08155679702758789, Backward: 0.1028144359588623, Optimizer: 0.1672041416168213
Epoch 1 Batch 1985 Train Loss 0.013396731577813625
Total Times. Batch: 1985, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.001108407974243164, Forward: 0.08188319206237793, Backward: 0.09392428398132324, Optimizer: 0
Epoch 1 Batch 1986 Train Loss 0.013551360927522182
Total Times. Batch: 1986, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0013113021850585938, Forward: 0.08141851425170898, Backward: 0.10340332984924316, Optimizer: 0
Epoch 1 Batch 1987 Train Loss 0.013547755777835846
Total Times. Batch: 1987, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011048316955566406, Forward: 0.08119487762451172, Backward: 0.10331869125366211, Optimizer: 0
Epoch 1 Batch 1988 Train Loss 0.013597098179161549
Total Times. Batch: 1988, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011265277862548828, Forward: 0.0810701847076416, Backward: 0.10344290733337402, Optimizer: 0
Epoch 1 Batch 1989 Train Loss 0.012904539704322815
Total Times. Batch: 1989, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010688304901123047, Forward: 0.08131098747253418, Backward: 0.1035003662109375, Optimizer: 0.1669619083404541
Epoch 1 Batch 1990 Train Loss 0.01395947951823473
Total Times. Batch: 1990, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010440349578857422, Forward: 0.08150768280029297, Backward: 0.09375452995300293, Optimizer: 0
Epoch 1 Batch 1991 Train Loss 0.013898486271500587
Total Times. Batch: 1991, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010530948638916016, Forward: 0.08115029335021973, Backward: 0.10350370407104492, Optimizer: 0
Epoch 1 Batch 1992 Train Loss 0.013413587585091591
Total Times. Batch: 1992, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010538101196289062, Forward: 0.08166050910949707, Backward: 0.1032247543334961, Optimizer: 0
Epoch 1 Batch 1993 Train Loss 0.014165359549224377
Total Times. Batch: 1993, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010488033294677734, Forward: 0.08130669593811035, Backward: 0.10319685935974121, Optimizer: 0
Epoch 1 Batch 1994 Train Loss 0.013420449569821358
Total Times. Batch: 1994, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010449886322021484, Forward: 0.08123397827148438, Backward: 0.1034398078918457, Optimizer: 0.16664838790893555
Epoch 1 Batch 1995 Train Loss 0.013753297738730907
Total Times. Batch: 1995, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010290145874023438, Forward: 0.0809485912322998, Backward: 0.0939335823059082, Optimizer: 0
Epoch 1 Batch 1996 Train Loss 0.014001610688865185
Total Times. Batch: 1996, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0011837482452392578, Forward: 0.08136391639709473, Backward: 0.10294461250305176, Optimizer: 0
Epoch 1 Batch 1997 Train Loss 0.013366847299039364
Total Times. Batch: 1997, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010464191436767578, Forward: 0.08131265640258789, Backward: 0.10322260856628418, Optimizer: 0
Epoch 1 Batch 1998 Train Loss 0.013595662079751492
Total Times. Batch: 1998, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010557174682617188, Forward: 0.0812842845916748, Backward: 0.10336899757385254, Optimizer: 0
Epoch 1 Batch 1999 Train Loss 0.013567049987614155
Total Times. Batch: 1999, Rank: 0, Data Shape: torch.Size([16, 20, 1, 128, 128]), Data time: 0.0010552406311035156, Forward: 0.08143091201782227, Backward: 0.10289645195007324, Optimizer: 0.16665911674499512
all reduces executed!
STARTING VALIDATION!!!
VALIDATING ON swe
DONE VALIDATING - NOW SYNCING
DONE SYNCING - NOW LOGGING
Time for train 536.2584018707275. For valid: 58.123542070388794. For postprocessing:1.0332493782043457
Time taken for epoch 1 is 595.4152209758759 sec
Train loss: tensor([0.0484], device='cuda:0'). Valid loss: 0.013592327944934368
DONE ---- rank 0
